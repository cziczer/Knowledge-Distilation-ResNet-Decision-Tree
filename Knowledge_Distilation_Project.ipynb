{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33eedced",
   "metadata": {
    "id": "33eedced"
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, \n",
    "    Dense, \n",
    "    BatchNormalization, \n",
    "    Activation, \n",
    "    MaxPool2D, \n",
    "    GlobalAveragePooling2D, \n",
    "    Add, \n",
    "    Input, \n",
    "    Flatten, \n",
    "    LeakyReLU, \n",
    "    Concatenate, \n",
    "    Dropout,\n",
    "    MaxPooling2D\n",
    ")\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.constraints import Constraint ,UnitNorm\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, Callback\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.initializers import Orthogonal\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy, KLDivergence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from io import StringIO\n",
    "import emnist as em\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "-vggzehYbJus",
   "metadata": {
    "id": "-vggzehYbJus"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d37471c",
   "metadata": {
    "id": "2d37471c"
   },
   "source": [
    "## ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b888271d",
   "metadata": {
    "id": "b888271d"
   },
   "outputs": [],
   "source": [
    "# https://arxiv.org/pdf/1512.03385.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ef02e9a",
   "metadata": {
    "id": "4ef02e9a"
   },
   "outputs": [],
   "source": [
    "def create_resnet(input_shape, class_count):\n",
    "    n = 9 # 56 layers\n",
    "    channels = [16, 32, 64]\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv2D(channels[0], kernel_size=(3, 3), padding=\"same\", kernel_initializer=\"he_normal\", kernel_regularizer=l2(1e-4))(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(tf.nn.relu)(x)\n",
    "\n",
    "    for c in channels:\n",
    "        for i in range(n):\n",
    "            subsampling = i == 0 and c > 16\n",
    "            strides = (2, 2) if subsampling else (1, 1)\n",
    "            y = Conv2D(c, kernel_size=(3, 3), padding=\"same\", strides=strides, kernel_initializer=\"he_normal\", kernel_regularizer=l2(1e-4))(x)\n",
    "            y = BatchNormalization()(y)\n",
    "            y = Activation(tf.nn.relu)(y)\n",
    "            y = Conv2D(c, kernel_size=(3, 3), padding=\"same\", kernel_initializer=\"he_normal\", kernel_regularizer=l2(1e-4))(y)\n",
    "            y = BatchNormalization()(y)        \n",
    "            if subsampling:\n",
    "                x = Conv2D(c, kernel_size=(1, 1), strides=(2, 2), padding=\"same\", kernel_initializer=\"he_normal\", kernel_regularizer=l2(1e-4))(x)\n",
    "            x = Add()([x, y])\n",
    "            x = Activation(tf.nn.relu)(x)\n",
    "\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Flatten()(x)\n",
    "    outputs = Dense(class_count, activation=tf.nn.softmax, kernel_initializer=\"he_normal\")(x)\n",
    "\n",
    "    return Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf26a4e1",
   "metadata": {
    "id": "bf26a4e1"
   },
   "outputs": [],
   "source": [
    "# PREPARE TRAINING\n",
    "lr = 0.1\n",
    "optimizer = SGD(learning_rate=lr, momentum=0.9)\n",
    "import time\n",
    "\n",
    "\n",
    "class LearningController(Callback):\n",
    "    def __init__(self, num_epoch=0, learn_minute=0):\n",
    "        self.num_epoch = num_epoch\n",
    "        self.learn_second = learn_minute * 60\n",
    "        if self.learn_second > 0:\n",
    "            print(\"Leraning rate is controled by time.\")\n",
    "        elif self.num_epoch > 0:\n",
    "            print(\"Leraning rate is controled by epoch.\")\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        if self.learn_second > 0:\n",
    "            self.start_time = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.learn_second > 0:\n",
    "            current_time = time.time()\n",
    "            if current_time - self.start_time > self.learn_second / 2:\n",
    "                self.model.optimizer.lr = lr * 0.1            \n",
    "            if current_time - self.start_time > self.learn_second * 3 / 4:\n",
    "                self.model.optimizer.lr = lr * 0.01\n",
    "                \n",
    "        elif self.num_epoch > 0:\n",
    "            if epoch > self.num_epoch / 2:\n",
    "                self.model.optimizer.lr = lr * 0.1            \n",
    "            if epoch > self.num_epoch * 3 / 4:\n",
    "                self.model.optimizer.lr = lr * 0.01\n",
    "                    \n",
    "        print('lr:%.2e' % self.model.optimizer.lr.value())\n",
    "        \n",
    "\n",
    "learn_minute = 120"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac38408",
   "metadata": {
    "id": "bac38408"
   },
   "source": [
    "## Dense NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb9a1091",
   "metadata": {
    "id": "bb9a1091"
   },
   "outputs": [],
   "source": [
    "def create_dense_nn(input_shape, class_count):\n",
    "    return Sequential(\n",
    "        [\n",
    "            Input(shape=input_shape),\n",
    "            Flatten(),\n",
    "            Dense(256),\n",
    "            Dense(class_count),\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c5ca61",
   "metadata": {
    "id": "39c5ca61"
   },
   "source": [
    "## Distiller class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3ffa5cd",
   "metadata": {
    "id": "f3ffa5cd"
   },
   "outputs": [],
   "source": [
    "class Distiller(Model):\n",
    "    def __init__(self, student, teacher):\n",
    "        super(Distiller, self).__init__()\n",
    "        self.teacher = teacher\n",
    "        self.student = student\n",
    "\n",
    "    def compile(\n",
    "        self,\n",
    "        optimizer,\n",
    "        metrics,\n",
    "        student_loss_fn,\n",
    "        distillation_loss_fn,\n",
    "        alpha=0.1,\n",
    "        temperature=3,\n",
    "    ):\n",
    "        \"\"\" Configure the distiller.\n",
    "\n",
    "        Args:\n",
    "            optimizer: Keras optimizer for the student weights\n",
    "            metrics: Keras metrics for evaluation\n",
    "            student_loss_fn: Loss function of difference between student\n",
    "                predictions and ground-truth\n",
    "            distillation_loss_fn: Loss function of difference between soft\n",
    "                student predictions and soft teacher predictions\n",
    "            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n",
    "            temperature: Temperature for softening probability distributions.\n",
    "                Larger temperature gives softer distributions.\n",
    "        \"\"\"\n",
    "        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics)\n",
    "        self.student_loss_fn = student_loss_fn\n",
    "        self.distillation_loss_fn = distillation_loss_fn\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Unpack data\n",
    "        x, y = data\n",
    "\n",
    "        # Forward pass of teacher\n",
    "        teacher_predictions = self.teacher(x, training=False)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass of student\n",
    "            student_predictions = self.student(x, training=True)\n",
    "\n",
    "            # Compute losses\n",
    "            student_loss = self.student_loss_fn(y, student_predictions)\n",
    "            distillation_loss = self.distillation_loss_fn(\n",
    "                tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n",
    "                tf.nn.softmax(student_predictions / self.temperature, axis=1),\n",
    "            )\n",
    "            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.student.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # Update the metrics configured in `compile()`.\n",
    "        self.compiled_metrics.update_state(y, student_predictions)\n",
    "\n",
    "        # Return a dict of performance\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update(\n",
    "            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n",
    "        )\n",
    "        return results\n",
    "\n",
    "    def test_step(self, data):\n",
    "        # Unpack the data\n",
    "        x, y = data\n",
    "\n",
    "        # Compute predictions\n",
    "        y_prediction = self.student(x, training=False)\n",
    "\n",
    "        # Calculate the loss\n",
    "        student_loss = self.student_loss_fn(y, y_prediction)\n",
    "\n",
    "        # Update the metrics.\n",
    "        self.compiled_metrics.update_state(y, y_prediction)\n",
    "\n",
    "        # Return a dict of performance\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update({\"student_loss\": student_loss})\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a970f3ab",
   "metadata": {
    "id": "a970f3ab"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_epoch = 192"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6375b25",
   "metadata": {
    "id": "f6375b25"
   },
   "source": [
    "## CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f889162",
   "metadata": {
    "id": "2f889162"
   },
   "outputs": [],
   "source": [
    "cifar = tfds.load('cifar10', as_supervised = True, batch_size = -1)\n",
    "cifar_test, cifar_train = cifar['test'], cifar['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2db0adc8",
   "metadata": {
    "id": "2db0adc8"
   },
   "outputs": [],
   "source": [
    "cifar_train_x = cifar_train[0].numpy() / 255\n",
    "cifar_train_y = cifar_train[1].numpy()\n",
    "cifar_test_x = cifar_test[0].numpy() / 255\n",
    "cifar_test_y = cifar_test[1].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad9a91f0",
   "metadata": {
    "id": "ad9a91f0"
   },
   "outputs": [],
   "source": [
    "cifar_train_x, cifar_val_x, cifar_train_y, cifar_val_y = train_test_split(cifar_train_x, cifar_train_y, \n",
    "    test_size=0.25, random_state= 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235aff14",
   "metadata": {
    "id": "235aff14"
   },
   "source": [
    "## Teacher "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8a1784f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a8a1784f",
    "outputId": "f12c0f5a-7710-43f2-f5c6-826c2e1f8565",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_57 (Conv2D)             (None, 32, 32, 16)   448         ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_55 (BatchN  (None, 32, 32, 16)  64          ['conv2d_57[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_55 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_55[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_58 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_55[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_56 (BatchN  (None, 32, 32, 16)  64          ['conv2d_58[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_56 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_56[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_59 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_56[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_57 (BatchN  (None, 32, 32, 16)  64          ['conv2d_59[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_27 (Add)                   (None, 32, 32, 16)   0           ['activation_55[0][0]',          \n",
      "                                                                  'batch_normalization_57[0][0]'] \n",
      "                                                                                                  \n",
      " activation_57 (Activation)     (None, 32, 32, 16)   0           ['add_27[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_60 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_57[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_58 (BatchN  (None, 32, 32, 16)  64          ['conv2d_60[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_58 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_58[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_61 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_58[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_59 (BatchN  (None, 32, 32, 16)  64          ['conv2d_61[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_28 (Add)                   (None, 32, 32, 16)   0           ['activation_57[0][0]',          \n",
      "                                                                  'batch_normalization_59[0][0]'] \n",
      "                                                                                                  \n",
      " activation_59 (Activation)     (None, 32, 32, 16)   0           ['add_28[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_62 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_59[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_60 (BatchN  (None, 32, 32, 16)  64          ['conv2d_62[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_60 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_60[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_63 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_60[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_61 (BatchN  (None, 32, 32, 16)  64          ['conv2d_63[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_29 (Add)                   (None, 32, 32, 16)   0           ['activation_59[0][0]',          \n",
      "                                                                  'batch_normalization_61[0][0]'] \n",
      "                                                                                                  \n",
      " activation_61 (Activation)     (None, 32, 32, 16)   0           ['add_29[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_64 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_61[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_62 (BatchN  (None, 32, 32, 16)  64          ['conv2d_64[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_62 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_62[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_65 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_62[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_63 (BatchN  (None, 32, 32, 16)  64          ['conv2d_65[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_30 (Add)                   (None, 32, 32, 16)   0           ['activation_61[0][0]',          \n",
      "                                                                  'batch_normalization_63[0][0]'] \n",
      "                                                                                                  \n",
      " activation_63 (Activation)     (None, 32, 32, 16)   0           ['add_30[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_66 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_63[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_64 (BatchN  (None, 32, 32, 16)  64          ['conv2d_66[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_64 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_64[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_67 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_64[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_65 (BatchN  (None, 32, 32, 16)  64          ['conv2d_67[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_31 (Add)                   (None, 32, 32, 16)   0           ['activation_63[0][0]',          \n",
      "                                                                  'batch_normalization_65[0][0]'] \n",
      "                                                                                                  \n",
      " activation_65 (Activation)     (None, 32, 32, 16)   0           ['add_31[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_68 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_65[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_66 (BatchN  (None, 32, 32, 16)  64          ['conv2d_68[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_66 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_66[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_69 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_66[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_67 (BatchN  (None, 32, 32, 16)  64          ['conv2d_69[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_32 (Add)                   (None, 32, 32, 16)   0           ['activation_65[0][0]',          \n",
      "                                                                  'batch_normalization_67[0][0]'] \n",
      "                                                                                                  \n",
      " activation_67 (Activation)     (None, 32, 32, 16)   0           ['add_32[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_70 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_67[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_68 (BatchN  (None, 32, 32, 16)  64          ['conv2d_70[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_68 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_68[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_71 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_68[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_69 (BatchN  (None, 32, 32, 16)  64          ['conv2d_71[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_33 (Add)                   (None, 32, 32, 16)   0           ['activation_67[0][0]',          \n",
      "                                                                  'batch_normalization_69[0][0]'] \n",
      "                                                                                                  \n",
      " activation_69 (Activation)     (None, 32, 32, 16)   0           ['add_33[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_72 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_69[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_70 (BatchN  (None, 32, 32, 16)  64          ['conv2d_72[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_70 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_70[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_73 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_70[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_71 (BatchN  (None, 32, 32, 16)  64          ['conv2d_73[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_34 (Add)                   (None, 32, 32, 16)   0           ['activation_69[0][0]',          \n",
      "                                                                  'batch_normalization_71[0][0]'] \n",
      "                                                                                                  \n",
      " activation_71 (Activation)     (None, 32, 32, 16)   0           ['add_34[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_74 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_71[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_72 (BatchN  (None, 32, 32, 16)  64          ['conv2d_74[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_72 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_72[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_75 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_72[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_73 (BatchN  (None, 32, 32, 16)  64          ['conv2d_75[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_35 (Add)                   (None, 32, 32, 16)   0           ['activation_71[0][0]',          \n",
      "                                                                  'batch_normalization_73[0][0]'] \n",
      "                                                                                                  \n",
      " activation_73 (Activation)     (None, 32, 32, 16)   0           ['add_35[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_76 (Conv2D)             (None, 16, 16, 32)   4640        ['activation_73[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_74 (BatchN  (None, 16, 16, 32)  128         ['conv2d_76[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_74 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_74[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_77 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_74[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_78 (Conv2D)             (None, 16, 16, 32)   544         ['activation_73[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_75 (BatchN  (None, 16, 16, 32)  128         ['conv2d_77[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_36 (Add)                   (None, 16, 16, 32)   0           ['conv2d_78[0][0]',              \n",
      "                                                                  'batch_normalization_75[0][0]'] \n",
      "                                                                                                  \n",
      " activation_75 (Activation)     (None, 16, 16, 32)   0           ['add_36[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_79 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_75[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_76 (BatchN  (None, 16, 16, 32)  128         ['conv2d_79[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_76 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_76[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_80 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_76[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_77 (BatchN  (None, 16, 16, 32)  128         ['conv2d_80[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_37 (Add)                   (None, 16, 16, 32)   0           ['activation_75[0][0]',          \n",
      "                                                                  'batch_normalization_77[0][0]'] \n",
      "                                                                                                  \n",
      " activation_77 (Activation)     (None, 16, 16, 32)   0           ['add_37[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_81 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_77[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_78 (BatchN  (None, 16, 16, 32)  128         ['conv2d_81[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_78 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_78[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_82 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_78[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_79 (BatchN  (None, 16, 16, 32)  128         ['conv2d_82[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_38 (Add)                   (None, 16, 16, 32)   0           ['activation_77[0][0]',          \n",
      "                                                                  'batch_normalization_79[0][0]'] \n",
      "                                                                                                  \n",
      " activation_79 (Activation)     (None, 16, 16, 32)   0           ['add_38[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_83 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_79[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_80 (BatchN  (None, 16, 16, 32)  128         ['conv2d_83[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_80 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_80[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_84 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_80[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_81 (BatchN  (None, 16, 16, 32)  128         ['conv2d_84[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_39 (Add)                   (None, 16, 16, 32)   0           ['activation_79[0][0]',          \n",
      "                                                                  'batch_normalization_81[0][0]'] \n",
      "                                                                                                  \n",
      " activation_81 (Activation)     (None, 16, 16, 32)   0           ['add_39[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_85 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_81[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_82 (BatchN  (None, 16, 16, 32)  128         ['conv2d_85[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_82 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_82[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_86 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_82[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_83 (BatchN  (None, 16, 16, 32)  128         ['conv2d_86[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_40 (Add)                   (None, 16, 16, 32)   0           ['activation_81[0][0]',          \n",
      "                                                                  'batch_normalization_83[0][0]'] \n",
      "                                                                                                  \n",
      " activation_83 (Activation)     (None, 16, 16, 32)   0           ['add_40[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_87 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_83[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_84 (BatchN  (None, 16, 16, 32)  128         ['conv2d_87[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_84 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_84[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_88 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_84[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_85 (BatchN  (None, 16, 16, 32)  128         ['conv2d_88[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_41 (Add)                   (None, 16, 16, 32)   0           ['activation_83[0][0]',          \n",
      "                                                                  'batch_normalization_85[0][0]'] \n",
      "                                                                                                  \n",
      " activation_85 (Activation)     (None, 16, 16, 32)   0           ['add_41[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_89 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_85[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_86 (BatchN  (None, 16, 16, 32)  128         ['conv2d_89[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_86 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_86[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_90 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_86[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_87 (BatchN  (None, 16, 16, 32)  128         ['conv2d_90[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_42 (Add)                   (None, 16, 16, 32)   0           ['activation_85[0][0]',          \n",
      "                                                                  'batch_normalization_87[0][0]'] \n",
      "                                                                                                  \n",
      " activation_87 (Activation)     (None, 16, 16, 32)   0           ['add_42[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_91 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_87[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_88 (BatchN  (None, 16, 16, 32)  128         ['conv2d_91[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_88 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_88[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_92 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_88[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_89 (BatchN  (None, 16, 16, 32)  128         ['conv2d_92[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_43 (Add)                   (None, 16, 16, 32)   0           ['activation_87[0][0]',          \n",
      "                                                                  'batch_normalization_89[0][0]'] \n",
      "                                                                                                  \n",
      " activation_89 (Activation)     (None, 16, 16, 32)   0           ['add_43[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_93 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_89[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_90 (BatchN  (None, 16, 16, 32)  128         ['conv2d_93[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_90 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_90[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_94 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_90[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_91 (BatchN  (None, 16, 16, 32)  128         ['conv2d_94[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_44 (Add)                   (None, 16, 16, 32)   0           ['activation_89[0][0]',          \n",
      "                                                                  'batch_normalization_91[0][0]'] \n",
      "                                                                                                  \n",
      " activation_91 (Activation)     (None, 16, 16, 32)   0           ['add_44[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_95 (Conv2D)             (None, 8, 8, 64)     18496       ['activation_91[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_92 (BatchN  (None, 8, 8, 64)    256         ['conv2d_95[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_92 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_92[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_96 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_92[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_97 (Conv2D)             (None, 8, 8, 64)     2112        ['activation_91[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_93 (BatchN  (None, 8, 8, 64)    256         ['conv2d_96[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_45 (Add)                   (None, 8, 8, 64)     0           ['conv2d_97[0][0]',              \n",
      "                                                                  'batch_normalization_93[0][0]'] \n",
      "                                                                                                  \n",
      " activation_93 (Activation)     (None, 8, 8, 64)     0           ['add_45[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_98 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_93[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_94 (BatchN  (None, 8, 8, 64)    256         ['conv2d_98[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_94 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_94[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_99 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_94[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_95 (BatchN  (None, 8, 8, 64)    256         ['conv2d_99[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_46 (Add)                   (None, 8, 8, 64)     0           ['activation_93[0][0]',          \n",
      "                                                                  'batch_normalization_95[0][0]'] \n",
      "                                                                                                  \n",
      " activation_95 (Activation)     (None, 8, 8, 64)     0           ['add_46[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_100 (Conv2D)            (None, 8, 8, 64)     36928       ['activation_95[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_96 (BatchN  (None, 8, 8, 64)    256         ['conv2d_100[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_96 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_96[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_101 (Conv2D)            (None, 8, 8, 64)     36928       ['activation_96[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_97 (BatchN  (None, 8, 8, 64)    256         ['conv2d_101[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_47 (Add)                   (None, 8, 8, 64)     0           ['activation_95[0][0]',          \n",
      "                                                                  'batch_normalization_97[0][0]'] \n",
      "                                                                                                  \n",
      " activation_97 (Activation)     (None, 8, 8, 64)     0           ['add_47[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_102 (Conv2D)            (None, 8, 8, 64)     36928       ['activation_97[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_98 (BatchN  (None, 8, 8, 64)    256         ['conv2d_102[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_98 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_98[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_103 (Conv2D)            (None, 8, 8, 64)     36928       ['activation_98[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_99 (BatchN  (None, 8, 8, 64)    256         ['conv2d_103[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_48 (Add)                   (None, 8, 8, 64)     0           ['activation_97[0][0]',          \n",
      "                                                                  'batch_normalization_99[0][0]'] \n",
      "                                                                                                  \n",
      " activation_99 (Activation)     (None, 8, 8, 64)     0           ['add_48[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_104 (Conv2D)            (None, 8, 8, 64)     36928       ['activation_99[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_100 (Batch  (None, 8, 8, 64)    256         ['conv2d_104[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_100 (Activation)    (None, 8, 8, 64)     0           ['batch_normalization_100[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_105 (Conv2D)            (None, 8, 8, 64)     36928       ['activation_100[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_101 (Batch  (None, 8, 8, 64)    256         ['conv2d_105[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " add_49 (Add)                   (None, 8, 8, 64)     0           ['activation_99[0][0]',          \n",
      "                                                                  'batch_normalization_101[0][0]']\n",
      "                                                                                                  \n",
      " activation_101 (Activation)    (None, 8, 8, 64)     0           ['add_49[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_106 (Conv2D)            (None, 8, 8, 64)     36928       ['activation_101[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_102 (Batch  (None, 8, 8, 64)    256         ['conv2d_106[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_102 (Activation)    (None, 8, 8, 64)     0           ['batch_normalization_102[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_107 (Conv2D)            (None, 8, 8, 64)     36928       ['activation_102[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_103 (Batch  (None, 8, 8, 64)    256         ['conv2d_107[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " add_50 (Add)                   (None, 8, 8, 64)     0           ['activation_101[0][0]',         \n",
      "                                                                  'batch_normalization_103[0][0]']\n",
      "                                                                                                  \n",
      " activation_103 (Activation)    (None, 8, 8, 64)     0           ['add_50[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_108 (Conv2D)            (None, 8, 8, 64)     36928       ['activation_103[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_104 (Batch  (None, 8, 8, 64)    256         ['conv2d_108[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_104 (Activation)    (None, 8, 8, 64)     0           ['batch_normalization_104[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_109 (Conv2D)            (None, 8, 8, 64)     36928       ['activation_104[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_105 (Batch  (None, 8, 8, 64)    256         ['conv2d_109[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " add_51 (Add)                   (None, 8, 8, 64)     0           ['activation_103[0][0]',         \n",
      "                                                                  'batch_normalization_105[0][0]']\n",
      "                                                                                                  \n",
      " activation_105 (Activation)    (None, 8, 8, 64)     0           ['add_51[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_110 (Conv2D)            (None, 8, 8, 64)     36928       ['activation_105[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_106 (Batch  (None, 8, 8, 64)    256         ['conv2d_110[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_106 (Activation)    (None, 8, 8, 64)     0           ['batch_normalization_106[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_111 (Conv2D)            (None, 8, 8, 64)     36928       ['activation_106[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_107 (Batch  (None, 8, 8, 64)    256         ['conv2d_111[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " add_52 (Add)                   (None, 8, 8, 64)     0           ['activation_105[0][0]',         \n",
      "                                                                  'batch_normalization_107[0][0]']\n",
      "                                                                                                  \n",
      " activation_107 (Activation)    (None, 8, 8, 64)     0           ['add_52[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_112 (Conv2D)            (None, 8, 8, 64)     36928       ['activation_107[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_108 (Batch  (None, 8, 8, 64)    256         ['conv2d_112[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_108 (Activation)    (None, 8, 8, 64)     0           ['batch_normalization_108[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_113 (Conv2D)            (None, 8, 8, 64)     36928       ['activation_108[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_109 (Batch  (None, 8, 8, 64)    256         ['conv2d_113[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " add_53 (Add)                   (None, 8, 8, 64)     0           ['activation_107[0][0]',         \n",
      "                                                                  'batch_normalization_109[0][0]']\n",
      "                                                                                                  \n",
      " activation_109 (Activation)    (None, 8, 8, 64)     0           ['add_53[0][0]']                 \n",
      "                                                                                                  \n",
      " global_average_pooling2d_1 (Gl  (None, 64)          0           ['activation_109[0][0]']         \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 64)           0           ['global_average_pooling2d_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 10)           650         ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 861,770\n",
      "Trainable params: 857,706\n",
      "Non-trainable params: 4,064\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cifar_teacher_model = create_resnet((32, 32, 3), 10)\n",
    "cifar_teacher_model.type = \"cifar_resnet\"\n",
    "cifar_teacher_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1db50d60",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1db50d60",
    "outputId": "32f6c35f-dcf3-49fc-8269-3d909a7e2019"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leraning rate is controled by epoch.\n"
     ]
    }
   ],
   "source": [
    "cifar_teacher_model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "cifar_teacher_path = \"ResNet-for-CIFAR-10.h5\"\n",
    "cifar_teacher_checkpoint = ModelCheckpoint(filepath = cifar_teacher_path, monitor=\"val_loss\", verbose=1, save_best_only=True)\n",
    "cifar_teacher_learning_controller = LearningController(num_epoch)\n",
    "cifar_teacher_callbacks = [cifar_teacher_checkpoint, cifar_teacher_learning_controller]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c1bf85d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5c1bf85d",
    "outputId": "519617d9-5b3c-447c-da8e-b6469fc0c3a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/191\n",
      "  6/293 [..............................] - ETA: 23s - loss: 9.5224 - accuracy: 0.0990"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0402s vs `on_train_batch_end` time: 0.0448s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "293/293 [==============================] - ETA: 0s - loss: 3.4581 - accuracy: 0.1042\n",
      "Epoch 1: val_loss improved from inf to 3.22777, saving model to ResNet-for-CIFAR-10.h5\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 37s 89ms/step - loss: 3.4581 - accuracy: 0.1042 - val_loss: 3.2278 - val_accuracy: 0.0984\n",
      "Epoch 2/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 3.1685 - accuracy: 0.0986\n",
      "Epoch 2: val_loss improved from 3.22777 to 3.11854, saving model to ResNet-for-CIFAR-10.h5\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 3.1685 - accuracy: 0.0986 - val_loss: 3.1185 - val_accuracy: 0.0954\n",
      "Epoch 3/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 3.0838 - accuracy: 0.0967\n",
      "Epoch 3: val_loss improved from 3.11854 to 3.04483, saving model to ResNet-for-CIFAR-10.h5\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 3.0838 - accuracy: 0.0967 - val_loss: 3.0448 - val_accuracy: 0.0984\n",
      "Epoch 4/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 3.0002 - accuracy: 0.1014\n",
      "Epoch 4: val_loss improved from 3.04483 to 2.96064, saving model to ResNet-for-CIFAR-10.h5\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 85ms/step - loss: 3.0002 - accuracy: 0.1014 - val_loss: 2.9606 - val_accuracy: 0.0984\n",
      "Epoch 5/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 2.9231 - accuracy: 0.1002\n",
      "Epoch 5: val_loss improved from 2.96064 to 2.89101, saving model to ResNet-for-CIFAR-10.h5\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 2.9231 - accuracy: 0.1002 - val_loss: 2.8910 - val_accuracy: 0.0984\n",
      "Epoch 6/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 2.8553 - accuracy: 0.0970\n",
      "Epoch 6: val_loss improved from 2.89101 to 2.82367, saving model to ResNet-for-CIFAR-10.h5\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 2.8553 - accuracy: 0.0970 - val_loss: 2.8237 - val_accuracy: 0.1013\n",
      "Epoch 7/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 2.7942 - accuracy: 0.0995\n",
      "Epoch 7: val_loss improved from 2.82367 to 2.76428, saving model to ResNet-for-CIFAR-10.h5\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 2.7942 - accuracy: 0.0995 - val_loss: 2.7643 - val_accuracy: 0.1013\n",
      "Epoch 8/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 2.7398 - accuracy: 0.1005\n",
      "Epoch 8: val_loss improved from 2.76428 to 2.71349, saving model to ResNet-for-CIFAR-10.h5\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 2.7398 - accuracy: 0.1005 - val_loss: 2.7135 - val_accuracy: 0.0984\n",
      "Epoch 9/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 2.6915 - accuracy: 0.1002\n",
      "Epoch 9: val_loss improved from 2.71349 to 2.66983, saving model to ResNet-for-CIFAR-10.h5\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 26s 87ms/step - loss: 2.6915 - accuracy: 0.1002 - val_loss: 2.6698 - val_accuracy: 0.0979\n",
      "Epoch 10/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 2.5601 - accuracy: 0.1474\n",
      "Epoch 10: val_loss improved from 2.66983 to 2.57571, saving model to ResNet-for-CIFAR-10.h5\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 87ms/step - loss: 2.5601 - accuracy: 0.1474 - val_loss: 2.5757 - val_accuracy: 0.1470\n",
      "Epoch 11/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 2.1474 - accuracy: 0.3101\n",
      "Epoch 11: val_loss improved from 2.57571 to 2.11802, saving model to ResNet-for-CIFAR-10.h5\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 26s 87ms/step - loss: 2.1474 - accuracy: 0.3101 - val_loss: 2.1180 - val_accuracy: 0.3140\n",
      "Epoch 12/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 1.8877 - accuracy: 0.4075\n",
      "Epoch 12: val_loss improved from 2.11802 to 1.96439, saving model to ResNet-for-CIFAR-10.h5\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 87ms/step - loss: 1.8877 - accuracy: 0.4075 - val_loss: 1.9644 - val_accuracy: 0.3788\n",
      "Epoch 13/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 1.7389 - accuracy: 0.4588\n",
      "Epoch 13: val_loss improved from 1.96439 to 1.94405, saving model to ResNet-for-CIFAR-10.h5\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 87ms/step - loss: 1.7389 - accuracy: 0.4588 - val_loss: 1.9440 - val_accuracy: 0.4102\n",
      "Epoch 14/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 1.6300 - accuracy: 0.4960\n",
      "Epoch 14: val_loss did not improve from 1.94405\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 1.6300 - accuracy: 0.4960 - val_loss: 2.8924 - val_accuracy: 0.2582\n",
      "Epoch 15/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 1.5219 - accuracy: 0.5353\n",
      "Epoch 15: val_loss did not improve from 1.94405\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 1.5219 - accuracy: 0.5353 - val_loss: 2.5424 - val_accuracy: 0.3230\n",
      "Epoch 16/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 1.3904 - accuracy: 0.5845\n",
      "Epoch 16: val_loss improved from 1.94405 to 1.80447, saving model to ResNet-for-CIFAR-10.h5\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 1.3904 - accuracy: 0.5845 - val_loss: 1.8045 - val_accuracy: 0.4604\n",
      "Epoch 17/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 1.2704 - accuracy: 0.6252\n",
      "Epoch 17: val_loss improved from 1.80447 to 1.70092, saving model to ResNet-for-CIFAR-10.h5\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 1.2704 - accuracy: 0.6252 - val_loss: 1.7009 - val_accuracy: 0.5041\n",
      "Epoch 18/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 1.1766 - accuracy: 0.6574\n",
      "Epoch 18: val_loss did not improve from 1.70092\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 1.1766 - accuracy: 0.6574 - val_loss: 1.9227 - val_accuracy: 0.4814\n",
      "Epoch 19/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 1.0960 - accuracy: 0.6863\n",
      "Epoch 19: val_loss improved from 1.70092 to 1.58985, saving model to ResNet-for-CIFAR-10.h5\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 1.0960 - accuracy: 0.6863 - val_loss: 1.5899 - val_accuracy: 0.5450\n",
      "Epoch 20/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 1.0226 - accuracy: 0.7136\n",
      "Epoch 20: val_loss improved from 1.58985 to 1.33313, saving model to ResNet-for-CIFAR-10.h5\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 1.0226 - accuracy: 0.7136 - val_loss: 1.3331 - val_accuracy: 0.6278\n",
      "Epoch 21/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.9496 - accuracy: 0.7389\n",
      "Epoch 21: val_loss did not improve from 1.33313\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 87ms/step - loss: 0.9496 - accuracy: 0.7389 - val_loss: 1.3550 - val_accuracy: 0.6246\n",
      "Epoch 22/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.8829 - accuracy: 0.7643\n",
      "Epoch 22: val_loss did not improve from 1.33313\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 87ms/step - loss: 0.8829 - accuracy: 0.7643 - val_loss: 1.5350 - val_accuracy: 0.5618\n",
      "Epoch 23/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.8306 - accuracy: 0.7849\n",
      "Epoch 23: val_loss did not improve from 1.33313\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 87ms/step - loss: 0.8306 - accuracy: 0.7849 - val_loss: 1.3499 - val_accuracy: 0.6246\n",
      "Epoch 24/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.7892 - accuracy: 0.7991\n",
      "Epoch 24: val_loss did not improve from 1.33313\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.7892 - accuracy: 0.7991 - val_loss: 1.5010 - val_accuracy: 0.6175\n",
      "Epoch 25/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.7496 - accuracy: 0.8120\n",
      "Epoch 25: val_loss did not improve from 1.33313\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.7496 - accuracy: 0.8120 - val_loss: 1.4026 - val_accuracy: 0.6356\n",
      "Epoch 26/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.7182 - accuracy: 0.8261\n",
      "Epoch 26: val_loss improved from 1.33313 to 1.16181, saving model to ResNet-for-CIFAR-10.h5\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 0.7182 - accuracy: 0.8261 - val_loss: 1.1618 - val_accuracy: 0.6968\n",
      "Epoch 27/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.7015 - accuracy: 0.8338\n",
      "Epoch 27: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.7015 - accuracy: 0.8338 - val_loss: 1.3510 - val_accuracy: 0.6674\n",
      "Epoch 28/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6645 - accuracy: 0.8509\n",
      "Epoch 28: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 85ms/step - loss: 0.6645 - accuracy: 0.8509 - val_loss: 1.4242 - val_accuracy: 0.6638\n",
      "Epoch 29/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6422 - accuracy: 0.8619\n",
      "Epoch 29: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 85ms/step - loss: 0.6422 - accuracy: 0.8619 - val_loss: 1.5254 - val_accuracy: 0.6444\n",
      "Epoch 30/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6218 - accuracy: 0.8717\n",
      "Epoch 30: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 85ms/step - loss: 0.6218 - accuracy: 0.8717 - val_loss: 1.6567 - val_accuracy: 0.6194\n",
      "Epoch 31/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6048 - accuracy: 0.8786\n",
      "Epoch 31: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.6048 - accuracy: 0.8786 - val_loss: 1.2251 - val_accuracy: 0.7141\n",
      "Epoch 32/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5956 - accuracy: 0.8857\n",
      "Epoch 32: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 87ms/step - loss: 0.5956 - accuracy: 0.8857 - val_loss: 1.6077 - val_accuracy: 0.6585\n",
      "Epoch 33/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5786 - accuracy: 0.8933\n",
      "Epoch 33: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.5786 - accuracy: 0.8933 - val_loss: 1.2810 - val_accuracy: 0.7249\n",
      "Epoch 34/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5833 - accuracy: 0.8958\n",
      "Epoch 34: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.5833 - accuracy: 0.8958 - val_loss: 1.7602 - val_accuracy: 0.6438\n",
      "Epoch 35/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5593 - accuracy: 0.9082\n",
      "Epoch 35: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.5593 - accuracy: 0.9082 - val_loss: 1.5991 - val_accuracy: 0.6678\n",
      "Epoch 36/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5552 - accuracy: 0.9117\n",
      "Epoch 36: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 85ms/step - loss: 0.5552 - accuracy: 0.9117 - val_loss: 1.3519 - val_accuracy: 0.6952\n",
      "Epoch 37/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5511 - accuracy: 0.9170\n",
      "Epoch 37: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.5511 - accuracy: 0.9170 - val_loss: 1.9127 - val_accuracy: 0.6186\n",
      "Epoch 38/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5515 - accuracy: 0.9171\n",
      "Epoch 38: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.5515 - accuracy: 0.9171 - val_loss: 1.8398 - val_accuracy: 0.6412\n",
      "Epoch 39/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5451 - accuracy: 0.9210\n",
      "Epoch 39: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.5451 - accuracy: 0.9210 - val_loss: 1.9248 - val_accuracy: 0.6402\n",
      "Epoch 40/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5403 - accuracy: 0.9258\n",
      "Epoch 40: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.5403 - accuracy: 0.9258 - val_loss: 1.4177 - val_accuracy: 0.7148\n",
      "Epoch 41/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5280 - accuracy: 0.9314\n",
      "Epoch 41: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.5280 - accuracy: 0.9314 - val_loss: 1.6181 - val_accuracy: 0.6851\n",
      "Epoch 42/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5400 - accuracy: 0.9298\n",
      "Epoch 42: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.5400 - accuracy: 0.9298 - val_loss: 1.9667 - val_accuracy: 0.6534\n",
      "Epoch 43/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5278 - accuracy: 0.9361\n",
      "Epoch 43: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.5278 - accuracy: 0.9361 - val_loss: 1.5816 - val_accuracy: 0.6977\n",
      "Epoch 44/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5325 - accuracy: 0.9364\n",
      "Epoch 44: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.5325 - accuracy: 0.9364 - val_loss: 1.4003 - val_accuracy: 0.7202\n",
      "Epoch 45/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5162 - accuracy: 0.9429\n",
      "Epoch 45: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.5162 - accuracy: 0.9429 - val_loss: 1.6403 - val_accuracy: 0.7103\n",
      "Epoch 46/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5191 - accuracy: 0.9407\n",
      "Epoch 46: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.5191 - accuracy: 0.9407 - val_loss: 1.5849 - val_accuracy: 0.7101\n",
      "Epoch 47/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5232 - accuracy: 0.9410\n",
      "Epoch 47: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.5232 - accuracy: 0.9410 - val_loss: 1.7546 - val_accuracy: 0.6757\n",
      "Epoch 48/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5218 - accuracy: 0.9439\n",
      "Epoch 48: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.5218 - accuracy: 0.9439 - val_loss: 1.9523 - val_accuracy: 0.6598\n",
      "Epoch 49/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5199 - accuracy: 0.9434\n",
      "Epoch 49: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.5199 - accuracy: 0.9434 - val_loss: 2.1960 - val_accuracy: 0.6265\n",
      "Epoch 50/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5157 - accuracy: 0.9455\n",
      "Epoch 50: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 84ms/step - loss: 0.5157 - accuracy: 0.9455 - val_loss: 1.6801 - val_accuracy: 0.7008\n",
      "Epoch 51/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5247 - accuracy: 0.9430\n",
      "Epoch 51: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.5247 - accuracy: 0.9430 - val_loss: 1.4974 - val_accuracy: 0.7290\n",
      "Epoch 52/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5150 - accuracy: 0.9477\n",
      "Epoch 52: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.5150 - accuracy: 0.9477 - val_loss: 1.5578 - val_accuracy: 0.7100\n",
      "Epoch 53/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5205 - accuracy: 0.9469\n",
      "Epoch 53: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 0.5205 - accuracy: 0.9469 - val_loss: 2.0024 - val_accuracy: 0.6730\n",
      "Epoch 54/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5221 - accuracy: 0.9469\n",
      "Epoch 54: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.5221 - accuracy: 0.9469 - val_loss: 1.8682 - val_accuracy: 0.6770\n",
      "Epoch 55/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5112 - accuracy: 0.9517\n",
      "Epoch 55: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.5112 - accuracy: 0.9517 - val_loss: 1.5477 - val_accuracy: 0.7194\n",
      "Epoch 56/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5097 - accuracy: 0.9510\n",
      "Epoch 56: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.5097 - accuracy: 0.9510 - val_loss: 1.7567 - val_accuracy: 0.6906\n",
      "Epoch 57/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5108 - accuracy: 0.9500\n",
      "Epoch 57: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.5108 - accuracy: 0.9500 - val_loss: 1.7199 - val_accuracy: 0.7151\n",
      "Epoch 58/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5066 - accuracy: 0.9518\n",
      "Epoch 58: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.5066 - accuracy: 0.9518 - val_loss: 2.7507 - val_accuracy: 0.6170\n",
      "Epoch 59/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5274 - accuracy: 0.9455\n",
      "Epoch 59: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.5274 - accuracy: 0.9455 - val_loss: 1.8559 - val_accuracy: 0.6889\n",
      "Epoch 60/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4998 - accuracy: 0.9555\n",
      "Epoch 60: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.4998 - accuracy: 0.9555 - val_loss: 1.5480 - val_accuracy: 0.7162\n",
      "Epoch 61/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4974 - accuracy: 0.9555\n",
      "Epoch 61: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.4974 - accuracy: 0.9555 - val_loss: 1.8413 - val_accuracy: 0.6919\n",
      "Epoch 62/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5202 - accuracy: 0.9483\n",
      "Epoch 62: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.5202 - accuracy: 0.9483 - val_loss: 2.0719 - val_accuracy: 0.6710\n",
      "Epoch 63/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5038 - accuracy: 0.9550\n",
      "Epoch 63: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.5038 - accuracy: 0.9550 - val_loss: 1.5845 - val_accuracy: 0.7296\n",
      "Epoch 64/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4991 - accuracy: 0.9562\n",
      "Epoch 64: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.4991 - accuracy: 0.9562 - val_loss: 1.6019 - val_accuracy: 0.7351\n",
      "Epoch 65/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5013 - accuracy: 0.9550\n",
      "Epoch 65: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.5013 - accuracy: 0.9550 - val_loss: 1.6568 - val_accuracy: 0.7176\n",
      "Epoch 66/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4998 - accuracy: 0.9545\n",
      "Epoch 66: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.4998 - accuracy: 0.9545 - val_loss: 2.0968 - val_accuracy: 0.6767\n",
      "Epoch 67/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5025 - accuracy: 0.9557\n",
      "Epoch 67: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 0.5025 - accuracy: 0.9557 - val_loss: 1.8162 - val_accuracy: 0.7002\n",
      "Epoch 68/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4965 - accuracy: 0.9556\n",
      "Epoch 68: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.4965 - accuracy: 0.9556 - val_loss: 2.3214 - val_accuracy: 0.6359\n",
      "Epoch 69/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5076 - accuracy: 0.9534\n",
      "Epoch 69: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 0.5076 - accuracy: 0.9534 - val_loss: 1.4779 - val_accuracy: 0.7274\n",
      "Epoch 70/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4986 - accuracy: 0.9566\n",
      "Epoch 70: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.4986 - accuracy: 0.9566 - val_loss: 1.8338 - val_accuracy: 0.6990\n",
      "Epoch 71/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4901 - accuracy: 0.9596\n",
      "Epoch 71: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.4901 - accuracy: 0.9596 - val_loss: 3.8023 - val_accuracy: 0.5054\n",
      "Epoch 72/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4863 - accuracy: 0.9611\n",
      "Epoch 72: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.4863 - accuracy: 0.9611 - val_loss: 1.5987 - val_accuracy: 0.7096\n",
      "Epoch 73/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5000 - accuracy: 0.9549\n",
      "Epoch 73: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.5000 - accuracy: 0.9549 - val_loss: 2.1308 - val_accuracy: 0.6272\n",
      "Epoch 74/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4987 - accuracy: 0.9563\n",
      "Epoch 74: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 87ms/step - loss: 0.4987 - accuracy: 0.9563 - val_loss: 1.7259 - val_accuracy: 0.7078\n",
      "Epoch 75/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4986 - accuracy: 0.9576\n",
      "Epoch 75: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 85ms/step - loss: 0.4986 - accuracy: 0.9576 - val_loss: 1.9903 - val_accuracy: 0.6812\n",
      "Epoch 76/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4908 - accuracy: 0.9594\n",
      "Epoch 76: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.4908 - accuracy: 0.9594 - val_loss: 1.5839 - val_accuracy: 0.7307\n",
      "Epoch 77/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5009 - accuracy: 0.9559\n",
      "Epoch 77: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 84ms/step - loss: 0.5009 - accuracy: 0.9559 - val_loss: 2.9247 - val_accuracy: 0.5518\n",
      "Epoch 78/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4960 - accuracy: 0.9597\n",
      "Epoch 78: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.4960 - accuracy: 0.9597 - val_loss: 1.8579 - val_accuracy: 0.6935\n",
      "Epoch 79/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4987 - accuracy: 0.9577\n",
      "Epoch 79: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.4987 - accuracy: 0.9577 - val_loss: 1.6247 - val_accuracy: 0.7272\n",
      "Epoch 80/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4940 - accuracy: 0.9588\n",
      "Epoch 80: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.4940 - accuracy: 0.9588 - val_loss: 1.7075 - val_accuracy: 0.7094\n",
      "Epoch 81/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5197 - accuracy: 0.9512\n",
      "Epoch 81: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.5197 - accuracy: 0.9512 - val_loss: 3.3446 - val_accuracy: 0.2614\n",
      "Epoch 82/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5177 - accuracy: 0.9543\n",
      "Epoch 82: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.5177 - accuracy: 0.9543 - val_loss: 2.2151 - val_accuracy: 0.6567\n",
      "Epoch 83/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4834 - accuracy: 0.9636\n",
      "Epoch 83: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.4834 - accuracy: 0.9636 - val_loss: 1.5652 - val_accuracy: 0.7223\n",
      "Epoch 84/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4931 - accuracy: 0.9603\n",
      "Epoch 84: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.4931 - accuracy: 0.9603 - val_loss: 1.5864 - val_accuracy: 0.7149\n",
      "Epoch 85/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5042 - accuracy: 0.9561\n",
      "Epoch 85: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.5042 - accuracy: 0.9561 - val_loss: 1.5727 - val_accuracy: 0.7346\n",
      "Epoch 86/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4906 - accuracy: 0.9604\n",
      "Epoch 86: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 85ms/step - loss: 0.4906 - accuracy: 0.9604 - val_loss: 1.8904 - val_accuracy: 0.6946\n",
      "Epoch 87/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5005 - accuracy: 0.9583\n",
      "Epoch 87: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 85ms/step - loss: 0.5005 - accuracy: 0.9583 - val_loss: 1.5270 - val_accuracy: 0.7379\n",
      "Epoch 88/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4919 - accuracy: 0.9613\n",
      "Epoch 88: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 85ms/step - loss: 0.4919 - accuracy: 0.9613 - val_loss: 2.1107 - val_accuracy: 0.6706\n",
      "Epoch 89/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4871 - accuracy: 0.9622\n",
      "Epoch 89: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 87ms/step - loss: 0.4871 - accuracy: 0.9622 - val_loss: 1.8038 - val_accuracy: 0.6919\n",
      "Epoch 90/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4839 - accuracy: 0.9635\n",
      "Epoch 90: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 85ms/step - loss: 0.4839 - accuracy: 0.9635 - val_loss: 2.3483 - val_accuracy: 0.6554\n",
      "Epoch 91/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4919 - accuracy: 0.9605\n",
      "Epoch 91: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 85ms/step - loss: 0.4919 - accuracy: 0.9605 - val_loss: 2.1100 - val_accuracy: 0.6903\n",
      "Epoch 92/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4972 - accuracy: 0.9586\n",
      "Epoch 92: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 85ms/step - loss: 0.4972 - accuracy: 0.9586 - val_loss: 1.6141 - val_accuracy: 0.7018\n",
      "Epoch 93/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4944 - accuracy: 0.9602\n",
      "Epoch 93: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 85ms/step - loss: 0.4944 - accuracy: 0.9602 - val_loss: 1.5596 - val_accuracy: 0.7367\n",
      "Epoch 94/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4969 - accuracy: 0.9599\n",
      "Epoch 94: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 85ms/step - loss: 0.4969 - accuracy: 0.9599 - val_loss: 2.0728 - val_accuracy: 0.6707\n",
      "Epoch 95/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4921 - accuracy: 0.9603\n",
      "Epoch 95: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 85ms/step - loss: 0.4921 - accuracy: 0.9603 - val_loss: 1.5721 - val_accuracy: 0.7400\n",
      "Epoch 96/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4823 - accuracy: 0.9636\n",
      "Epoch 96: val_loss did not improve from 1.16181\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 85ms/step - loss: 0.4823 - accuracy: 0.9636 - val_loss: 2.3664 - val_accuracy: 0.6585\n",
      "Epoch 97/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5098 - accuracy: 0.9549\n",
      "Epoch 97: val_loss did not improve from 1.16181\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 25s 85ms/step - loss: 0.5098 - accuracy: 0.9549 - val_loss: 1.4150 - val_accuracy: 0.7525\n",
      "Epoch 98/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4426 - accuracy: 0.9816\n",
      "Epoch 98: val_loss did not improve from 1.16181\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 25s 85ms/step - loss: 0.4426 - accuracy: 0.9816 - val_loss: 1.1764 - val_accuracy: 0.8086\n",
      "Epoch 99/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3940 - accuracy: 0.9988\n",
      "Epoch 99: val_loss did not improve from 1.16181\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 25s 85ms/step - loss: 0.3940 - accuracy: 0.9988 - val_loss: 1.1774 - val_accuracy: 0.8140\n",
      "Epoch 100/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3844 - accuracy: 0.9998\n",
      "Epoch 100: val_loss did not improve from 1.16181\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 25s 85ms/step - loss: 0.3844 - accuracy: 0.9998 - val_loss: 1.1871 - val_accuracy: 0.8158\n",
      "Epoch 101/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3778 - accuracy: 1.0000\n",
      "Epoch 101: val_loss did not improve from 1.16181\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 25s 85ms/step - loss: 0.3778 - accuracy: 1.0000 - val_loss: 1.1923 - val_accuracy: 0.8166\n",
      "Epoch 102/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3723 - accuracy: 1.0000\n",
      "Epoch 102: val_loss did not improve from 1.16181\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 25s 85ms/step - loss: 0.3723 - accuracy: 1.0000 - val_loss: 1.1983 - val_accuracy: 0.8174\n",
      "Epoch 103/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3674 - accuracy: 0.9999\n",
      "Epoch 103: val_loss did not improve from 1.16181\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 25s 85ms/step - loss: 0.3674 - accuracy: 0.9999 - val_loss: 1.2011 - val_accuracy: 0.8177\n",
      "Epoch 104/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3626 - accuracy: 1.0000\n",
      "Epoch 104: val_loss did not improve from 1.16181\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 25s 85ms/step - loss: 0.3626 - accuracy: 1.0000 - val_loss: 1.2040 - val_accuracy: 0.8185\n",
      "Epoch 105/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3581 - accuracy: 1.0000\n",
      "Epoch 105: val_loss did not improve from 1.16181\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 25s 85ms/step - loss: 0.3581 - accuracy: 1.0000 - val_loss: 1.2105 - val_accuracy: 0.8187\n",
      "Epoch 106/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3535 - accuracy: 1.0000\n",
      "Epoch 106: val_loss did not improve from 1.16181\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.3535 - accuracy: 1.0000 - val_loss: 1.2137 - val_accuracy: 0.8195\n",
      "Epoch 107/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3492 - accuracy: 1.0000\n",
      "Epoch 107: val_loss did not improve from 1.16181\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.3492 - accuracy: 1.0000 - val_loss: 1.2159 - val_accuracy: 0.8192\n",
      "Epoch 108/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3450 - accuracy: 1.0000\n",
      "Epoch 108: val_loss did not improve from 1.16181\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.3450 - accuracy: 1.0000 - val_loss: 1.2173 - val_accuracy: 0.8204\n",
      "Epoch 109/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3408 - accuracy: 1.0000\n",
      "Epoch 109: val_loss did not improve from 1.16181\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 25s 85ms/step - loss: 0.3408 - accuracy: 1.0000 - val_loss: 1.2182 - val_accuracy: 0.8213\n",
      "Epoch 110/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3367 - accuracy: 1.0000\n",
      "Epoch 110: val_loss did not improve from 1.16181\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 25s 85ms/step - loss: 0.3367 - accuracy: 1.0000 - val_loss: 1.2186 - val_accuracy: 0.8209\n",
      "Epoch 111/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3327 - accuracy: 1.0000\n",
      "Epoch 111: val_loss did not improve from 1.16181\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 25s 85ms/step - loss: 0.3327 - accuracy: 1.0000 - val_loss: 1.2201 - val_accuracy: 0.8207\n",
      "Epoch 112/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3288 - accuracy: 1.0000\n",
      "Epoch 112: val_loss did not improve from 1.16181\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 25s 87ms/step - loss: 0.3288 - accuracy: 1.0000 - val_loss: 1.2203 - val_accuracy: 0.8214\n",
      "Epoch 113/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3249 - accuracy: 1.0000\n",
      "Epoch 113: val_loss did not improve from 1.16181\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 25s 85ms/step - loss: 0.3249 - accuracy: 1.0000 - val_loss: 1.2213 - val_accuracy: 0.8214\n",
      "Epoch 114/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3211 - accuracy: 1.0000\n",
      "Epoch 114: val_loss did not improve from 1.16181\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 25s 85ms/step - loss: 0.3211 - accuracy: 1.0000 - val_loss: 1.2218 - val_accuracy: 0.8216\n",
      "Epoch 115/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3172 - accuracy: 1.0000\n",
      "Epoch 115: val_loss did not improve from 1.16181\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 26s 87ms/step - loss: 0.3172 - accuracy: 1.0000 - val_loss: 1.2207 - val_accuracy: 0.8216\n",
      "Epoch 116/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3135 - accuracy: 1.0000\n",
      "Epoch 116: val_loss did not improve from 1.16181\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 25s 85ms/step - loss: 0.3135 - accuracy: 1.0000 - val_loss: 1.2230 - val_accuracy: 0.8218\n",
      "Epoch 117/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3098 - accuracy: 1.0000\n",
      "Epoch 117: val_loss did not improve from 1.16181\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 25s 85ms/step - loss: 0.3098 - accuracy: 1.0000 - val_loss: 1.2226 - val_accuracy: 0.8222\n",
      "Epoch 118/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3061 - accuracy: 1.0000\n",
      "Epoch 118: val_loss did not improve from 1.16181\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 25s 85ms/step - loss: 0.3061 - accuracy: 1.0000 - val_loss: 1.2224 - val_accuracy: 0.8222\n",
      "Epoch 119/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3026 - accuracy: 1.0000\n",
      "Epoch 119: val_loss did not improve from 1.16181\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 25s 85ms/step - loss: 0.3026 - accuracy: 1.0000 - val_loss: 1.2232 - val_accuracy: 0.8218\n",
      "Epoch 120/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2990 - accuracy: 1.0000\n",
      "Epoch 120: val_loss did not improve from 1.16181\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 25s 85ms/step - loss: 0.2990 - accuracy: 1.0000 - val_loss: 1.2210 - val_accuracy: 0.8219\n",
      "Epoch 121/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2955 - accuracy: 1.0000\n",
      "Epoch 121: val_loss did not improve from 1.16181\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 25s 85ms/step - loss: 0.2955 - accuracy: 1.0000 - val_loss: 1.2212 - val_accuracy: 0.8214\n",
      "Epoch 122/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2921 - accuracy: 1.0000\n",
      "Epoch 122: val_loss did not improve from 1.16181\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 25s 85ms/step - loss: 0.2921 - accuracy: 1.0000 - val_loss: 1.2217 - val_accuracy: 0.8215\n",
      "Epoch 123/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2887 - accuracy: 1.0000\n",
      "Epoch 123: val_loss did not improve from 1.16181\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 25s 85ms/step - loss: 0.2887 - accuracy: 1.0000 - val_loss: 1.2198 - val_accuracy: 0.8220\n",
      "Epoch 124/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2853 - accuracy: 1.0000\n",
      "Epoch 124: val_loss did not improve from 1.16181\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 25s 85ms/step - loss: 0.2853 - accuracy: 1.0000 - val_loss: 1.2191 - val_accuracy: 0.8222\n",
      "Epoch 125/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2820 - accuracy: 1.0000\n",
      "Epoch 125: val_loss did not improve from 1.16181\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 25s 85ms/step - loss: 0.2820 - accuracy: 1.0000 - val_loss: 1.2191 - val_accuracy: 0.8219\n",
      "Epoch 126/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2786 - accuracy: 1.0000\n",
      "Epoch 126: val_loss did not improve from 1.16181\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 0.2786 - accuracy: 1.0000 - val_loss: 1.2186 - val_accuracy: 0.8214\n",
      "Epoch 127/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2754 - accuracy: 1.0000\n",
      "Epoch 127: val_loss did not improve from 1.16181\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 0.2754 - accuracy: 1.0000 - val_loss: 1.2175 - val_accuracy: 0.8226\n",
      "Epoch 128/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2722 - accuracy: 1.0000\n",
      "Epoch 128: val_loss did not improve from 1.16181\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 0.2722 - accuracy: 1.0000 - val_loss: 1.2178 - val_accuracy: 0.8231\n",
      "Epoch 129/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2690 - accuracy: 1.0000\n",
      "Epoch 129: val_loss did not improve from 1.16181\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 26s 88ms/step - loss: 0.2690 - accuracy: 1.0000 - val_loss: 1.2172 - val_accuracy: 0.8228\n",
      "Epoch 130/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2658 - accuracy: 1.0000\n",
      "Epoch 130: val_loss did not improve from 1.16181\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 26s 88ms/step - loss: 0.2658 - accuracy: 1.0000 - val_loss: 1.2179 - val_accuracy: 0.8226\n",
      "Epoch 131/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2627 - accuracy: 1.0000\n",
      "Epoch 131: val_loss did not improve from 1.16181\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 26s 88ms/step - loss: 0.2627 - accuracy: 1.0000 - val_loss: 1.2180 - val_accuracy: 0.8223\n",
      "Epoch 132/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2597 - accuracy: 1.0000\n",
      "Epoch 132: val_loss did not improve from 1.16181\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 26s 88ms/step - loss: 0.2597 - accuracy: 1.0000 - val_loss: 1.2160 - val_accuracy: 0.8222\n",
      "Epoch 133/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2567 - accuracy: 1.0000\n",
      "Epoch 133: val_loss did not improve from 1.16181\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 0.2567 - accuracy: 1.0000 - val_loss: 1.2169 - val_accuracy: 0.8225\n",
      "Epoch 134/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2536 - accuracy: 1.0000\n",
      "Epoch 134: val_loss did not improve from 1.16181\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 0.2536 - accuracy: 1.0000 - val_loss: 1.2156 - val_accuracy: 0.8220\n",
      "Epoch 135/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2507 - accuracy: 1.0000\n",
      "Epoch 135: val_loss did not improve from 1.16181\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 25s 85ms/step - loss: 0.2507 - accuracy: 1.0000 - val_loss: 1.2142 - val_accuracy: 0.8226\n",
      "Epoch 136/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2478 - accuracy: 1.0000\n",
      "Epoch 136: val_loss did not improve from 1.16181\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 25s 85ms/step - loss: 0.2478 - accuracy: 1.0000 - val_loss: 1.2139 - val_accuracy: 0.8228\n",
      "Epoch 137/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2449 - accuracy: 1.0000\n",
      "Epoch 137: val_loss did not improve from 1.16181\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 25s 85ms/step - loss: 0.2449 - accuracy: 1.0000 - val_loss: 1.2136 - val_accuracy: 0.8232\n",
      "Epoch 138/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2421 - accuracy: 1.0000\n",
      "Epoch 138: val_loss did not improve from 1.16181\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 0.2421 - accuracy: 1.0000 - val_loss: 1.2134 - val_accuracy: 0.8228\n",
      "Epoch 139/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2392 - accuracy: 1.0000\n",
      "Epoch 139: val_loss did not improve from 1.16181\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 0.2392 - accuracy: 1.0000 - val_loss: 1.2122 - val_accuracy: 0.8233\n",
      "Epoch 140/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2364 - accuracy: 1.0000\n",
      "Epoch 140: val_loss did not improve from 1.16181\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 25s 85ms/step - loss: 0.2364 - accuracy: 1.0000 - val_loss: 1.2121 - val_accuracy: 0.8226\n",
      "Epoch 141/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2337 - accuracy: 1.0000\n",
      "Epoch 141: val_loss did not improve from 1.16181\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 25s 85ms/step - loss: 0.2337 - accuracy: 1.0000 - val_loss: 1.2115 - val_accuracy: 0.8222\n",
      "Epoch 142/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2310 - accuracy: 1.0000\n",
      "Epoch 142: val_loss did not improve from 1.16181\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 0.2310 - accuracy: 1.0000 - val_loss: 1.2104 - val_accuracy: 0.8227\n",
      "Epoch 143/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2283 - accuracy: 1.0000\n",
      "Epoch 143: val_loss did not improve from 1.16181\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 0.2283 - accuracy: 1.0000 - val_loss: 1.2073 - val_accuracy: 0.8233\n",
      "Epoch 144/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2256 - accuracy: 1.0000\n",
      "Epoch 144: val_loss did not improve from 1.16181\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 0.2256 - accuracy: 1.0000 - val_loss: 1.2091 - val_accuracy: 0.8235\n",
      "Epoch 145/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2230 - accuracy: 1.0000\n",
      "Epoch 145: val_loss did not improve from 1.16181\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 25s 85ms/step - loss: 0.2230 - accuracy: 1.0000 - val_loss: 1.2098 - val_accuracy: 0.8229\n",
      "Epoch 146/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2215 - accuracy: 1.0000\n",
      "Epoch 146: val_loss did not improve from 1.16181\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 0.2215 - accuracy: 1.0000 - val_loss: 1.2112 - val_accuracy: 0.8232\n",
      "Epoch 147/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2212 - accuracy: 1.0000\n",
      "Epoch 147: val_loss did not improve from 1.16181\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 0.2212 - accuracy: 1.0000 - val_loss: 1.2119 - val_accuracy: 0.8238\n",
      "Epoch 148/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2210 - accuracy: 1.0000\n",
      "Epoch 148: val_loss did not improve from 1.16181\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 25s 85ms/step - loss: 0.2210 - accuracy: 1.0000 - val_loss: 1.2121 - val_accuracy: 0.8236\n",
      "Epoch 149/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2207 - accuracy: 1.0000\n",
      "Epoch 149: val_loss did not improve from 1.16181\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 25s 85ms/step - loss: 0.2207 - accuracy: 1.0000 - val_loss: 1.2122 - val_accuracy: 0.8236\n",
      "Epoch 150/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2204 - accuracy: 1.0000\n",
      "Epoch 150: val_loss did not improve from 1.16181\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 0.2204 - accuracy: 1.0000 - val_loss: 1.2130 - val_accuracy: 0.8234\n",
      "Epoch 151/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2202 - accuracy: 1.0000\n",
      "Epoch 151: val_loss did not improve from 1.16181\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 0.2202 - accuracy: 1.0000 - val_loss: 1.2123 - val_accuracy: 0.8236\n",
      "Epoch 152/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2199 - accuracy: 1.0000\n",
      "Epoch 152: val_loss did not improve from 1.16181\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 0.2199 - accuracy: 1.0000 - val_loss: 1.2112 - val_accuracy: 0.8237\n",
      "Epoch 153/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2197 - accuracy: 1.0000\n",
      "Epoch 153: val_loss did not improve from 1.16181\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 25s 85ms/step - loss: 0.2197 - accuracy: 1.0000 - val_loss: 1.2121 - val_accuracy: 0.8235\n",
      "Epoch 154/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2194 - accuracy: 1.0000\n",
      "Epoch 154: val_loss did not improve from 1.16181\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 0.2194 - accuracy: 1.0000 - val_loss: 1.2114 - val_accuracy: 0.8236\n",
      "Epoch 155/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2191 - accuracy: 1.0000\n",
      "Epoch 155: val_loss did not improve from 1.16181\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 0.2191 - accuracy: 1.0000 - val_loss: 1.2124 - val_accuracy: 0.8236\n",
      "Epoch 156/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2189 - accuracy: 1.0000\n",
      "Epoch 156: val_loss did not improve from 1.16181\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 0.2189 - accuracy: 1.0000 - val_loss: 1.2122 - val_accuracy: 0.8242\n",
      "Epoch 157/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2186 - accuracy: 1.0000\n",
      "Epoch 157: val_loss did not improve from 1.16181\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 0.2186 - accuracy: 1.0000 - val_loss: 1.2114 - val_accuracy: 0.8238\n",
      "Epoch 158/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2184 - accuracy: 1.0000\n",
      "Epoch 158: val_loss did not improve from 1.16181\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 0.2184 - accuracy: 1.0000 - val_loss: 1.2128 - val_accuracy: 0.8237\n",
      "Epoch 159/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2181 - accuracy: 1.0000\n",
      "Epoch 159: val_loss did not improve from 1.16181\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 0.2181 - accuracy: 1.0000 - val_loss: 1.2116 - val_accuracy: 0.8236\n",
      "Epoch 160/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2179 - accuracy: 1.0000\n",
      "Epoch 160: val_loss did not improve from 1.16181\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 0.2179 - accuracy: 1.0000 - val_loss: 1.2119 - val_accuracy: 0.8233\n",
      "Epoch 161/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2176 - accuracy: 1.0000\n",
      "Epoch 161: val_loss did not improve from 1.16181\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 0.2176 - accuracy: 1.0000 - val_loss: 1.2116 - val_accuracy: 0.8234\n",
      "Epoch 162/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2174 - accuracy: 1.0000\n",
      "Epoch 162: val_loss did not improve from 1.16181\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 0.2174 - accuracy: 1.0000 - val_loss: 1.2114 - val_accuracy: 0.8235\n",
      "Epoch 163/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2171 - accuracy: 1.0000\n",
      "Epoch 163: val_loss did not improve from 1.16181\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 0.2171 - accuracy: 1.0000 - val_loss: 1.2111 - val_accuracy: 0.8232\n",
      "Epoch 164/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2168 - accuracy: 1.0000\n",
      "Epoch 164: val_loss did not improve from 1.16181\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 26s 88ms/step - loss: 0.2168 - accuracy: 1.0000 - val_loss: 1.2101 - val_accuracy: 0.8237\n",
      "Epoch 165/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2166 - accuracy: 1.0000\n",
      "Epoch 165: val_loss did not improve from 1.16181\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 0.2166 - accuracy: 1.0000 - val_loss: 1.2119 - val_accuracy: 0.8236\n",
      "Epoch 166/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2164 - accuracy: 1.0000\n",
      "Epoch 166: val_loss did not improve from 1.16181\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 0.2164 - accuracy: 1.0000 - val_loss: 1.2117 - val_accuracy: 0.8234\n",
      "Epoch 167/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2161 - accuracy: 1.0000\n",
      "Epoch 167: val_loss did not improve from 1.16181\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 0.2161 - accuracy: 1.0000 - val_loss: 1.2111 - val_accuracy: 0.8233\n",
      "Epoch 168/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2158 - accuracy: 1.0000\n",
      "Epoch 168: val_loss did not improve from 1.16181\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 0.2158 - accuracy: 1.0000 - val_loss: 1.2105 - val_accuracy: 0.8238\n",
      "Epoch 169/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2156 - accuracy: 1.0000\n",
      "Epoch 169: val_loss did not improve from 1.16181\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 0.2156 - accuracy: 1.0000 - val_loss: 1.2102 - val_accuracy: 0.8238\n",
      "Epoch 170/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2153 - accuracy: 1.0000\n",
      "Epoch 170: val_loss did not improve from 1.16181\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 0.2153 - accuracy: 1.0000 - val_loss: 1.2105 - val_accuracy: 0.8234\n",
      "Epoch 171/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2151 - accuracy: 1.0000\n",
      "Epoch 171: val_loss did not improve from 1.16181\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 0.2151 - accuracy: 1.0000 - val_loss: 1.2108 - val_accuracy: 0.8235\n",
      "Epoch 172/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2149 - accuracy: 1.0000\n",
      "Epoch 172: val_loss did not improve from 1.16181\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 0.2149 - accuracy: 1.0000 - val_loss: 1.2117 - val_accuracy: 0.8235\n",
      "Epoch 173/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2146 - accuracy: 1.0000\n",
      "Epoch 173: val_loss did not improve from 1.16181\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 0.2146 - accuracy: 1.0000 - val_loss: 1.2113 - val_accuracy: 0.8235\n",
      "Epoch 174/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2143 - accuracy: 1.0000\n",
      "Epoch 174: val_loss did not improve from 1.16181\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 25s 87ms/step - loss: 0.2143 - accuracy: 1.0000 - val_loss: 1.2111 - val_accuracy: 0.8233\n",
      "Epoch 175/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2141 - accuracy: 1.0000\n",
      "Epoch 175: val_loss did not improve from 1.16181\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 0.2141 - accuracy: 1.0000 - val_loss: 1.2119 - val_accuracy: 0.8233\n",
      "Epoch 176/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2138 - accuracy: 1.0000\n",
      "Epoch 176: val_loss did not improve from 1.16181\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 0.2138 - accuracy: 1.0000 - val_loss: 1.2112 - val_accuracy: 0.8235\n",
      "Epoch 177/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2136 - accuracy: 1.0000\n",
      "Epoch 177: val_loss did not improve from 1.16181\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 0.2136 - accuracy: 1.0000 - val_loss: 1.2106 - val_accuracy: 0.8235\n",
      "Epoch 178/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2133 - accuracy: 1.0000\n",
      "Epoch 178: val_loss did not improve from 1.16181\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 0.2133 - accuracy: 1.0000 - val_loss: 1.2117 - val_accuracy: 0.8234\n",
      "Epoch 179/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2131 - accuracy: 1.0000\n",
      "Epoch 179: val_loss did not improve from 1.16181\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 0.2131 - accuracy: 1.0000 - val_loss: 1.2112 - val_accuracy: 0.8235\n",
      "Epoch 180/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2128 - accuracy: 1.0000\n",
      "Epoch 180: val_loss did not improve from 1.16181\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 0.2128 - accuracy: 1.0000 - val_loss: 1.2103 - val_accuracy: 0.8235\n",
      "Epoch 181/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2126 - accuracy: 1.0000\n",
      "Epoch 181: val_loss did not improve from 1.16181\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 0.2126 - accuracy: 1.0000 - val_loss: 1.2110 - val_accuracy: 0.8231\n",
      "Epoch 182/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2123 - accuracy: 1.0000\n",
      "Epoch 182: val_loss did not improve from 1.16181\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 0.2123 - accuracy: 1.0000 - val_loss: 1.2109 - val_accuracy: 0.8238\n",
      "Epoch 183/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2121 - accuracy: 1.0000\n",
      "Epoch 183: val_loss did not improve from 1.16181\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 0.2121 - accuracy: 1.0000 - val_loss: 1.2103 - val_accuracy: 0.8234\n",
      "Epoch 184/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2118 - accuracy: 1.0000\n",
      "Epoch 184: val_loss did not improve from 1.16181\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 0.2118 - accuracy: 1.0000 - val_loss: 1.2109 - val_accuracy: 0.8239\n",
      "Epoch 185/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2116 - accuracy: 1.0000\n",
      "Epoch 185: val_loss did not improve from 1.16181\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 0.2116 - accuracy: 1.0000 - val_loss: 1.2122 - val_accuracy: 0.8232\n",
      "Epoch 186/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2113 - accuracy: 1.0000\n",
      "Epoch 186: val_loss did not improve from 1.16181\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 0.2113 - accuracy: 1.0000 - val_loss: 1.2099 - val_accuracy: 0.8235\n",
      "Epoch 187/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2111 - accuracy: 1.0000\n",
      "Epoch 187: val_loss did not improve from 1.16181\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 25s 87ms/step - loss: 0.2111 - accuracy: 1.0000 - val_loss: 1.2107 - val_accuracy: 0.8240\n",
      "Epoch 188/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2108 - accuracy: 1.0000\n",
      "Epoch 188: val_loss did not improve from 1.16181\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 0.2108 - accuracy: 1.0000 - val_loss: 1.2115 - val_accuracy: 0.8232\n",
      "Epoch 189/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2106 - accuracy: 1.0000\n",
      "Epoch 189: val_loss did not improve from 1.16181\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 25s 87ms/step - loss: 0.2106 - accuracy: 1.0000 - val_loss: 1.2101 - val_accuracy: 0.8238\n",
      "Epoch 190/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2103 - accuracy: 1.0000\n",
      "Epoch 190: val_loss did not improve from 1.16181\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 25s 86ms/step - loss: 0.2103 - accuracy: 1.0000 - val_loss: 1.2104 - val_accuracy: 0.8234\n",
      "Epoch 191/191\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2101 - accuracy: 1.0000\n",
      "Epoch 191: val_loss did not improve from 1.16181\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 25s 87ms/step - loss: 0.2101 - accuracy: 1.0000 - val_loss: 1.2097 - val_accuracy: 0.8231\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TRAIN\n",
    "\n",
    "if os.path.isfile(cifar_teacher_path):\n",
    "    cifar_teacher_model.load_weights(cifar_teacher_path)\n",
    "else:\n",
    "    cifar_history = cifar_teacher_model.fit(cifar_train_x, cifar_train_y,\n",
    "                    batch_size=batch_size, \n",
    "                    epochs=num_epoch, \n",
    "                    validation_data=(cifar_val_x, cifar_val_y), \n",
    "                    verbose=1,                    \n",
    "                    callbacks=cifar_teacher_callbacks)\n",
    "\n",
    "    plt.plot(cifar_history.history[\"accuracy\"], label = 'Train Accuracy')\n",
    "    plt.plot(cifar_history.history[\"val_accuracy\"], linestyle = 'dashed', label = 'Test Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55db92e2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "55db92e2",
    "outputId": "e53d0a74-a960-430a-c80a-4a8efc5845d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 10ms/step - loss: 1.2503 - accuracy: 0.8159\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.250322699546814, 0.8159000277519226]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar_teacher_model.evaluate(cifar_test_x, cifar_test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db04b76",
   "metadata": {
    "id": "7db04b76"
   },
   "source": [
    "### Reduced Teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2bd84916",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2bd84916",
    "outputId": "91b1a40a-e13c-4243-b252-fd25954cb01c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_2 (Flatten)         (None, 3072)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 789,258\n",
      "Trainable params: 789,258\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cifar_reduced_teacher = create_dense_nn((32, 32, 3), 10)\n",
    "cifar_reduced_teacher.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b08b4c",
   "metadata": {
    "id": "c0b08b4c"
   },
   "source": [
    "### CIFAR Teacher - Reduced Teacher Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80f3787d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "80f3787d",
    "outputId": "a4c3c718-25ed-4c53-e06e-77dc01268f49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1172/1172 [==============================] - 47s 35ms/step - sparse_categorical_accuracy: 0.9000 - student_loss: 1.5682 - distillation_loss: 0.1324 - val_sparse_categorical_accuracy: 0.7663 - val_student_loss: 1.7147\n",
      "Epoch 2/20\n",
      "1172/1172 [==============================] - 39s 33ms/step - sparse_categorical_accuracy: 0.9071 - student_loss: 1.5595 - distillation_loss: 0.1324 - val_sparse_categorical_accuracy: 0.7110 - val_student_loss: 1.6762\n",
      "Epoch 3/20\n",
      "1172/1172 [==============================] - 39s 33ms/step - sparse_categorical_accuracy: 0.9167 - student_loss: 1.5493 - distillation_loss: 0.1326 - val_sparse_categorical_accuracy: 0.7645 - val_student_loss: 1.7162\n",
      "Epoch 4/20\n",
      "1172/1172 [==============================] - 40s 34ms/step - sparse_categorical_accuracy: 0.9273 - student_loss: 1.5387 - distillation_loss: 0.1326 - val_sparse_categorical_accuracy: 0.7693 - val_student_loss: 1.6730\n",
      "Epoch 5/20\n",
      "1172/1172 [==============================] - 39s 33ms/step - sparse_categorical_accuracy: 0.9347 - student_loss: 1.5314 - distillation_loss: 0.1327 - val_sparse_categorical_accuracy: 0.7634 - val_student_loss: 1.6816\n",
      "Epoch 6/20\n",
      "1172/1172 [==============================] - 39s 33ms/step - sparse_categorical_accuracy: 0.9425 - student_loss: 1.5238 - distillation_loss: 0.1327 - val_sparse_categorical_accuracy: 0.7596 - val_student_loss: 1.8306\n",
      "Epoch 7/20\n",
      "1172/1172 [==============================] - 39s 33ms/step - sparse_categorical_accuracy: 0.9469 - student_loss: 1.5192 - distillation_loss: 0.1328 - val_sparse_categorical_accuracy: 0.7522 - val_student_loss: 1.6609\n",
      "Epoch 8/20\n",
      "1172/1172 [==============================] - 39s 33ms/step - sparse_categorical_accuracy: 0.9505 - student_loss: 1.5152 - distillation_loss: 0.1328 - val_sparse_categorical_accuracy: 0.7653 - val_student_loss: 1.7736\n",
      "Epoch 9/20\n",
      "1172/1172 [==============================] - 42s 36ms/step - sparse_categorical_accuracy: 0.9557 - student_loss: 1.5096 - distillation_loss: 0.1328 - val_sparse_categorical_accuracy: 0.7425 - val_student_loss: 1.8413\n",
      "Epoch 10/20\n",
      "1172/1172 [==============================] - 39s 34ms/step - sparse_categorical_accuracy: 0.9599 - student_loss: 1.5060 - distillation_loss: 0.1328 - val_sparse_categorical_accuracy: 0.7748 - val_student_loss: 1.6997\n",
      "Epoch 11/20\n",
      "1172/1172 [==============================] - 40s 34ms/step - sparse_categorical_accuracy: 0.9596 - student_loss: 1.5049 - distillation_loss: 0.1329 - val_sparse_categorical_accuracy: 0.7758 - val_student_loss: 1.7345\n",
      "Epoch 12/20\n",
      "1172/1172 [==============================] - 39s 34ms/step - sparse_categorical_accuracy: 0.9651 - student_loss: 1.5005 - distillation_loss: 0.1329 - val_sparse_categorical_accuracy: 0.7836 - val_student_loss: 1.7063\n",
      "Epoch 13/20\n",
      "1172/1172 [==============================] - 39s 34ms/step - sparse_categorical_accuracy: 0.9672 - student_loss: 1.4976 - distillation_loss: 0.1329 - val_sparse_categorical_accuracy: 0.7770 - val_student_loss: 1.7838\n",
      "Epoch 14/20\n",
      "1172/1172 [==============================] - 39s 33ms/step - sparse_categorical_accuracy: 0.9666 - student_loss: 1.4985 - distillation_loss: 0.1329 - val_sparse_categorical_accuracy: 0.7738 - val_student_loss: 1.6234\n",
      "Epoch 15/20\n",
      "1172/1172 [==============================] - 39s 34ms/step - sparse_categorical_accuracy: 0.9711 - student_loss: 1.4937 - distillation_loss: 0.1329 - val_sparse_categorical_accuracy: 0.7806 - val_student_loss: 1.6816\n",
      "Epoch 16/20\n",
      "1172/1172 [==============================] - 39s 34ms/step - sparse_categorical_accuracy: 0.9713 - student_loss: 1.4930 - distillation_loss: 0.1329 - val_sparse_categorical_accuracy: 0.7866 - val_student_loss: 1.6195\n",
      "Epoch 17/20\n",
      "1172/1172 [==============================] - 40s 34ms/step - sparse_categorical_accuracy: 0.9733 - student_loss: 1.4907 - distillation_loss: 0.1329 - val_sparse_categorical_accuracy: 0.7934 - val_student_loss: 1.6796\n",
      "Epoch 18/20\n",
      "1172/1172 [==============================] - 40s 34ms/step - sparse_categorical_accuracy: 0.9753 - student_loss: 1.4889 - distillation_loss: 0.1330 - val_sparse_categorical_accuracy: 0.7814 - val_student_loss: 1.7255\n",
      "Epoch 19/20\n",
      "1172/1172 [==============================] - 40s 34ms/step - sparse_categorical_accuracy: 0.9778 - student_loss: 1.4859 - distillation_loss: 0.1330 - val_sparse_categorical_accuracy: 0.7788 - val_student_loss: 1.6298\n",
      "Epoch 20/20\n",
      "1172/1172 [==============================] - 40s 34ms/step - sparse_categorical_accuracy: 0.9777 - student_loss: 1.4859 - distillation_loss: 0.1330 - val_sparse_categorical_accuracy: 0.7835 - val_student_loss: 1.7694\n",
      "313/313 [==============================] - 3s 9ms/step - sparse_categorical_accuracy: 0.7712 - student_loss: 1.6885\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cifar_distiller = Distiller(student=cifar_teacher_model, teacher=cifar_reduced_teacher)\n",
    "cifar_distiller.compile(\n",
    "    optimizer=Adam(),\n",
    "    metrics=[SparseCategoricalAccuracy()],\n",
    "    student_loss_fn=SparseCategoricalCrossentropy(from_logits=True),\n",
    "    distillation_loss_fn=KLDivergence(),\n",
    "    alpha=0.2,\n",
    "    temperature=3,\n",
    ")\n",
    "\n",
    "# Distill teacher to student\n",
    "cifar_reduced_teacher_history = cifar_distiller.fit(cifar_train_x, cifar_train_y, validation_data=(cifar_val_x, cifar_val_y), epochs=20)\n",
    "\n",
    "# Evaluate student on test dataset\n",
    "cifar_distiller.evaluate(cifar_test_x, cifar_test_y)\n",
    "\n",
    "plt.plot(cifar_reduced_teacher_history.history[\"sparse_categorical_accuracy\"], label = 'Train Accuracy')\n",
    "plt.plot(cifar_reduced_teacher_history.history[\"val_sparse_categorical_accuracy\"], linestyle = 'dashed', label = 'Test Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0198aef",
   "metadata": {
    "id": "b0198aef"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ca9c895",
   "metadata": {
    "id": "0ca9c895"
   },
   "source": [
    "## EMINIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "76add38e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "76add38e",
    "outputId": "c1b8dbdd-31d0-421b-db8b-edaa54268169"
   },
   "outputs": [],
   "source": [
    "(emnist_train_x, emnist_train_y) = em.extract_training_samples('letters')\n",
    "emnist_train_x, emnist_train_y = shuffle(emnist_train_x, emnist_train_y)\n",
    "(emnist_test_x, emnist_test_y) = em.extract_test_samples('letters')\n",
    "emnist_train_x = emnist_train_x.reshape((emnist_train_x.shape[0], 28, 28, 1))\n",
    "emnist_test_x = emnist_test_x.reshape((emnist_test_x.shape[0], 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "58f18c97",
   "metadata": {
    "id": "58f18c97"
   },
   "outputs": [],
   "source": [
    "emnist_train_x = emnist_train_x.astype('float32')\n",
    "emnist_test_x = emnist_test_x.astype('float32')\n",
    "emnist_train_x = emnist_train_x / 255.0\n",
    "emnist_test_x = emnist_test_x / 255.0\n",
    "\n",
    "emnist_train_x, emnist_val_x, emnist_train_y, emnist_val_y = train_test_split(emnist_train_x, emnist_train_y, \n",
    "    test_size=0.25, random_state= 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f471e6dd",
   "metadata": {
    "id": "f471e6dd"
   },
   "source": [
    "### Teacher "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c4dd5c9a",
   "metadata": {
    "id": "c4dd5c9a"
   },
   "outputs": [],
   "source": [
    "emnist_teacher_model = create_resnet((28, 28, 1), 27)\n",
    "emnist_teacher_model.type = \"emnist_resnet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8bdcd8d6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8bdcd8d6",
    "outputId": "bca510ba-de42-4b40-86e1-1d6bd5a93e1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leraning rate is controled by epoch.\n"
     ]
    }
   ],
   "source": [
    "emnist_teacher_model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "emnist_teacher_path = \"ResNet-for-EMNIST.h5\"\n",
    "emnist_teacher_checkpoint = ModelCheckpoint(filepath = emnist_teacher_path, monitor=\"val_loss\", verbose=1, save_best_only=True)\n",
    "emnist_teacher_learning_controller = LearningController(num_epoch)\n",
    "emnist_teacher_callbacks = [emnist_teacher_checkpoint, emnist_teacher_learning_controller]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a48a7733",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "a48a7733",
    "outputId": "bbe98876-f8df-43f9-9825-bc56729f44c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.7620 - accuracy: 0.8948\n",
      "Epoch 1: val_loss improved from 1.51989 to 0.77015, saving model to ResNet-for-EMNIST.h5\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 52s 71ms/step - loss: 0.7620 - accuracy: 0.8949 - val_loss: 0.7702 - val_accuracy: 0.8890\n",
      "Epoch 2/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.6940 - accuracy: 0.9127\n",
      "Epoch 2: val_loss improved from 0.77015 to 0.70164, saving model to ResNet-for-EMNIST.h5\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 52s 72ms/step - loss: 0.6941 - accuracy: 0.9127 - val_loss: 0.7016 - val_accuracy: 0.9098\n",
      "Epoch 3/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.6569 - accuracy: 0.9230\n",
      "Epoch 3: val_loss improved from 0.70164 to 0.66223, saving model to ResNet-for-EMNIST.h5\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.6569 - accuracy: 0.9230 - val_loss: 0.6622 - val_accuracy: 0.9213\n",
      "Epoch 4/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.6326 - accuracy: 0.9287\n",
      "Epoch 4: val_loss improved from 0.66223 to 0.65566, saving model to ResNet-for-EMNIST.h5\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.6326 - accuracy: 0.9287 - val_loss: 0.6557 - val_accuracy: 0.9228\n",
      "Epoch 5/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.6145 - accuracy: 0.9344\n",
      "Epoch 5: val_loss improved from 0.65566 to 0.65185, saving model to ResNet-for-EMNIST.h5\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.6145 - accuracy: 0.9344 - val_loss: 0.6519 - val_accuracy: 0.9239\n",
      "Epoch 6/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.5996 - accuracy: 0.9381\n",
      "Epoch 6: val_loss did not improve from 0.65185\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 50s 69ms/step - loss: 0.5996 - accuracy: 0.9381 - val_loss: 0.6526 - val_accuracy: 0.9222\n",
      "Epoch 7/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.5878 - accuracy: 0.9416\n",
      "Epoch 7: val_loss improved from 0.65185 to 0.62529, saving model to ResNet-for-EMNIST.h5\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.5878 - accuracy: 0.9416 - val_loss: 0.6253 - val_accuracy: 0.9308\n",
      "Epoch 8/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.5781 - accuracy: 0.9441\n",
      "Epoch 8: val_loss improved from 0.62529 to 0.62126, saving model to ResNet-for-EMNIST.h5\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.5781 - accuracy: 0.9440 - val_loss: 0.6213 - val_accuracy: 0.9309\n",
      "Epoch 9/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.5679 - accuracy: 0.9473\n",
      "Epoch 9: val_loss did not improve from 0.62126\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 69ms/step - loss: 0.5679 - accuracy: 0.9473 - val_loss: 0.6300 - val_accuracy: 0.9264\n",
      "Epoch 10/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.5603 - accuracy: 0.9492\n",
      "Epoch 10: val_loss improved from 0.62126 to 0.62023, saving model to ResNet-for-EMNIST.h5\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.5603 - accuracy: 0.9492 - val_loss: 0.6202 - val_accuracy: 0.9315\n",
      "Epoch 11/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.5514 - accuracy: 0.9518\n",
      "Epoch 11: val_loss improved from 0.62023 to 0.60107, saving model to ResNet-for-EMNIST.h5\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.5515 - accuracy: 0.9518 - val_loss: 0.6011 - val_accuracy: 0.9362\n",
      "Epoch 12/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.5443 - accuracy: 0.9540\n",
      "Epoch 12: val_loss did not improve from 0.60107\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 52s 71ms/step - loss: 0.5443 - accuracy: 0.9540 - val_loss: 0.6161 - val_accuracy: 0.9314\n",
      "Epoch 13/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.5375 - accuracy: 0.9558\n",
      "Epoch 13: val_loss did not improve from 0.60107\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 69ms/step - loss: 0.5375 - accuracy: 0.9557 - val_loss: 0.6049 - val_accuracy: 0.9351\n",
      "Epoch 14/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.5325 - accuracy: 0.9563\n",
      "Epoch 14: val_loss improved from 0.60107 to 0.59038, saving model to ResNet-for-EMNIST.h5\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.5324 - accuracy: 0.9563 - val_loss: 0.5904 - val_accuracy: 0.9389\n",
      "Epoch 15/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.5247 - accuracy: 0.9590\n",
      "Epoch 15: val_loss did not improve from 0.59038\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 69ms/step - loss: 0.5247 - accuracy: 0.9590 - val_loss: 0.6006 - val_accuracy: 0.9352\n",
      "Epoch 16/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.5181 - accuracy: 0.9604\n",
      "Epoch 16: val_loss did not improve from 0.59038\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 69ms/step - loss: 0.5181 - accuracy: 0.9604 - val_loss: 0.6032 - val_accuracy: 0.9347\n",
      "Epoch 17/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.5128 - accuracy: 0.9618\n",
      "Epoch 17: val_loss did not improve from 0.59038\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 69ms/step - loss: 0.5128 - accuracy: 0.9618 - val_loss: 0.5947 - val_accuracy: 0.9361\n",
      "Epoch 18/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.5070 - accuracy: 0.9631\n",
      "Epoch 18: val_loss improved from 0.59038 to 0.58919, saving model to ResNet-for-EMNIST.h5\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.5070 - accuracy: 0.9631 - val_loss: 0.5892 - val_accuracy: 0.9388\n",
      "Epoch 19/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.5008 - accuracy: 0.9661\n",
      "Epoch 19: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 69ms/step - loss: 0.5008 - accuracy: 0.9661 - val_loss: 0.6012 - val_accuracy: 0.9344\n",
      "Epoch 20/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4957 - accuracy: 0.9668\n",
      "Epoch 20: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 69ms/step - loss: 0.4958 - accuracy: 0.9668 - val_loss: 0.5960 - val_accuracy: 0.9370\n",
      "Epoch 21/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4906 - accuracy: 0.9687\n",
      "Epoch 21: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 69ms/step - loss: 0.4906 - accuracy: 0.9687 - val_loss: 0.6140 - val_accuracy: 0.9321\n",
      "Epoch 22/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4844 - accuracy: 0.9700\n",
      "Epoch 22: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.4845 - accuracy: 0.9700 - val_loss: 0.6004 - val_accuracy: 0.9350\n",
      "Epoch 23/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4794 - accuracy: 0.9712\n",
      "Epoch 23: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 52s 71ms/step - loss: 0.4794 - accuracy: 0.9712 - val_loss: 0.6013 - val_accuracy: 0.9337\n",
      "Epoch 24/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4730 - accuracy: 0.9729\n",
      "Epoch 24: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 69ms/step - loss: 0.4731 - accuracy: 0.9729 - val_loss: 0.6119 - val_accuracy: 0.9308\n",
      "Epoch 25/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4691 - accuracy: 0.9743\n",
      "Epoch 25: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 69ms/step - loss: 0.4691 - accuracy: 0.9743 - val_loss: 0.5937 - val_accuracy: 0.9373\n",
      "Epoch 26/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4645 - accuracy: 0.9758\n",
      "Epoch 26: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 69ms/step - loss: 0.4644 - accuracy: 0.9758 - val_loss: 0.5995 - val_accuracy: 0.9347\n",
      "Epoch 27/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4602 - accuracy: 0.9764\n",
      "Epoch 27: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 69ms/step - loss: 0.4602 - accuracy: 0.9764 - val_loss: 0.6143 - val_accuracy: 0.9326\n",
      "Epoch 28/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4538 - accuracy: 0.9786\n",
      "Epoch 28: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 69ms/step - loss: 0.4538 - accuracy: 0.9786 - val_loss: 0.6243 - val_accuracy: 0.9297\n",
      "Epoch 29/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4501 - accuracy: 0.9792\n",
      "Epoch 29: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 50s 69ms/step - loss: 0.4501 - accuracy: 0.9792 - val_loss: 0.6319 - val_accuracy: 0.9261\n",
      "Epoch 30/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4463 - accuracy: 0.9799\n",
      "Epoch 30: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 69ms/step - loss: 0.4463 - accuracy: 0.9799 - val_loss: 0.6170 - val_accuracy: 0.9308\n",
      "Epoch 31/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4414 - accuracy: 0.9819\n",
      "Epoch 31: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 69ms/step - loss: 0.4414 - accuracy: 0.9819 - val_loss: 0.6330 - val_accuracy: 0.9288\n",
      "Epoch 32/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4374 - accuracy: 0.9826\n",
      "Epoch 32: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 52s 71ms/step - loss: 0.4374 - accuracy: 0.9826 - val_loss: 0.6354 - val_accuracy: 0.9287\n",
      "Epoch 33/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4348 - accuracy: 0.9831\n",
      "Epoch 33: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 50s 69ms/step - loss: 0.4348 - accuracy: 0.9831 - val_loss: 0.6815 - val_accuracy: 0.9154\n",
      "Epoch 34/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4320 - accuracy: 0.9835\n",
      "Epoch 34: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 52s 71ms/step - loss: 0.4320 - accuracy: 0.9835 - val_loss: 0.6224 - val_accuracy: 0.9305\n",
      "Epoch 35/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4270 - accuracy: 0.9852\n",
      "Epoch 35: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 52s 71ms/step - loss: 0.4270 - accuracy: 0.9852 - val_loss: 0.6361 - val_accuracy: 0.9311\n",
      "Epoch 36/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4241 - accuracy: 0.9861\n",
      "Epoch 36: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 69ms/step - loss: 0.4241 - accuracy: 0.9861 - val_loss: 0.6383 - val_accuracy: 0.9295\n",
      "Epoch 37/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4220 - accuracy: 0.9859\n",
      "Epoch 37: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 69ms/step - loss: 0.4220 - accuracy: 0.9860 - val_loss: 0.6443 - val_accuracy: 0.9274\n",
      "Epoch 38/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4187 - accuracy: 0.9866\n",
      "Epoch 38: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 69ms/step - loss: 0.4187 - accuracy: 0.9866 - val_loss: 0.6409 - val_accuracy: 0.9272\n",
      "Epoch 39/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4156 - accuracy: 0.9879\n",
      "Epoch 39: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 69ms/step - loss: 0.4156 - accuracy: 0.9879 - val_loss: 0.6454 - val_accuracy: 0.9312\n",
      "Epoch 40/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4120 - accuracy: 0.9890\n",
      "Epoch 40: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 69ms/step - loss: 0.4120 - accuracy: 0.9890 - val_loss: 0.6471 - val_accuracy: 0.9290\n",
      "Epoch 41/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4094 - accuracy: 0.9892\n",
      "Epoch 41: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 69ms/step - loss: 0.4095 - accuracy: 0.9892 - val_loss: 0.6705 - val_accuracy: 0.9293\n",
      "Epoch 42/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4101 - accuracy: 0.9883\n",
      "Epoch 42: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 69ms/step - loss: 0.4101 - accuracy: 0.9883 - val_loss: 0.6746 - val_accuracy: 0.9229\n",
      "Epoch 43/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4060 - accuracy: 0.9897\n",
      "Epoch 43: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 69ms/step - loss: 0.4060 - accuracy: 0.9897 - val_loss: 0.6662 - val_accuracy: 0.9255\n",
      "Epoch 44/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4030 - accuracy: 0.9904\n",
      "Epoch 44: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 69ms/step - loss: 0.4030 - accuracy: 0.9904 - val_loss: 0.6713 - val_accuracy: 0.9231\n",
      "Epoch 45/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4009 - accuracy: 0.9905\n",
      "Epoch 45: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 69ms/step - loss: 0.4009 - accuracy: 0.9905 - val_loss: 0.6738 - val_accuracy: 0.9234\n",
      "Epoch 46/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3973 - accuracy: 0.9920\n",
      "Epoch 46: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 69ms/step - loss: 0.3973 - accuracy: 0.9920 - val_loss: 0.6991 - val_accuracy: 0.9246\n",
      "Epoch 47/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3978 - accuracy: 0.9911\n",
      "Epoch 47: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 52s 71ms/step - loss: 0.3978 - accuracy: 0.9911 - val_loss: 0.6505 - val_accuracy: 0.9292\n",
      "Epoch 48/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3943 - accuracy: 0.9924\n",
      "Epoch 48: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 69ms/step - loss: 0.3943 - accuracy: 0.9923 - val_loss: 0.6954 - val_accuracy: 0.9206\n",
      "Epoch 49/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3940 - accuracy: 0.9918\n",
      "Epoch 49: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 50s 68ms/step - loss: 0.3940 - accuracy: 0.9918 - val_loss: 0.6690 - val_accuracy: 0.9301\n",
      "Epoch 50/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3910 - accuracy: 0.9925\n",
      "Epoch 50: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 50s 68ms/step - loss: 0.3911 - accuracy: 0.9925 - val_loss: 0.6692 - val_accuracy: 0.9280\n",
      "Epoch 51/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3887 - accuracy: 0.9935\n",
      "Epoch 51: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 50s 69ms/step - loss: 0.3887 - accuracy: 0.9935 - val_loss: 0.6576 - val_accuracy: 0.9312\n",
      "Epoch 52/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3867 - accuracy: 0.9935\n",
      "Epoch 52: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 50s 68ms/step - loss: 0.3867 - accuracy: 0.9935 - val_loss: 0.6849 - val_accuracy: 0.9279\n",
      "Epoch 53/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3858 - accuracy: 0.9934\n",
      "Epoch 53: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 50s 68ms/step - loss: 0.3858 - accuracy: 0.9934 - val_loss: 0.6520 - val_accuracy: 0.9322\n",
      "Epoch 54/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3814 - accuracy: 0.9947\n",
      "Epoch 54: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 50s 68ms/step - loss: 0.3814 - accuracy: 0.9947 - val_loss: 0.6698 - val_accuracy: 0.9281\n",
      "Epoch 55/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3813 - accuracy: 0.9944\n",
      "Epoch 55: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 50s 68ms/step - loss: 0.3813 - accuracy: 0.9944 - val_loss: 0.6870 - val_accuracy: 0.9260\n",
      "Epoch 56/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3817 - accuracy: 0.9935\n",
      "Epoch 56: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 50s 68ms/step - loss: 0.3817 - accuracy: 0.9935 - val_loss: 0.6909 - val_accuracy: 0.9234\n",
      "Epoch 57/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3773 - accuracy: 0.9950\n",
      "Epoch 57: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 50s 68ms/step - loss: 0.3773 - accuracy: 0.9950 - val_loss: 0.7034 - val_accuracy: 0.9257\n",
      "Epoch 58/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3767 - accuracy: 0.9950\n",
      "Epoch 58: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 50s 68ms/step - loss: 0.3767 - accuracy: 0.9950 - val_loss: 0.6773 - val_accuracy: 0.9292\n",
      "Epoch 59/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3784 - accuracy: 0.9936\n",
      "Epoch 59: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 50s 68ms/step - loss: 0.3784 - accuracy: 0.9936 - val_loss: 0.6689 - val_accuracy: 0.9301\n",
      "Epoch 60/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3763 - accuracy: 0.9945\n",
      "Epoch 60: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 50s 68ms/step - loss: 0.3763 - accuracy: 0.9945 - val_loss: 0.7006 - val_accuracy: 0.9238\n",
      "Epoch 61/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3726 - accuracy: 0.9954\n",
      "Epoch 61: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 50s 68ms/step - loss: 0.3726 - accuracy: 0.9954 - val_loss: 0.6971 - val_accuracy: 0.9236\n",
      "Epoch 62/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3725 - accuracy: 0.9948\n",
      "Epoch 62: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 50s 68ms/step - loss: 0.3725 - accuracy: 0.9948 - val_loss: 0.6493 - val_accuracy: 0.9321\n",
      "Epoch 63/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3665 - accuracy: 0.9968\n",
      "Epoch 63: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 50s 68ms/step - loss: 0.3665 - accuracy: 0.9968 - val_loss: 0.6795 - val_accuracy: 0.9309\n",
      "Epoch 64/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3678 - accuracy: 0.9960\n",
      "Epoch 64: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 50s 68ms/step - loss: 0.3678 - accuracy: 0.9960 - val_loss: 0.7175 - val_accuracy: 0.9208\n",
      "Epoch 65/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3642 - accuracy: 0.9969\n",
      "Epoch 65: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 50s 69ms/step - loss: 0.3642 - accuracy: 0.9969 - val_loss: 0.6775 - val_accuracy: 0.9307\n",
      "Epoch 66/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3621 - accuracy: 0.9975\n",
      "Epoch 66: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 50s 69ms/step - loss: 0.3621 - accuracy: 0.9975 - val_loss: 0.6940 - val_accuracy: 0.9280\n",
      "Epoch 67/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3645 - accuracy: 0.9961\n",
      "Epoch 67: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 50s 68ms/step - loss: 0.3645 - accuracy: 0.9961 - val_loss: 0.6987 - val_accuracy: 0.9254\n",
      "Epoch 68/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3629 - accuracy: 0.9961\n",
      "Epoch 68: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 50s 68ms/step - loss: 0.3629 - accuracy: 0.9961 - val_loss: 0.7004 - val_accuracy: 0.9294\n",
      "Epoch 69/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3617 - accuracy: 0.9963\n",
      "Epoch 69: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.3617 - accuracy: 0.9963 - val_loss: 0.6846 - val_accuracy: 0.9296\n",
      "Epoch 70/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3573 - accuracy: 0.9976\n",
      "Epoch 70: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 50s 68ms/step - loss: 0.3573 - accuracy: 0.9976 - val_loss: 0.6930 - val_accuracy: 0.9270\n",
      "Epoch 71/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3582 - accuracy: 0.9971\n",
      "Epoch 71: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 50s 68ms/step - loss: 0.3583 - accuracy: 0.9971 - val_loss: 0.6988 - val_accuracy: 0.9258\n",
      "Epoch 72/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3582 - accuracy: 0.9966\n",
      "Epoch 72: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.3582 - accuracy: 0.9966 - val_loss: 0.7218 - val_accuracy: 0.9241\n",
      "Epoch 73/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3582 - accuracy: 0.9961\n",
      "Epoch 73: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 50s 68ms/step - loss: 0.3582 - accuracy: 0.9961 - val_loss: 0.7009 - val_accuracy: 0.9302\n",
      "Epoch 74/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3526 - accuracy: 0.9978\n",
      "Epoch 74: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 50s 68ms/step - loss: 0.3526 - accuracy: 0.9978 - val_loss: 0.6804 - val_accuracy: 0.9333\n",
      "Epoch 75/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3508 - accuracy: 0.9980\n",
      "Epoch 75: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 50s 68ms/step - loss: 0.3508 - accuracy: 0.9980 - val_loss: 0.6951 - val_accuracy: 0.9312\n",
      "Epoch 76/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3536 - accuracy: 0.9969\n",
      "Epoch 76: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 50s 69ms/step - loss: 0.3536 - accuracy: 0.9969 - val_loss: 0.8246 - val_accuracy: 0.9021\n",
      "Epoch 77/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3556 - accuracy: 0.9958\n",
      "Epoch 77: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 50s 69ms/step - loss: 0.3556 - accuracy: 0.9958 - val_loss: 0.6885 - val_accuracy: 0.9312\n",
      "Epoch 78/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3510 - accuracy: 0.9970\n",
      "Epoch 78: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 50s 69ms/step - loss: 0.3510 - accuracy: 0.9970 - val_loss: 0.6764 - val_accuracy: 0.9326\n",
      "Epoch 79/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3455 - accuracy: 0.9987\n",
      "Epoch 79: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 50s 69ms/step - loss: 0.3455 - accuracy: 0.9987 - val_loss: 0.7118 - val_accuracy: 0.9301\n",
      "Epoch 80/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3506 - accuracy: 0.9965\n",
      "Epoch 80: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 69ms/step - loss: 0.3506 - accuracy: 0.9965 - val_loss: 0.6758 - val_accuracy: 0.9306\n",
      "Epoch 81/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3446 - accuracy: 0.9983\n",
      "Epoch 81: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 52s 71ms/step - loss: 0.3446 - accuracy: 0.9983 - val_loss: 0.6975 - val_accuracy: 0.9286\n",
      "Epoch 82/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3480 - accuracy: 0.9965\n",
      "Epoch 82: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 69ms/step - loss: 0.3480 - accuracy: 0.9965 - val_loss: 0.6920 - val_accuracy: 0.9287\n",
      "Epoch 83/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3423 - accuracy: 0.9984\n",
      "Epoch 83: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 50s 69ms/step - loss: 0.3423 - accuracy: 0.9984 - val_loss: 0.7423 - val_accuracy: 0.9286\n",
      "Epoch 84/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3440 - accuracy: 0.9976\n",
      "Epoch 84: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.3440 - accuracy: 0.9976 - val_loss: 0.7624 - val_accuracy: 0.9207\n",
      "Epoch 85/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3416 - accuracy: 0.9980\n",
      "Epoch 85: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 52s 71ms/step - loss: 0.3416 - accuracy: 0.9980 - val_loss: 0.7271 - val_accuracy: 0.9227\n",
      "Epoch 86/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3422 - accuracy: 0.9973\n",
      "Epoch 86: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.3422 - accuracy: 0.9973 - val_loss: 0.6771 - val_accuracy: 0.9323\n",
      "Epoch 87/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3357 - accuracy: 0.9993\n",
      "Epoch 87: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.3357 - accuracy: 0.9993 - val_loss: 0.6667 - val_accuracy: 0.9366\n",
      "Epoch 88/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3326 - accuracy: 0.9999\n",
      "Epoch 88: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.3326 - accuracy: 0.9999 - val_loss: 0.6758 - val_accuracy: 0.9365\n",
      "Epoch 89/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3313 - accuracy: 1.0000\n",
      "Epoch 89: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 50s 69ms/step - loss: 0.3314 - accuracy: 0.9999 - val_loss: 0.6806 - val_accuracy: 0.9347\n",
      "Epoch 90/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3382 - accuracy: 0.9975\n",
      "Epoch 90: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 50s 69ms/step - loss: 0.3382 - accuracy: 0.9975 - val_loss: 0.7085 - val_accuracy: 0.9294\n",
      "Epoch 91/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3439 - accuracy: 0.9949\n",
      "Epoch 91: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 50s 69ms/step - loss: 0.3439 - accuracy: 0.9949 - val_loss: 0.7038 - val_accuracy: 0.9314\n",
      "Epoch 92/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3387 - accuracy: 0.9965\n",
      "Epoch 92: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 50s 68ms/step - loss: 0.3387 - accuracy: 0.9965 - val_loss: 0.7170 - val_accuracy: 0.9250\n",
      "Epoch 93/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3342 - accuracy: 0.9979\n",
      "Epoch 93: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 50s 69ms/step - loss: 0.3342 - accuracy: 0.9979 - val_loss: 0.7153 - val_accuracy: 0.9285\n",
      "Epoch 94/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3320 - accuracy: 0.9983\n",
      "Epoch 94: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 50s 69ms/step - loss: 0.3320 - accuracy: 0.9983 - val_loss: 0.6982 - val_accuracy: 0.9282\n",
      "Epoch 95/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3287 - accuracy: 0.9991\n",
      "Epoch 95: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 50s 69ms/step - loss: 0.3287 - accuracy: 0.9991 - val_loss: 0.6886 - val_accuracy: 0.9308\n",
      "Epoch 96/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3257 - accuracy: 0.9999\n",
      "Epoch 96: val_loss did not improve from 0.58919\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 50s 68ms/step - loss: 0.3257 - accuracy: 0.9999 - val_loss: 0.6842 - val_accuracy: 0.9356\n",
      "Epoch 97/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3240 - accuracy: 0.9999\n",
      "Epoch 97: val_loss did not improve from 0.58919\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 50s 68ms/step - loss: 0.3240 - accuracy: 0.9999 - val_loss: 0.6810 - val_accuracy: 0.9368\n",
      "Epoch 98/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.5896 - accuracy: 0.9137\n",
      "Epoch 98: val_loss did not improve from 0.58919\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.5896 - accuracy: 0.9137 - val_loss: 0.6895 - val_accuracy: 0.8863\n",
      "Epoch 99/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4838 - accuracy: 0.9411\n",
      "Epoch 99: val_loss did not improve from 0.58919\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 50s 68ms/step - loss: 0.4838 - accuracy: 0.9411 - val_loss: 0.6887 - val_accuracy: 0.8841\n",
      "Epoch 100/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4539 - accuracy: 0.9476\n",
      "Epoch 100: val_loss improved from 0.58919 to 0.52669, saving model to ResNet-for-EMNIST.h5\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.4539 - accuracy: 0.9476 - val_loss: 0.5267 - val_accuracy: 0.9266\n",
      "Epoch 101/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4324 - accuracy: 0.9516\n",
      "Epoch 101: val_loss did not improve from 0.52669\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 51s 69ms/step - loss: 0.4324 - accuracy: 0.9515 - val_loss: 0.5387 - val_accuracy: 0.9221\n",
      "Epoch 102/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4133 - accuracy: 0.9553\n",
      "Epoch 102: val_loss improved from 0.52669 to 0.50999, saving model to ResNet-for-EMNIST.h5\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.4134 - accuracy: 0.9553 - val_loss: 0.5100 - val_accuracy: 0.9263\n",
      "Epoch 103/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3978 - accuracy: 0.9578\n",
      "Epoch 103: val_loss did not improve from 0.50999\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 51s 69ms/step - loss: 0.3977 - accuracy: 0.9579 - val_loss: 0.5385 - val_accuracy: 0.9194\n",
      "Epoch 104/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3847 - accuracy: 0.9598\n",
      "Epoch 104: val_loss improved from 0.50999 to 0.49273, saving model to ResNet-for-EMNIST.h5\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 52s 72ms/step - loss: 0.3846 - accuracy: 0.9598 - val_loss: 0.4927 - val_accuracy: 0.9308\n",
      "Epoch 105/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3706 - accuracy: 0.9618\n",
      "Epoch 105: val_loss improved from 0.49273 to 0.46885, saving model to ResNet-for-EMNIST.h5\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.3706 - accuracy: 0.9618 - val_loss: 0.4688 - val_accuracy: 0.9366\n",
      "Epoch 106/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3584 - accuracy: 0.9632\n",
      "Epoch 106: val_loss did not improve from 0.46885\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 50s 68ms/step - loss: 0.3584 - accuracy: 0.9632 - val_loss: 0.4973 - val_accuracy: 0.9255\n",
      "Epoch 107/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3482 - accuracy: 0.9643\n",
      "Epoch 107: val_loss did not improve from 0.46885\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 51s 69ms/step - loss: 0.3482 - accuracy: 0.9643 - val_loss: 0.4968 - val_accuracy: 0.9268\n",
      "Epoch 108/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3346 - accuracy: 0.9669\n",
      "Epoch 108: val_loss improved from 0.46885 to 0.46273, saving model to ResNet-for-EMNIST.h5\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 53s 72ms/step - loss: 0.3346 - accuracy: 0.9669 - val_loss: 0.4627 - val_accuracy: 0.9355\n",
      "Epoch 109/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3270 - accuracy: 0.9677\n",
      "Epoch 109: val_loss did not improve from 0.46273\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 51s 69ms/step - loss: 0.3270 - accuracy: 0.9677 - val_loss: 0.5485 - val_accuracy: 0.9142\n",
      "Epoch 110/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3168 - accuracy: 0.9690\n",
      "Epoch 110: val_loss did not improve from 0.46273\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 51s 69ms/step - loss: 0.3169 - accuracy: 0.9690 - val_loss: 0.4671 - val_accuracy: 0.9329\n",
      "Epoch 111/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3078 - accuracy: 0.9706\n",
      "Epoch 111: val_loss did not improve from 0.46273\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 51s 69ms/step - loss: 0.3079 - accuracy: 0.9705 - val_loss: 0.4646 - val_accuracy: 0.9309\n",
      "Epoch 112/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3009 - accuracy: 0.9704\n",
      "Epoch 112: val_loss did not improve from 0.46273\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 51s 69ms/step - loss: 0.3009 - accuracy: 0.9704 - val_loss: 0.4994 - val_accuracy: 0.9232\n",
      "Epoch 113/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.2936 - accuracy: 0.9714\n",
      "Epoch 113: val_loss did not improve from 0.46273\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 50s 69ms/step - loss: 0.2936 - accuracy: 0.9714 - val_loss: 0.5295 - val_accuracy: 0.9180\n",
      "Epoch 114/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.2855 - accuracy: 0.9732\n",
      "Epoch 114: val_loss improved from 0.46273 to 0.45344, saving model to ResNet-for-EMNIST.h5\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.2855 - accuracy: 0.9732 - val_loss: 0.4534 - val_accuracy: 0.9329\n",
      "Epoch 115/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.2792 - accuracy: 0.9735\n",
      "Epoch 115: val_loss did not improve from 0.45344\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 52s 70ms/step - loss: 0.2791 - accuracy: 0.9735 - val_loss: 0.4816 - val_accuracy: 0.9277\n",
      "Epoch 116/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.2700 - accuracy: 0.9754\n",
      "Epoch 116: val_loss did not improve from 0.45344\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 51s 69ms/step - loss: 0.2700 - accuracy: 0.9754 - val_loss: 0.4857 - val_accuracy: 0.9229\n",
      "Epoch 117/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.2643 - accuracy: 0.9755\n",
      "Epoch 117: val_loss improved from 0.45344 to 0.44977, saving model to ResNet-for-EMNIST.h5\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 53s 72ms/step - loss: 0.2643 - accuracy: 0.9755 - val_loss: 0.4498 - val_accuracy: 0.9319\n",
      "Epoch 118/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.2622 - accuracy: 0.9748\n",
      "Epoch 118: val_loss did not improve from 0.44977\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.2622 - accuracy: 0.9748 - val_loss: 0.4748 - val_accuracy: 0.9240\n",
      "Epoch 119/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.2535 - accuracy: 0.9764\n",
      "Epoch 119: val_loss did not improve from 0.44977\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 51s 69ms/step - loss: 0.2535 - accuracy: 0.9764 - val_loss: 0.4865 - val_accuracy: 0.9229\n",
      "Epoch 120/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.2506 - accuracy: 0.9769\n",
      "Epoch 120: val_loss improved from 0.44977 to 0.44922, saving model to ResNet-for-EMNIST.h5\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.2506 - accuracy: 0.9770 - val_loss: 0.4492 - val_accuracy: 0.9312\n",
      "Epoch 121/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.2444 - accuracy: 0.9773\n",
      "Epoch 121: val_loss did not improve from 0.44922\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 52s 71ms/step - loss: 0.2445 - accuracy: 0.9773 - val_loss: 0.5513 - val_accuracy: 0.9122\n",
      "Epoch 122/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.2420 - accuracy: 0.9768\n",
      "Epoch 122: val_loss improved from 0.44922 to 0.41880, saving model to ResNet-for-EMNIST.h5\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 52s 71ms/step - loss: 0.2420 - accuracy: 0.9768 - val_loss: 0.4188 - val_accuracy: 0.9402\n",
      "Epoch 123/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.2375 - accuracy: 0.9767\n",
      "Epoch 123: val_loss did not improve from 0.41880\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 51s 69ms/step - loss: 0.2375 - accuracy: 0.9767 - val_loss: 0.4711 - val_accuracy: 0.9205\n",
      "Epoch 124/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.2286 - accuracy: 0.9798\n",
      "Epoch 124: val_loss did not improve from 0.41880\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 50s 69ms/step - loss: 0.2286 - accuracy: 0.9798 - val_loss: 0.4762 - val_accuracy: 0.9212\n",
      "Epoch 125/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.2293 - accuracy: 0.9783\n",
      "Epoch 125: val_loss did not improve from 0.41880\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 50s 69ms/step - loss: 0.2293 - accuracy: 0.9783 - val_loss: 0.4583 - val_accuracy: 0.9280\n",
      "Epoch 126/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.2238 - accuracy: 0.9789\n",
      "Epoch 126: val_loss improved from 0.41880 to 0.41194, saving model to ResNet-for-EMNIST.h5\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 52s 71ms/step - loss: 0.2238 - accuracy: 0.9789 - val_loss: 0.4119 - val_accuracy: 0.9383\n",
      "Epoch 127/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.2156 - accuracy: 0.9811\n",
      "Epoch 127: val_loss improved from 0.41194 to 0.40827, saving model to ResNet-for-EMNIST.h5\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 53s 73ms/step - loss: 0.2156 - accuracy: 0.9811 - val_loss: 0.4083 - val_accuracy: 0.9346\n",
      "Epoch 128/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.2129 - accuracy: 0.9808\n",
      "Epoch 128: val_loss did not improve from 0.40827\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.2129 - accuracy: 0.9808 - val_loss: 0.6095 - val_accuracy: 0.8977\n",
      "Epoch 129/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.2134 - accuracy: 0.9797\n",
      "Epoch 129: val_loss did not improve from 0.40827\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.2134 - accuracy: 0.9797 - val_loss: 0.4816 - val_accuracy: 0.9203\n",
      "Epoch 130/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.2067 - accuracy: 0.9812\n",
      "Epoch 130: val_loss did not improve from 0.40827\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 51s 69ms/step - loss: 0.2067 - accuracy: 0.9812 - val_loss: 0.5290 - val_accuracy: 0.9154\n",
      "Epoch 131/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.2016 - accuracy: 0.9824\n",
      "Epoch 131: val_loss did not improve from 0.40827\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.2016 - accuracy: 0.9824 - val_loss: 0.6015 - val_accuracy: 0.8985\n",
      "Epoch 132/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.2012 - accuracy: 0.9818\n",
      "Epoch 132: val_loss did not improve from 0.40827\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.2012 - accuracy: 0.9818 - val_loss: 0.4278 - val_accuracy: 0.9343\n",
      "Epoch 133/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1925 - accuracy: 0.9842\n",
      "Epoch 133: val_loss did not improve from 0.40827\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.1925 - accuracy: 0.9842 - val_loss: 0.4599 - val_accuracy: 0.9277\n",
      "Epoch 134/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1868 - accuracy: 0.9854\n",
      "Epoch 134: val_loss did not improve from 0.40827\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.1868 - accuracy: 0.9854 - val_loss: 0.4718 - val_accuracy: 0.9252\n",
      "Epoch 135/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1893 - accuracy: 0.9835\n",
      "Epoch 135: val_loss did not improve from 0.40827\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.1893 - accuracy: 0.9835 - val_loss: 0.5012 - val_accuracy: 0.9160\n",
      "Epoch 136/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1859 - accuracy: 0.9839\n",
      "Epoch 136: val_loss did not improve from 0.40827\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.1859 - accuracy: 0.9839 - val_loss: 0.4206 - val_accuracy: 0.9288\n",
      "Epoch 137/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1832 - accuracy: 0.9847\n",
      "Epoch 137: val_loss did not improve from 0.40827\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 51s 69ms/step - loss: 0.1832 - accuracy: 0.9847 - val_loss: 0.4751 - val_accuracy: 0.9200\n",
      "Epoch 138/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1817 - accuracy: 0.9839\n",
      "Epoch 138: val_loss did not improve from 0.40827\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 51s 69ms/step - loss: 0.1817 - accuracy: 0.9839 - val_loss: 0.4646 - val_accuracy: 0.9201\n",
      "Epoch 139/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1787 - accuracy: 0.9848\n",
      "Epoch 139: val_loss improved from 0.40827 to 0.40392, saving model to ResNet-for-EMNIST.h5\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 52s 71ms/step - loss: 0.1787 - accuracy: 0.9848 - val_loss: 0.4039 - val_accuracy: 0.9346\n",
      "Epoch 140/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1692 - accuracy: 0.9878\n",
      "Epoch 140: val_loss did not improve from 0.40392\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.1692 - accuracy: 0.9878 - val_loss: 0.6022 - val_accuracy: 0.8988\n",
      "Epoch 141/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1715 - accuracy: 0.9861\n",
      "Epoch 141: val_loss did not improve from 0.40392\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 52s 71ms/step - loss: 0.1715 - accuracy: 0.9861 - val_loss: 0.5705 - val_accuracy: 0.9035\n",
      "Epoch 142/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1712 - accuracy: 0.9855\n",
      "Epoch 142: val_loss did not improve from 0.40392\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.1712 - accuracy: 0.9855 - val_loss: 0.4200 - val_accuracy: 0.9284\n",
      "Epoch 143/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1676 - accuracy: 0.9864\n",
      "Epoch 143: val_loss did not improve from 0.40392\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.1676 - accuracy: 0.9864 - val_loss: 0.4535 - val_accuracy: 0.9172\n",
      "Epoch 144/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1638 - accuracy: 0.9871\n",
      "Epoch 144: val_loss did not improve from 0.40392\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.1638 - accuracy: 0.9871 - val_loss: 0.4235 - val_accuracy: 0.9312\n",
      "Epoch 145/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1627 - accuracy: 0.9872\n",
      "Epoch 145: val_loss did not improve from 0.40392\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.1627 - accuracy: 0.9871 - val_loss: 0.4589 - val_accuracy: 0.9169\n",
      "Epoch 146/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1485 - accuracy: 0.9943\n",
      "Epoch 146: val_loss improved from 0.40392 to 0.36155, saving model to ResNet-for-EMNIST.h5\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 52s 71ms/step - loss: 0.1485 - accuracy: 0.9943 - val_loss: 0.3616 - val_accuracy: 0.9455\n",
      "Epoch 147/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1327 - accuracy: 0.9992\n",
      "Epoch 147: val_loss did not improve from 0.36155\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.1327 - accuracy: 0.9992 - val_loss: 0.3772 - val_accuracy: 0.9458\n",
      "Epoch 148/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1301 - accuracy: 0.9998\n",
      "Epoch 148: val_loss did not improve from 0.36155\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.1301 - accuracy: 0.9998 - val_loss: 0.3916 - val_accuracy: 0.9457\n",
      "Epoch 149/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1290 - accuracy: 0.9999\n",
      "Epoch 149: val_loss did not improve from 0.36155\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.1290 - accuracy: 0.9999 - val_loss: 0.4005 - val_accuracy: 0.9459\n",
      "Epoch 150/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1281 - accuracy: 1.0000\n",
      "Epoch 150: val_loss did not improve from 0.36155\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 52s 71ms/step - loss: 0.1281 - accuracy: 1.0000 - val_loss: 0.4053 - val_accuracy: 0.9459\n",
      "Epoch 151/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1275 - accuracy: 1.0000\n",
      "Epoch 151: val_loss did not improve from 0.36155\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.1275 - accuracy: 1.0000 - val_loss: 0.4128 - val_accuracy: 0.9460\n",
      "Epoch 152/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1269 - accuracy: 1.0000\n",
      "Epoch 152: val_loss did not improve from 0.36155\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.1269 - accuracy: 1.0000 - val_loss: 0.4165 - val_accuracy: 0.9459\n",
      "Epoch 153/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1264 - accuracy: 1.0000\n",
      "Epoch 153: val_loss did not improve from 0.36155\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.1264 - accuracy: 1.0000 - val_loss: 0.4208 - val_accuracy: 0.9459\n",
      "Epoch 154/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1259 - accuracy: 1.0000\n",
      "Epoch 154: val_loss did not improve from 0.36155\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.1259 - accuracy: 1.0000 - val_loss: 0.4242 - val_accuracy: 0.9463\n",
      "Epoch 155/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1255 - accuracy: 1.0000\n",
      "Epoch 155: val_loss did not improve from 0.36155\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 52s 72ms/step - loss: 0.1255 - accuracy: 1.0000 - val_loss: 0.4275 - val_accuracy: 0.9465\n",
      "Epoch 156/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1251 - accuracy: 1.0000\n",
      "Epoch 156: val_loss did not improve from 0.36155\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.1251 - accuracy: 1.0000 - val_loss: 0.4284 - val_accuracy: 0.9464\n",
      "Epoch 157/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1247 - accuracy: 1.0000\n",
      "Epoch 157: val_loss did not improve from 0.36155\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.1247 - accuracy: 1.0000 - val_loss: 0.4309 - val_accuracy: 0.9466\n",
      "Epoch 158/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1243 - accuracy: 1.0000\n",
      "Epoch 158: val_loss did not improve from 0.36155\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.1243 - accuracy: 1.0000 - val_loss: 0.4328 - val_accuracy: 0.9468\n",
      "Epoch 159/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1239 - accuracy: 1.0000\n",
      "Epoch 159: val_loss did not improve from 0.36155\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.1239 - accuracy: 1.0000 - val_loss: 0.4341 - val_accuracy: 0.9467\n",
      "Epoch 160/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1235 - accuracy: 1.0000\n",
      "Epoch 160: val_loss did not improve from 0.36155\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.1235 - accuracy: 1.0000 - val_loss: 0.4355 - val_accuracy: 0.9466\n",
      "Epoch 161/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1232 - accuracy: 1.0000\n",
      "Epoch 161: val_loss did not improve from 0.36155\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.1232 - accuracy: 1.0000 - val_loss: 0.4370 - val_accuracy: 0.9469\n",
      "Epoch 162/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1228 - accuracy: 1.0000\n",
      "Epoch 162: val_loss did not improve from 0.36155\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.1228 - accuracy: 1.0000 - val_loss: 0.4377 - val_accuracy: 0.9469\n",
      "Epoch 163/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1224 - accuracy: 1.0000\n",
      "Epoch 163: val_loss did not improve from 0.36155\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.1224 - accuracy: 1.0000 - val_loss: 0.4398 - val_accuracy: 0.9469\n",
      "Epoch 164/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1220 - accuracy: 1.0000\n",
      "Epoch 164: val_loss did not improve from 0.36155\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.1220 - accuracy: 1.0000 - val_loss: 0.4401 - val_accuracy: 0.9470\n",
      "Epoch 165/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1217 - accuracy: 1.0000\n",
      "Epoch 165: val_loss did not improve from 0.36155\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.1217 - accuracy: 1.0000 - val_loss: 0.4405 - val_accuracy: 0.9474\n",
      "Epoch 166/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1213 - accuracy: 1.0000\n",
      "Epoch 166: val_loss did not improve from 0.36155\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.1213 - accuracy: 1.0000 - val_loss: 0.4413 - val_accuracy: 0.9473\n",
      "Epoch 167/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1209 - accuracy: 1.0000\n",
      "Epoch 167: val_loss did not improve from 0.36155\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.1209 - accuracy: 1.0000 - val_loss: 0.4425 - val_accuracy: 0.9472\n",
      "Epoch 168/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1205 - accuracy: 1.0000\n",
      "Epoch 168: val_loss did not improve from 0.36155\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.1205 - accuracy: 1.0000 - val_loss: 0.4431 - val_accuracy: 0.9473\n",
      "Epoch 169/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1202 - accuracy: 1.0000\n",
      "Epoch 169: val_loss did not improve from 0.36155\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.1202 - accuracy: 1.0000 - val_loss: 0.4440 - val_accuracy: 0.9473\n",
      "Epoch 170/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1198 - accuracy: 1.0000\n",
      "Epoch 170: val_loss did not improve from 0.36155\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.1198 - accuracy: 1.0000 - val_loss: 0.4438 - val_accuracy: 0.9471\n",
      "Epoch 171/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1195 - accuracy: 1.0000\n",
      "Epoch 171: val_loss did not improve from 0.36155\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.1195 - accuracy: 1.0000 - val_loss: 0.4442 - val_accuracy: 0.9473\n",
      "Epoch 172/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1191 - accuracy: 1.0000\n",
      "Epoch 172: val_loss did not improve from 0.36155\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.1191 - accuracy: 1.0000 - val_loss: 0.4453 - val_accuracy: 0.9473\n",
      "Epoch 173/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1188 - accuracy: 1.0000\n",
      "Epoch 173: val_loss did not improve from 0.36155\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.1188 - accuracy: 1.0000 - val_loss: 0.4459 - val_accuracy: 0.9473\n",
      "Epoch 174/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1184 - accuracy: 1.0000\n",
      "Epoch 174: val_loss did not improve from 0.36155\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.1184 - accuracy: 1.0000 - val_loss: 0.4472 - val_accuracy: 0.9474\n",
      "Epoch 175/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1181 - accuracy: 1.0000\n",
      "Epoch 175: val_loss did not improve from 0.36155\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.1181 - accuracy: 1.0000 - val_loss: 0.4480 - val_accuracy: 0.9474\n",
      "Epoch 176/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1177 - accuracy: 1.0000\n",
      "Epoch 176: val_loss did not improve from 0.36155\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.1177 - accuracy: 1.0000 - val_loss: 0.4488 - val_accuracy: 0.9474\n",
      "Epoch 177/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1174 - accuracy: 1.0000\n",
      "Epoch 177: val_loss did not improve from 0.36155\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.1174 - accuracy: 1.0000 - val_loss: 0.4490 - val_accuracy: 0.9473\n",
      "Epoch 178/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1170 - accuracy: 1.0000\n",
      "Epoch 178: val_loss did not improve from 0.36155\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.1170 - accuracy: 1.0000 - val_loss: 0.4488 - val_accuracy: 0.9473\n",
      "Epoch 179/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1167 - accuracy: 1.0000\n",
      "Epoch 179: val_loss did not improve from 0.36155\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.1167 - accuracy: 1.0000 - val_loss: 0.4501 - val_accuracy: 0.9472\n",
      "Epoch 180/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1163 - accuracy: 1.0000\n",
      "Epoch 180: val_loss did not improve from 0.36155\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.1163 - accuracy: 1.0000 - val_loss: 0.4505 - val_accuracy: 0.9474\n",
      "Epoch 181/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1160 - accuracy: 1.0000\n",
      "Epoch 181: val_loss did not improve from 0.36155\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.1160 - accuracy: 1.0000 - val_loss: 0.4507 - val_accuracy: 0.9474\n",
      "Epoch 182/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1156 - accuracy: 1.0000\n",
      "Epoch 182: val_loss did not improve from 0.36155\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.1156 - accuracy: 1.0000 - val_loss: 0.4509 - val_accuracy: 0.9474\n",
      "Epoch 183/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1153 - accuracy: 1.0000\n",
      "Epoch 183: val_loss did not improve from 0.36155\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.1153 - accuracy: 1.0000 - val_loss: 0.4512 - val_accuracy: 0.9472\n",
      "Epoch 184/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1150 - accuracy: 1.0000\n",
      "Epoch 184: val_loss did not improve from 0.36155\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.1150 - accuracy: 1.0000 - val_loss: 0.4521 - val_accuracy: 0.9473\n",
      "Epoch 185/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1146 - accuracy: 1.0000\n",
      "Epoch 185: val_loss did not improve from 0.36155\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.1146 - accuracy: 1.0000 - val_loss: 0.4518 - val_accuracy: 0.9473\n",
      "Epoch 186/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1143 - accuracy: 1.0000\n",
      "Epoch 186: val_loss did not improve from 0.36155\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.1143 - accuracy: 1.0000 - val_loss: 0.4524 - val_accuracy: 0.9472\n",
      "Epoch 187/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1139 - accuracy: 1.0000\n",
      "Epoch 187: val_loss did not improve from 0.36155\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 52s 70ms/step - loss: 0.1139 - accuracy: 1.0000 - val_loss: 0.4530 - val_accuracy: 0.9473\n",
      "Epoch 188/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1136 - accuracy: 1.0000\n",
      "Epoch 188: val_loss did not improve from 0.36155\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.1136 - accuracy: 1.0000 - val_loss: 0.4527 - val_accuracy: 0.9472\n",
      "Epoch 189/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1133 - accuracy: 1.0000\n",
      "Epoch 189: val_loss did not improve from 0.36155\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.1133 - accuracy: 1.0000 - val_loss: 0.4538 - val_accuracy: 0.9473\n",
      "Epoch 190/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1129 - accuracy: 1.0000\n",
      "Epoch 190: val_loss did not improve from 0.36155\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.1129 - accuracy: 1.0000 - val_loss: 0.4542 - val_accuracy: 0.9473\n",
      "Epoch 191/191\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1126 - accuracy: 1.0000\n",
      "Epoch 191: val_loss did not improve from 0.36155\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 51s 70ms/step - loss: 0.1126 - accuracy: 1.0000 - val_loss: 0.4545 - val_accuracy: 0.9473\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TRAIN\n",
    "\n",
    "if os.path.isfile(emnist_teacher_path):\n",
    "    emnist_teacher_model.load_weights(cifar_teacher_path)\n",
    "else:\n",
    "    emnist_history = emnist_teacher_model.fit(emnist_train_x, emnist_train_y, \n",
    "                    epochs=num_epoch, \n",
    "                    batch_size=batch_size, \n",
    "                    validation_data=(emnist_val_x, emnist_val_y), \n",
    "                    callbacks=emnist_teacher_callbacks,\n",
    "          )\n",
    "\n",
    "\n",
    "    plt.plot(emnist_history.history[\"accuracy\"], label = 'Train Accuracy')\n",
    "    plt.plot(emnist_history.history[\"val_accuracy\"], linestyle = 'dashed', label = 'Test Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f2413435",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f2413435",
    "outputId": "e070ab41-e152-4479-bf1c-2bb04cd0339f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "650/650 [==============================] - 6s 10ms/step - loss: 0.4751 - accuracy: 0.9455\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.47508567571640015, 0.9454807639122009]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emnist_teacher_model.evaluate(emnist_test_x, emnist_test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc95e25d",
   "metadata": {
    "id": "fc95e25d"
   },
   "source": [
    "### Reduced Teacher "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b4cdd7f3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b4cdd7f3",
    "outputId": "61e9c5cf-d209-4b2e-9fc1-dc76d8d84581"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_5 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               200960    \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 27)                6939      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 207,899\n",
      "Trainable params: 207,899\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "emnist_reduced_teacher = create_dense_nn((28, 28, 1), 27)\n",
    "emnist_reduced_teacher.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e49c57",
   "metadata": {
    "id": "e3e49c57"
   },
   "source": [
    "### EMNIST Teacher - Reduced Teacher Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a0b1282c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "a0b1282c",
    "outputId": "50019cec-3908-44db-9061-fdd072db23a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2925/2925 [==============================] - 164s 31ms/step - sparse_categorical_accuracy: 0.8982 - student_loss: 2.4639 - distillation_loss: 0.0211 - val_sparse_categorical_accuracy: 0.8772 - val_student_loss: 2.4219\n",
      "Epoch 2/20\n",
      "2925/2925 [==============================] - 91s 31ms/step - sparse_categorical_accuracy: 0.9194 - student_loss: 2.4409 - distillation_loss: 0.0211 - val_sparse_categorical_accuracy: 0.8727 - val_student_loss: 2.5147\n",
      "Epoch 3/20\n",
      "2925/2925 [==============================] - 93s 32ms/step - sparse_categorical_accuracy: 0.9267 - student_loss: 2.4325 - distillation_loss: 0.0211 - val_sparse_categorical_accuracy: 0.9147 - val_student_loss: 2.4417\n",
      "Epoch 4/20\n",
      "2925/2925 [==============================] - 92s 31ms/step - sparse_categorical_accuracy: 0.9316 - student_loss: 2.4273 - distillation_loss: 0.0211 - val_sparse_categorical_accuracy: 0.9279 - val_student_loss: 2.4693\n",
      "Epoch 5/20\n",
      "2925/2925 [==============================] - 88s 30ms/step - sparse_categorical_accuracy: 0.9358 - student_loss: 2.4230 - distillation_loss: 0.0211 - val_sparse_categorical_accuracy: 0.8348 - val_student_loss: 2.4872\n",
      "Epoch 6/20\n",
      "2925/2925 [==============================] - 87s 30ms/step - sparse_categorical_accuracy: 0.9389 - student_loss: 2.4198 - distillation_loss: 0.0211 - val_sparse_categorical_accuracy: 0.9349 - val_student_loss: 2.4329\n",
      "Epoch 7/20\n",
      "2925/2925 [==============================] - 87s 30ms/step - sparse_categorical_accuracy: 0.9406 - student_loss: 2.4177 - distillation_loss: 0.0212 - val_sparse_categorical_accuracy: 0.9289 - val_student_loss: 2.3879\n",
      "Epoch 8/20\n",
      "2925/2925 [==============================] - 88s 30ms/step - sparse_categorical_accuracy: 0.9436 - student_loss: 2.4149 - distillation_loss: 0.0211 - val_sparse_categorical_accuracy: 0.9311 - val_student_loss: 2.4220\n",
      "Epoch 9/20\n",
      "2925/2925 [==============================] - 88s 30ms/step - sparse_categorical_accuracy: 0.9448 - student_loss: 2.4136 - distillation_loss: 0.0212 - val_sparse_categorical_accuracy: 0.9248 - val_student_loss: 2.4219\n",
      "Epoch 10/20\n",
      "2925/2925 [==============================] - 87s 30ms/step - sparse_categorical_accuracy: 0.9454 - student_loss: 2.4128 - distillation_loss: 0.0212 - val_sparse_categorical_accuracy: 0.9418 - val_student_loss: 2.3606\n",
      "Epoch 11/20\n",
      "2925/2925 [==============================] - 88s 30ms/step - sparse_categorical_accuracy: 0.9476 - student_loss: 2.4106 - distillation_loss: 0.0212 - val_sparse_categorical_accuracy: 0.9423 - val_student_loss: 2.3890\n",
      "Epoch 12/20\n",
      "2925/2925 [==============================] - 88s 30ms/step - sparse_categorical_accuracy: 0.9483 - student_loss: 2.4098 - distillation_loss: 0.0212 - val_sparse_categorical_accuracy: 0.9439 - val_student_loss: 2.3864\n",
      "Epoch 13/20\n",
      "2925/2925 [==============================] - 88s 30ms/step - sparse_categorical_accuracy: 0.9491 - student_loss: 2.4091 - distillation_loss: 0.0212 - val_sparse_categorical_accuracy: 0.9302 - val_student_loss: 2.4756\n",
      "Epoch 14/20\n",
      "2925/2925 [==============================] - 87s 30ms/step - sparse_categorical_accuracy: 0.9504 - student_loss: 2.4077 - distillation_loss: 0.0212 - val_sparse_categorical_accuracy: 0.9422 - val_student_loss: 2.4182\n",
      "Epoch 15/20\n",
      "2925/2925 [==============================] - 87s 30ms/step - sparse_categorical_accuracy: 0.9508 - student_loss: 2.4071 - distillation_loss: 0.0212 - val_sparse_categorical_accuracy: 0.9411 - val_student_loss: 2.4195\n",
      "Epoch 16/20\n",
      "2925/2925 [==============================] - 87s 30ms/step - sparse_categorical_accuracy: 0.9525 - student_loss: 2.4057 - distillation_loss: 0.0212 - val_sparse_categorical_accuracy: 0.9459 - val_student_loss: 2.3970\n",
      "Epoch 17/20\n",
      "2925/2925 [==============================] - 88s 30ms/step - sparse_categorical_accuracy: 0.9528 - student_loss: 2.4052 - distillation_loss: 0.0212 - val_sparse_categorical_accuracy: 0.9448 - val_student_loss: 2.3589\n",
      "Epoch 18/20\n",
      "2925/2925 [==============================] - 87s 30ms/step - sparse_categorical_accuracy: 0.9545 - student_loss: 2.4036 - distillation_loss: 0.0212 - val_sparse_categorical_accuracy: 0.9456 - val_student_loss: 2.3969\n",
      "Epoch 19/20\n",
      "2925/2925 [==============================] - 87s 30ms/step - sparse_categorical_accuracy: 0.9547 - student_loss: 2.4034 - distillation_loss: 0.0212 - val_sparse_categorical_accuracy: 0.9481 - val_student_loss: 2.3740\n",
      "Epoch 20/20\n",
      "2925/2925 [==============================] - 88s 30ms/step - sparse_categorical_accuracy: 0.9553 - student_loss: 2.4028 - distillation_loss: 0.0212 - val_sparse_categorical_accuracy: 0.9454 - val_student_loss: 2.4506\n",
      "650/650 [==============================] - 6s 8ms/step - sparse_categorical_accuracy: 0.9443 - student_loss: 2.4135\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "emnist_distiller = Distiller(student=emnist_teacher_model, teacher=emnist_reduced_teacher)\n",
    "emnist_distiller.compile(\n",
    "    optimizer=Adam(),\n",
    "    metrics=[SparseCategoricalAccuracy()],\n",
    "    student_loss_fn=SparseCategoricalCrossentropy(from_logits=True),\n",
    "    distillation_loss_fn=KLDivergence(),\n",
    "    alpha=0.2,\n",
    "    temperature=3,\n",
    ")\n",
    "\n",
    "# Distill teacher to student\n",
    "emnist_reduced_teacher_history = emnist_distiller.fit(emnist_train_x, emnist_train_y, validation_data=(emnist_val_x, emnist_val_y), epochs=20)\n",
    "\n",
    "# Evaluate student on test dataset\n",
    "emnist_distiller.evaluate(emnist_test_x, emnist_test_y)\n",
    "\n",
    "plt.plot(emnist_reduced_teacher_history.history[\"sparse_categorical_accuracy\"], label = 'Train Accuracy')\n",
    "plt.plot(emnist_reduced_teacher_history.history[\"val_sparse_categorical_accuracy\"], linestyle = 'dashed', label = 'Test Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427805c6",
   "metadata": {
    "id": "06d84fb8"
   },
   "source": [
    "# Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e1a9584",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65d78af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [\n",
    "    ('gini', 3),\n",
    "    ('gini', 5),\n",
    "    ('gini', 10),\n",
    "    ('entropy', 3),\n",
    "    ('entropy', 5),\n",
    "    ('entropy', 10),\n",
    "]\n",
    "seed = 1234"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528daa70",
   "metadata": {},
   "source": [
    "## CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b9d300e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37500, 32, 32, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar_train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "44029d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples, nx, ny, dim = cifar_train_x.shape\n",
    "dt_cifar_train_x = cifar_train_x.reshape((nsamples,nx*ny*dim))\n",
    "\n",
    "nsamples, nx, ny, dim = cifar_val_x.shape\n",
    "dt_cifar_val_x = cifar_val_x.reshape((nsamples,nx*ny*dim))\n",
    "\n",
    "nsamples, nx, ny, dim = cifar_test_x.shape\n",
    "dt_cifar_test_x = cifar_test_x.reshape((nsamples,nx*ny*dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "22aecc7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree parameters: critertion - gini max depth - 3\n",
      "Training accuracy: 0.24013333333333334%\n",
      "Validation accuracy: 0.23672%\n",
      "Test accuracy: 0.2398%\n",
      "\n",
      "Tree parameters: critertion - gini max depth - 5\n",
      "Training accuracy: 0.27296%\n",
      "Validation accuracy: 0.26592%\n",
      "Test accuracy: 0.2642%\n",
      "\n",
      "Tree parameters: critertion - gini max depth - 10\n",
      "Training accuracy: 0.4394133333333333%\n",
      "Validation accuracy: 0.29176%\n",
      "Test accuracy: 0.2945%\n",
      "\n",
      "Tree parameters: critertion - entropy max depth - 3\n",
      "Training accuracy: 0.2392%\n",
      "Validation accuracy: 0.23368%\n",
      "Test accuracy: 0.2377%\n",
      "\n",
      "Tree parameters: critertion - entropy max depth - 5\n",
      "Training accuracy: 0.2736266666666667%\n",
      "Validation accuracy: 0.26096%\n",
      "Test accuracy: 0.2598%\n",
      "\n",
      "Tree parameters: critertion - entropy max depth - 10\n",
      "Training accuracy: 0.43933333333333335%\n",
      "Validation accuracy: 0.2952%\n",
      "Test accuracy: 0.3034%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for criterion, max_depth in parameters:\n",
    "    decision_tree = DecisionTreeClassifier(criterion=criterion, max_depth=max_depth, random_state=seed)\n",
    "    decision_tree.fit(dt_cifar_train_x, cifar_train_y)\n",
    "    \n",
    "    cifar_tree_train_predictions = decision_tree.predict(dt_cifar_train_x)\n",
    "    cifar_tree_val_predictions = decision_tree.predict(dt_cifar_val_x)\n",
    "    cifar_tree_test_predictions = decision_tree.predict(dt_cifar_test_x)\n",
    "    \n",
    "    cifar_tree_train_accuracy = metrics.accuracy_score(cifar_tree_train_predictions, cifar_train_y)\n",
    "    cifar_tree_val_accuracy = metrics.accuracy_score(cifar_tree_val_predictions, cifar_val_y)\n",
    "    cifar_tree_test_accuracy = metrics.accuracy_score(cifar_tree_test_predictions, cifar_test_y)\n",
    "    \n",
    "    print(f\"Tree parameters: critertion - {criterion} max depth - {max_depth}\")\n",
    "    print(f'Training accuracy: {cifar_tree_train_accuracy}%')\n",
    "    print(f'Validation accuracy: {cifar_tree_val_accuracy}%')\n",
    "    print(f'Test accuracy: {cifar_tree_test_accuracy}%')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a517933",
   "metadata": {},
   "source": [
    "## EMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8dd2f29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples, nx, ny, dim = emnist_train_x.shape\n",
    "dt_emnist_train_x = emnist_train_x.reshape((nsamples,nx*ny*dim))\n",
    "\n",
    "nsamples, nx, ny, dim = emnist_val_x.shape\n",
    "dt_emnist_val_x = emnist_val_x.reshape((nsamples,nx*ny*dim))\n",
    "\n",
    "nsamples, nx, ny, dim = emnist_test_x.shape\n",
    "dt_emnist_test_x = emnist_test_x.reshape((nsamples,nx*ny*dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c52b05ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree parameters: critertion - gini max depth - 3\n",
      "Training accuracy: 0.21027777777777779%\n",
      "Validation accuracy: 0.20631410256410257%\n",
      "Test accuracy: 0.20860576923076923%\n",
      "\n",
      "Tree parameters: critertion - gini max depth - 5\n",
      "Training accuracy: 0.4167948717948718%\n",
      "Validation accuracy: 0.412275641025641%\n",
      "Test accuracy: 0.4084615384615385%\n",
      "\n",
      "Tree parameters: critertion - gini max depth - 10\n",
      "Training accuracy: 0.6911752136752137%\n",
      "Validation accuracy: 0.6471153846153846%\n",
      "Test accuracy: 0.6442788461538461%\n",
      "\n",
      "Tree parameters: critertion - entropy max depth - 3\n",
      "Training accuracy: 0.1983119658119658%\n",
      "Validation accuracy: 0.1967948717948718%\n",
      "Test accuracy: 0.19798076923076924%\n",
      "\n",
      "Tree parameters: critertion - entropy max depth - 5\n",
      "Training accuracy: 0.4022542735042735%\n",
      "Validation accuracy: 0.3996794871794872%\n",
      "Test accuracy: 0.3971153846153846%\n",
      "\n",
      "Tree parameters: critertion - entropy max depth - 10\n",
      "Training accuracy: 0.7022008547008547%\n",
      "Validation accuracy: 0.6594551282051282%\n",
      "Test accuracy: 0.6613461538461538%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for criterion, max_depth in parameters:\n",
    "    decision_tree = DecisionTreeClassifier(criterion=criterion, max_depth=max_depth, random_state=seed)\n",
    "    decision_tree.fit(dt_emnist_train_x, emnist_train_y)\n",
    "    \n",
    "    emnist_tree_train_predictions = decision_tree.predict(dt_emnist_train_x)\n",
    "    emnist_tree_val_predictions = decision_tree.predict(dt_emnist_val_x)\n",
    "    emnist_tree_test_predictions = decision_tree.predict(dt_emnist_test_x)\n",
    "    \n",
    "    emnist_tree_train_accuracy = metrics.accuracy_score(emnist_tree_train_predictions, emnist_train_y)\n",
    "    emnist_tree_val_accuracy = metrics.accuracy_score(emnist_tree_val_predictions, emnist_val_y)\n",
    "    emnist_tree_test_accuracy = metrics.accuracy_score(emnist_tree_test_predictions, emnist_test_y)\n",
    "    \n",
    "    print(f\"Tree parameters: critertion - {criterion} max depth - {max_depth}\")\n",
    "    print(f'Training accuracy: {emnist_tree_train_accuracy}%')\n",
    "    print(f'Validation accuracy: {emnist_tree_val_accuracy}%')\n",
    "    print(f'Test accuracy: {emnist_tree_test_accuracy}%')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0296d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "umapkernel",
   "language": "python",
   "name": "umapkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
