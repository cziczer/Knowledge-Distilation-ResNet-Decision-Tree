{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33eedced",
   "metadata": {
    "id": "33eedced"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-13 14:00:54.789782: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/mmastalerczyk/opt/anaconda3/envs/umap2/lib/python3.9/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (5.1.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, \n",
    "    Dense, \n",
    "    BatchNormalization, \n",
    "    Activation, \n",
    "    MaxPool2D, \n",
    "    GlobalAveragePooling2D, \n",
    "    Add, \n",
    "    Input, \n",
    "    Flatten, \n",
    "    LeakyReLU, \n",
    "    Concatenate, \n",
    "    Dropout,\n",
    "    MaxPooling2D\n",
    ")\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.constraints import Constraint ,UnitNorm\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, Callback\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.initializers import Orthogonal\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy, KLDivergence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from io import StringIO\n",
    "import emnist as em\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "-vggzehYbJus",
   "metadata": {
    "id": "-vggzehYbJus"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d37471c",
   "metadata": {
    "id": "2d37471c"
   },
   "source": [
    "## ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b888271d",
   "metadata": {
    "id": "b888271d"
   },
   "outputs": [],
   "source": [
    "# https://arxiv.org/pdf/1512.03385.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ef02e9a",
   "metadata": {
    "id": "4ef02e9a"
   },
   "outputs": [],
   "source": [
    "def create_resnet(input_shape, class_count):\n",
    "    n = 9 # 56 layers\n",
    "    channels = [16, 32, 64]\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv2D(channels[0], kernel_size=(3, 3), padding=\"same\", kernel_initializer=\"he_normal\", kernel_regularizer=l2(1e-4))(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(tf.nn.relu)(x)\n",
    "\n",
    "    for c in channels:\n",
    "        for i in range(n):\n",
    "            subsampling = i == 0 and c > 16\n",
    "            strides = (2, 2) if subsampling else (1, 1)\n",
    "            y = Conv2D(c, kernel_size=(3, 3), padding=\"same\", strides=strides, kernel_initializer=\"he_normal\", kernel_regularizer=l2(1e-4))(x)\n",
    "            y = BatchNormalization()(y)\n",
    "            y = Activation(tf.nn.relu)(y)\n",
    "            y = Conv2D(c, kernel_size=(3, 3), padding=\"same\", kernel_initializer=\"he_normal\", kernel_regularizer=l2(1e-4))(y)\n",
    "            y = BatchNormalization()(y)        \n",
    "            if subsampling:\n",
    "                x = Conv2D(c, kernel_size=(1, 1), strides=(2, 2), padding=\"same\", kernel_initializer=\"he_normal\", kernel_regularizer=l2(1e-4))(x)\n",
    "            x = Add()([x, y])\n",
    "            x = Activation(tf.nn.relu)(x)\n",
    "\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Flatten()(x)\n",
    "    outputs = Dense(class_count, activation=tf.nn.softmax, kernel_initializer=\"he_normal\")(x)\n",
    "\n",
    "    return Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf26a4e1",
   "metadata": {
    "id": "bf26a4e1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-13 20:54:17.342490: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# PREPARE TRAINING\n",
    "lr = 0.1\n",
    "optimizer = SGD(learning_rate=lr, momentum=0.9)\n",
    "import time\n",
    "\n",
    "\n",
    "class LearningController(Callback):\n",
    "    def __init__(self, num_epoch=0, learn_minute=0):\n",
    "        self.num_epoch = num_epoch\n",
    "        self.learn_second = learn_minute * 60\n",
    "        if self.learn_second > 0:\n",
    "            print(\"Leraning rate is controled by time.\")\n",
    "        elif self.num_epoch > 0:\n",
    "            print(\"Leraning rate is controled by epoch.\")\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        if self.learn_second > 0:\n",
    "            self.start_time = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.learn_second > 0:\n",
    "            current_time = time.time()\n",
    "            if current_time - self.start_time > self.learn_second / 2:\n",
    "                self.model.optimizer.lr = lr * 0.1            \n",
    "            if current_time - self.start_time > self.learn_second * 3 / 4:\n",
    "                self.model.optimizer.lr = lr * 0.01\n",
    "                \n",
    "        elif self.num_epoch > 0:\n",
    "            if epoch > self.num_epoch / 2:\n",
    "                self.model.optimizer.lr = lr * 0.1            \n",
    "            if epoch > self.num_epoch * 3 / 4:\n",
    "                self.model.optimizer.lr = lr * 0.01\n",
    "                    \n",
    "        print('lr:%.2e' % self.model.optimizer.lr.value())\n",
    "        \n",
    "\n",
    "learn_minute = 120"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac38408",
   "metadata": {
    "id": "bac38408"
   },
   "source": [
    "## Dense NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb9a1091",
   "metadata": {
    "id": "bb9a1091"
   },
   "outputs": [],
   "source": [
    "def create_dense_nn(input_shape, class_count):\n",
    "    return Sequential(\n",
    "        [\n",
    "            Input(shape=input_shape),\n",
    "            Flatten(),\n",
    "            Dense(256),\n",
    "            Dense(class_count),\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c5ca61",
   "metadata": {
    "id": "39c5ca61"
   },
   "source": [
    "## Distiller class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3ffa5cd",
   "metadata": {
    "id": "f3ffa5cd"
   },
   "outputs": [],
   "source": [
    "class Distiller(Model):\n",
    "    def __init__(self, student, teacher):\n",
    "        super(Distiller, self).__init__()\n",
    "        self.teacher = teacher\n",
    "        self.student = student\n",
    "\n",
    "    def compile(\n",
    "        self,\n",
    "        optimizer,\n",
    "        metrics,\n",
    "        student_loss_fn,\n",
    "        distillation_loss_fn,\n",
    "        alpha=0.1,\n",
    "        temperature=3,\n",
    "    ):\n",
    "        \"\"\" Configure the distiller.\n",
    "\n",
    "        Args:\n",
    "            optimizer: Keras optimizer for the student weights\n",
    "            metrics: Keras metrics for evaluation\n",
    "            student_loss_fn: Loss function of difference between student\n",
    "                predictions and ground-truth\n",
    "            distillation_loss_fn: Loss function of difference between soft\n",
    "                student predictions and soft teacher predictions\n",
    "            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n",
    "            temperature: Temperature for softening probability distributions.\n",
    "                Larger temperature gives softer distributions.\n",
    "        \"\"\"\n",
    "        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics)\n",
    "        self.student_loss_fn = student_loss_fn\n",
    "        self.distillation_loss_fn = distillation_loss_fn\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Unpack data\n",
    "        x, y = data\n",
    "\n",
    "        # Forward pass of teacher\n",
    "        teacher_predictions = self.teacher(x, training=False)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass of student\n",
    "            student_predictions = self.student(x, training=True)\n",
    "\n",
    "            # Compute losses\n",
    "            student_loss = self.student_loss_fn(y, student_predictions)\n",
    "            distillation_loss = self.distillation_loss_fn(\n",
    "                tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n",
    "                tf.nn.softmax(student_predictions / self.temperature, axis=1),\n",
    "            )\n",
    "            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.student.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # Update the metrics configured in `compile()`.\n",
    "        self.compiled_metrics.update_state(y, student_predictions)\n",
    "\n",
    "        # Return a dict of performance\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update(\n",
    "            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n",
    "        )\n",
    "        return results\n",
    "\n",
    "    def test_step(self, data):\n",
    "        # Unpack the data\n",
    "        x, y = data\n",
    "\n",
    "        # Compute predictions\n",
    "        y_prediction = self.student(x, training=False)\n",
    "\n",
    "        # Calculate the loss\n",
    "        student_loss = self.student_loss_fn(y, y_prediction)\n",
    "\n",
    "        # Update the metrics.\n",
    "        self.compiled_metrics.update_state(y, y_prediction)\n",
    "\n",
    "        # Return a dict of performance\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update({\"student_loss\": student_loss})\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a970f3ab",
   "metadata": {
    "id": "a970f3ab"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_epoch = 192"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6375b25",
   "metadata": {
    "id": "f6375b25"
   },
   "source": [
    "## CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f889162",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 149,
     "referenced_widgets": [
      "2e81a0d3314b4ebeb867a451a4994d95",
      "f4cc2328bba14da9bc88a3e871a65630",
      "5b476977667c459e875464430baeb67d",
      "087b8374f1ae43d6ae3355970cfd3f1d",
      "55f281da8951413d881ba93c8edf7643",
      "3610d4cf85e24fc8a5c27f31e6d3c013",
      "6f70c89758db4e5fb27af895f65e7222",
      "fc3ba54af0e647008523e22f374b6bc3",
      "f482717d8df946d6b669930fe53788f6",
      "11e540259f874c7ca5ff2b64bb034ca0",
      "630d0d7038e5465ab0c7ad2833b493aa",
      "699ce7ea32ec4851ab2709f6da3ad791",
      "55b23fbf66ea4e4682405f9d65a9dd70",
      "32e9a31e881e4bc6909070a246cc8020",
      "8b0773d43b9e4014aa6f1a7e1222980e",
      "87284029504d44a2887d1d9b794c33bf",
      "8398d5fe20dd41e2ad750523d63b31b4",
      "2f98eee1a36f4caaa96c2317f7c6d45b",
      "e0acd1ec81b64d01a48161330c49583d",
      "ee1d801653a24b88a00303361acd9c4f",
      "87b78abbacd6485aaaf3b4dfc298cbef",
      "fe2825a1081e41b19f388a10af2013c6",
      "8318fe1b4e1b4bd59e3f8b608fa9be07",
      "b54faad548a540eea491eb6d8d65e20f",
      "9738eb5a1e914d5595758ebe6804b4f1",
      "bc72f078c13f4b2f91b075f9283d79b0",
      "d9d5e8655e4645039a21ef9ea9046302",
      "7fce1d456e4f45f8853eebad35368f32",
      "869e5601ae4549019ff4d83a9576d16a",
      "2a60002d7cac43f2b1a086c42e98a363",
      "0e6ce32b01f543069113f976af76412e",
      "812a586dd4774aa99d01f8cc6e28c736",
      "cc6040969c6044e7b9b9a7f6eff8fac8",
      "9c162973a23e450ebf0cf815459978d9",
      "4f3eb6a685d443a98050c810b80c66eb",
      "5592de8c98914247b347de98e60fcbf8",
      "fc306416ec354c4886302fed4a1e5d3d",
      "883354a9df79462398addebf3dd07901",
      "34f8418e8f974779934c426b2e92e3c4",
      "05c7287a847b4d3aa0dc966b713be867",
      "7f2db97f8af14336b6c7ccf29483ed2d",
      "44fbb912208c445d8b47656d71663a9c",
      "f5bcfbe6c8124c8890c566232dce6fe5",
      "a3ae645c0e434144a5f87ed3ea273c8c",
      "10b3883a6a3746e281d0415d5a813d5f",
      "eeadb96745ac4c01955a9cc49ca70422",
      "418e5953a39346cba2e511d61ca9ba3b",
      "b74e7aa6999643ff938a8f12ca01af38",
      "ed09fe40563c415aae8464167b4502a1",
      "24c564c16d804d8ebd631cb1bb186852",
      "c699df79ed3349039ff734532933edf9",
      "d5da1c4a994d48369be9ce70666d1e14",
      "064675c8f95f40f4aaa8f122692d9e65",
      "99f505890c754e04b507ee76ec4a56ef",
      "5ac697d6f3e247d5a111d561bc56d9ac",
      "aed4d4bb68af4fbaaab89d6b5e2af97b",
      "910a4b6a484e45d4b2cddd80aae33268",
      "e7dbf5bd95e2422eb5086c393cab4cdd",
      "02b668a13d7e4cb19f7b635104cd9c4b",
      "91e91ec17f7843e4b093fba531288091",
      "f5f95e763eb04036b3b3f2322143c2cd",
      "3f19b9bc8f6d4d8c983ce1d34f81a767",
      "18b2586c05264c2da5027beccd325db4",
      "a709771612f34d748e06b1c9e072847c",
      "3b72750205d5477f9dd20df186542056",
      "f5054cffa6ea4f12be8488c44db3c9f2",
      "514543680d0e4453a1613e25aff67357",
      "70cfb572baaa44a889646639dccfaa91",
      "4b5eb12b8291421088164d9da6247460",
      "46f86d65327e42aabe6664ff032dbeaa",
      "e566d55b87e0477d8b7dfa03e93f8f23",
      "be9b75b7518c4b80b74b6b9c13fd0427",
      "ce9359d077914d4c9df88d09cfce6ad3",
      "efc52e4a329d46d0b684ff358cb66b5a",
      "0244cba9e3ba43fa8d86e606a0a53520",
      "87ef0a051cd0461f9de3edad1e8ba990",
      "4a10199c947949abab900fb449715713",
      "dec8fb115fea4557aa0cc826b30a0321",
      "0c9b216f5b36425895e2300f4d341492",
      "c944bafc8af444eb8f6240139b9a8e6c",
      "eba48eb71dc04a13aab0ea8bacd33a40",
      "15d00b142cd94b03911a2fa477c9e395",
      "b699f772beb2418c9827eb4beebb8711",
      "47ddde0b18c844a0b0243c9f1326d8c5",
      "0c9bdda417af4101aed2083a8e09da4c",
      "bafb106450f84b009bac88d64659f41c",
      "f644873732224e609673d9c66b189688",
      "6931537da6aa4c14820c98f8ded71905"
     ]
    },
    "id": "2f889162",
    "outputId": "ad543893-4d6b-4863-95fe-0c6e8e40d38e"
   },
   "outputs": [],
   "source": [
    "cifar = tfds.load('cifar10', as_supervised = True, batch_size = -1)\n",
    "cifar_test, cifar_train = cifar['test'], cifar['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2db0adc8",
   "metadata": {
    "id": "2db0adc8"
   },
   "outputs": [],
   "source": [
    "cifar_train_x = cifar_train[0].numpy() / 255\n",
    "cifar_train_y = cifar_train[1].numpy()\n",
    "cifar_test_x = cifar_test[0].numpy() / 255\n",
    "cifar_test_y = cifar_test[1].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad9a91f0",
   "metadata": {
    "id": "ad9a91f0"
   },
   "outputs": [],
   "source": [
    "cifar_train_x, cifar_val_x, cifar_train_y, cifar_val_y = train_test_split(cifar_train_x, cifar_train_y, \n",
    "    test_size=0.25, random_state= 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235aff14",
   "metadata": {
    "id": "235aff14"
   },
   "source": [
    "## Teacher "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8a1784f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a8a1784f",
    "outputId": "255e037d-20ed-4827-f365-b81b1be98289",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 32, 32, 16)   448         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 32, 32, 16)  64          ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 32, 32, 16)   0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 32, 32, 16)   2320        ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 32, 32, 16)   0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 32, 32, 16)   0           ['activation[0][0]',             \n",
      "                                                                  'batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 32, 32, 16)   0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 32, 32, 16)   0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 32, 32, 16)   0           ['activation_2[0][0]',           \n",
      "                                                                  'batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 32, 32, 16)   0           ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 32, 32, 16)   0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 32, 32, 16)   0           ['activation_4[0][0]',           \n",
      "                                                                  'batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 32, 32, 16)   0           ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_7[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 32, 32, 16)   0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_8[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 32, 32, 16)   0           ['activation_6[0][0]',           \n",
      "                                                                  'batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 32, 32, 16)   0           ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_9[0][0]']               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 32, 32, 16)   0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_9[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 32, 32, 16)  64          ['conv2d_10[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 32, 32, 16)   0           ['activation_8[0][0]',           \n",
      "                                                                  'batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 32, 32, 16)   0           ['add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_10[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 32, 32, 16)  64          ['conv2d_11[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_11[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 32, 32, 16)  64          ['conv2d_12[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 32, 32, 16)   0           ['activation_10[0][0]',          \n",
      "                                                                  'batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 32, 32, 16)   0           ['add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_12[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 32, 32, 16)  64          ['conv2d_13[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_13[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 32, 32, 16)  64          ['conv2d_14[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 32, 32, 16)   0           ['activation_12[0][0]',          \n",
      "                                                                  'batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " activation_14 (Activation)     (None, 32, 32, 16)   0           ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_14[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 32, 32, 16)  64          ['conv2d_15[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_15 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_15[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 32, 32, 16)  64          ['conv2d_16[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 32, 32, 16)   0           ['activation_14[0][0]',          \n",
      "                                                                  'batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 32, 32, 16)   0           ['add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_16[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 32, 32, 16)  64          ['conv2d_17[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_17[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 32, 32, 16)  64          ['conv2d_18[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 32, 32, 16)   0           ['activation_16[0][0]',          \n",
      "                                                                  'batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " activation_18 (Activation)     (None, 32, 32, 16)   0           ['add_8[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 16, 16, 32)   4640        ['activation_18[0][0]']          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 16, 16, 32)  128         ['conv2d_19[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_19 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_19[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 16, 16, 32)   544         ['activation_18[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 16, 16, 32)  128         ['conv2d_20[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 16, 16, 32)   0           ['conv2d_21[0][0]',              \n",
      "                                                                  'batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " activation_20 (Activation)     (None, 16, 16, 32)   0           ['add_9[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_20[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 16, 16, 32)  128         ['conv2d_22[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_21 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_21[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 16, 16, 32)  128         ['conv2d_23[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 16, 16, 32)   0           ['activation_20[0][0]',          \n",
      "                                                                  'batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " activation_22 (Activation)     (None, 16, 16, 32)   0           ['add_10[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_22[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 16, 16, 32)  128         ['conv2d_24[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_23 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_23[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_24 (BatchN  (None, 16, 16, 32)  128         ['conv2d_25[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 16, 16, 32)   0           ['activation_22[0][0]',          \n",
      "                                                                  'batch_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " activation_24 (Activation)     (None, 16, 16, 32)   0           ['add_11[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_24[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 16, 16, 32)  128         ['conv2d_26[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_25 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_25[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_26 (BatchN  (None, 16, 16, 32)  128         ['conv2d_27[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 16, 16, 32)   0           ['activation_24[0][0]',          \n",
      "                                                                  'batch_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " activation_26 (Activation)     (None, 16, 16, 32)   0           ['add_12[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_26[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_27 (BatchN  (None, 16, 16, 32)  128         ['conv2d_28[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_27 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_27[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_28 (BatchN  (None, 16, 16, 32)  128         ['conv2d_29[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 16, 16, 32)   0           ['activation_26[0][0]',          \n",
      "                                                                  'batch_normalization_28[0][0]'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " activation_28 (Activation)     (None, 16, 16, 32)   0           ['add_13[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_28[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_29 (BatchN  (None, 16, 16, 32)  128         ['conv2d_30[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_29 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_29[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_29[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_30 (BatchN  (None, 16, 16, 32)  128         ['conv2d_31[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 16, 16, 32)   0           ['activation_28[0][0]',          \n",
      "                                                                  'batch_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      " activation_30 (Activation)     (None, 16, 16, 32)   0           ['add_14[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_30[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_31 (BatchN  (None, 16, 16, 32)  128         ['conv2d_32[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_31 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_31[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_32 (BatchN  (None, 16, 16, 32)  128         ['conv2d_33[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, 16, 16, 32)   0           ['activation_30[0][0]',          \n",
      "                                                                  'batch_normalization_32[0][0]'] \n",
      "                                                                                                  \n",
      " activation_32 (Activation)     (None, 16, 16, 32)   0           ['add_15[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_32[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_33 (BatchN  (None, 16, 16, 32)  128         ['conv2d_34[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_33 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_33[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_35 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_33[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_34 (BatchN  (None, 16, 16, 32)  128         ['conv2d_35[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_16 (Add)                   (None, 16, 16, 32)   0           ['activation_32[0][0]',          \n",
      "                                                                  'batch_normalization_34[0][0]'] \n",
      "                                                                                                  \n",
      " activation_34 (Activation)     (None, 16, 16, 32)   0           ['add_16[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_36 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_34[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_35 (BatchN  (None, 16, 16, 32)  128         ['conv2d_36[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_35 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_35[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_37 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_35[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_36 (BatchN  (None, 16, 16, 32)  128         ['conv2d_37[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_17 (Add)                   (None, 16, 16, 32)   0           ['activation_34[0][0]',          \n",
      "                                                                  'batch_normalization_36[0][0]'] \n",
      "                                                                                                  \n",
      " activation_36 (Activation)     (None, 16, 16, 32)   0           ['add_17[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_38 (Conv2D)             (None, 8, 8, 64)     18496       ['activation_36[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_37 (BatchN  (None, 8, 8, 64)    256         ['conv2d_38[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_37 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_37[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_39 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_37[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_40 (Conv2D)             (None, 8, 8, 64)     2112        ['activation_36[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_38 (BatchN  (None, 8, 8, 64)    256         ['conv2d_39[0][0]']              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_18 (Add)                   (None, 8, 8, 64)     0           ['conv2d_40[0][0]',              \n",
      "                                                                  'batch_normalization_38[0][0]'] \n",
      "                                                                                                  \n",
      " activation_38 (Activation)     (None, 8, 8, 64)     0           ['add_18[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_41 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_38[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_39 (BatchN  (None, 8, 8, 64)    256         ['conv2d_41[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_39 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_39[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_42 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_39[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_40 (BatchN  (None, 8, 8, 64)    256         ['conv2d_42[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_19 (Add)                   (None, 8, 8, 64)     0           ['activation_38[0][0]',          \n",
      "                                                                  'batch_normalization_40[0][0]'] \n",
      "                                                                                                  \n",
      " activation_40 (Activation)     (None, 8, 8, 64)     0           ['add_19[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_43 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_40[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_41 (BatchN  (None, 8, 8, 64)    256         ['conv2d_43[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_41 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_41[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_44 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_41[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_42 (BatchN  (None, 8, 8, 64)    256         ['conv2d_44[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_20 (Add)                   (None, 8, 8, 64)     0           ['activation_40[0][0]',          \n",
      "                                                                  'batch_normalization_42[0][0]'] \n",
      "                                                                                                  \n",
      " activation_42 (Activation)     (None, 8, 8, 64)     0           ['add_20[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_45 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_42[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_43 (BatchN  (None, 8, 8, 64)    256         ['conv2d_45[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_43 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_43[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_46 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_43[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_44 (BatchN  (None, 8, 8, 64)    256         ['conv2d_46[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_21 (Add)                   (None, 8, 8, 64)     0           ['activation_42[0][0]',          \n",
      "                                                                  'batch_normalization_44[0][0]'] \n",
      "                                                                                                  \n",
      " activation_44 (Activation)     (None, 8, 8, 64)     0           ['add_21[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_47 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_44[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_45 (BatchN  (None, 8, 8, 64)    256         ['conv2d_47[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_45 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_45[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_48 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_45[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_46 (BatchN  (None, 8, 8, 64)    256         ['conv2d_48[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_22 (Add)                   (None, 8, 8, 64)     0           ['activation_44[0][0]',          \n",
      "                                                                  'batch_normalization_46[0][0]'] \n",
      "                                                                                                  \n",
      " activation_46 (Activation)     (None, 8, 8, 64)     0           ['add_22[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_49 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_46[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_47 (BatchN  (None, 8, 8, 64)    256         ['conv2d_49[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_47 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_47[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_50 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_47[0][0]']          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " batch_normalization_48 (BatchN  (None, 8, 8, 64)    256         ['conv2d_50[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_23 (Add)                   (None, 8, 8, 64)     0           ['activation_46[0][0]',          \n",
      "                                                                  'batch_normalization_48[0][0]'] \n",
      "                                                                                                  \n",
      " activation_48 (Activation)     (None, 8, 8, 64)     0           ['add_23[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_51 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_48[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_49 (BatchN  (None, 8, 8, 64)    256         ['conv2d_51[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_49 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_49[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_52 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_49[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_50 (BatchN  (None, 8, 8, 64)    256         ['conv2d_52[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_24 (Add)                   (None, 8, 8, 64)     0           ['activation_48[0][0]',          \n",
      "                                                                  'batch_normalization_50[0][0]'] \n",
      "                                                                                                  \n",
      " activation_50 (Activation)     (None, 8, 8, 64)     0           ['add_24[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_53 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_50[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_51 (BatchN  (None, 8, 8, 64)    256         ['conv2d_53[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_51 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_51[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_54 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_51[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_52 (BatchN  (None, 8, 8, 64)    256         ['conv2d_54[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_25 (Add)                   (None, 8, 8, 64)     0           ['activation_50[0][0]',          \n",
      "                                                                  'batch_normalization_52[0][0]'] \n",
      "                                                                                                  \n",
      " activation_52 (Activation)     (None, 8, 8, 64)     0           ['add_25[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_55 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_52[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_53 (BatchN  (None, 8, 8, 64)    256         ['conv2d_55[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_53 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_53[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_56 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_53[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_54 (BatchN  (None, 8, 8, 64)    256         ['conv2d_56[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_26 (Add)                   (None, 8, 8, 64)     0           ['activation_52[0][0]',          \n",
      "                                                                  'batch_normalization_54[0][0]'] \n",
      "                                                                                                  \n",
      " activation_54 (Activation)     (None, 8, 8, 64)     0           ['add_26[0][0]']                 \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 64)          0           ['activation_54[0][0]']          \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 64)           0           ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 10)           650         ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 861,770\n",
      "Trainable params: 857,706\n",
      "Non-trainable params: 4,064\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cifar_teacher_model = create_resnet((32, 32, 3), 10)\n",
    "cifar_teacher_model.type = \"cifar_resnet\"\n",
    "cifar_teacher_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1db50d60",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1db50d60",
    "outputId": "49decba2-cc69-429d-fd7c-37afdb44ee03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leraning rate is controled by epoch.\n"
     ]
    }
   ],
   "source": [
    "cifar_teacher_model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "cifar_teacher_path = \"ResNet-for-CIFAR-10.h5\"\n",
    "cifar_teacher_checkpoint = ModelCheckpoint(filepath = cifar_teacher_path, monitor=\"val_loss\", verbose=1, save_best_only=True)\n",
    "cifar_teacher_learning_controller = LearningController(num_epoch)\n",
    "cifar_teacher_callbacks = [cifar_teacher_checkpoint, cifar_teacher_learning_controller]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1bf85d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5c1bf85d",
    "outputId": "c8d73ea7-c34b-464c-ad99-3d6667c956d0",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 3.1918 - accuracy: 0.1179\n",
      "Epoch 1: val_loss improved from inf to 2.88271, saving model to ResNet-for-CIFAR-10.h5\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 39s 85ms/step - loss: 3.1918 - accuracy: 0.1179 - val_loss: 2.8827 - val_accuracy: 0.1491\n",
      "Epoch 2/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 2.6052 - accuracy: 0.2416\n",
      "Epoch 2: val_loss did not improve from 2.88271\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 23s 79ms/step - loss: 2.6052 - accuracy: 0.2416 - val_loss: 5.2574 - val_accuracy: 0.1370\n",
      "Epoch 3/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 2.3139 - accuracy: 0.3338\n",
      "Epoch 3: val_loss improved from 2.88271 to 2.27426, saving model to ResNet-for-CIFAR-10.h5\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 81ms/step - loss: 2.3139 - accuracy: 0.3338 - val_loss: 2.2743 - val_accuracy: 0.3466\n",
      "Epoch 4/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 2.0615 - accuracy: 0.4184\n",
      "Epoch 4: val_loss improved from 2.27426 to 2.06795, saving model to ResNet-for-CIFAR-10.h5\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 2.0615 - accuracy: 0.4184 - val_loss: 2.0680 - val_accuracy: 0.4315\n",
      "Epoch 5/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 1.8691 - accuracy: 0.4833\n",
      "Epoch 5: val_loss improved from 2.06795 to 1.89539, saving model to ResNet-for-CIFAR-10.h5\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 1.8691 - accuracy: 0.4833 - val_loss: 1.8954 - val_accuracy: 0.4666\n",
      "Epoch 6/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 1.7268 - accuracy: 0.5275\n",
      "Epoch 6: val_loss did not improve from 1.89539\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 1.7268 - accuracy: 0.5275 - val_loss: 1.9798 - val_accuracy: 0.4403\n",
      "Epoch 7/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 1.6047 - accuracy: 0.5620\n",
      "Epoch 7: val_loss improved from 1.89539 to 1.82620, saving model to ResNet-for-CIFAR-10.h5\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 1.6047 - accuracy: 0.5620 - val_loss: 1.8262 - val_accuracy: 0.4842\n",
      "Epoch 8/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 1.4945 - accuracy: 0.5945\n",
      "Epoch 8: val_loss did not improve from 1.82620\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 1.4945 - accuracy: 0.5945 - val_loss: 1.9409 - val_accuracy: 0.4742\n",
      "Epoch 9/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 1.3921 - accuracy: 0.6244\n",
      "Epoch 9: val_loss improved from 1.82620 to 1.53298, saving model to ResNet-for-CIFAR-10.h5\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 1.3921 - accuracy: 0.6244 - val_loss: 1.5330 - val_accuracy: 0.5800\n",
      "Epoch 10/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 1.2960 - accuracy: 0.6564\n",
      "Epoch 10: val_loss improved from 1.53298 to 1.50381, saving model to ResNet-for-CIFAR-10.h5\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 1.2960 - accuracy: 0.6564 - val_loss: 1.5038 - val_accuracy: 0.5868\n",
      "Epoch 11/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 1.2194 - accuracy: 0.6812\n",
      "Epoch 11: val_loss did not improve from 1.50381\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 1.2194 - accuracy: 0.6812 - val_loss: 1.8787 - val_accuracy: 0.5110\n",
      "Epoch 12/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 1.1505 - accuracy: 0.7001\n",
      "Epoch 12: val_loss did not improve from 1.50381\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 1.1505 - accuracy: 0.7001 - val_loss: 1.5330 - val_accuracy: 0.5998\n",
      "Epoch 13/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 1.0771 - accuracy: 0.7253\n",
      "Epoch 13: val_loss did not improve from 1.50381\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 1.0771 - accuracy: 0.7253 - val_loss: 1.9867 - val_accuracy: 0.5150\n",
      "Epoch 14/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 1.0232 - accuracy: 0.7458\n",
      "Epoch 14: val_loss did not improve from 1.50381\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 1.0232 - accuracy: 0.7458 - val_loss: 1.6648 - val_accuracy: 0.5861\n",
      "Epoch 15/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.9726 - accuracy: 0.7618\n",
      "Epoch 15: val_loss improved from 1.50381 to 1.41868, saving model to ResNet-for-CIFAR-10.h5\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 84ms/step - loss: 0.9726 - accuracy: 0.7618 - val_loss: 1.4187 - val_accuracy: 0.6374\n",
      "Epoch 16/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.9272 - accuracy: 0.7787\n",
      "Epoch 16: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.9272 - accuracy: 0.7787 - val_loss: 1.6560 - val_accuracy: 0.5805\n",
      "Epoch 17/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.8940 - accuracy: 0.7930\n",
      "Epoch 17: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.8940 - accuracy: 0.7930 - val_loss: 1.4201 - val_accuracy: 0.6424\n",
      "Epoch 18/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.8684 - accuracy: 0.8024\n",
      "Epoch 18: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.8684 - accuracy: 0.8024 - val_loss: 1.6806 - val_accuracy: 0.5995\n",
      "Epoch 19/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.8196 - accuracy: 0.8225\n",
      "Epoch 19: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.8196 - accuracy: 0.8225 - val_loss: 1.5487 - val_accuracy: 0.6208\n",
      "Epoch 20/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.8018 - accuracy: 0.8306\n",
      "Epoch 20: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.8018 - accuracy: 0.8306 - val_loss: 1.7432 - val_accuracy: 0.6138\n",
      "Epoch 21/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.7806 - accuracy: 0.8418\n",
      "Epoch 21: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.7806 - accuracy: 0.8418 - val_loss: 2.1551 - val_accuracy: 0.5581\n",
      "Epoch 22/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.7469 - accuracy: 0.8575\n",
      "Epoch 22: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.7469 - accuracy: 0.8575 - val_loss: 1.5216 - val_accuracy: 0.6615\n",
      "Epoch 23/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.7355 - accuracy: 0.8639\n",
      "Epoch 23: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.7355 - accuracy: 0.8639 - val_loss: 1.7924 - val_accuracy: 0.6474\n",
      "Epoch 24/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.7391 - accuracy: 0.8665\n",
      "Epoch 24: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.7391 - accuracy: 0.8665 - val_loss: 1.7142 - val_accuracy: 0.6447\n",
      "Epoch 25/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.7254 - accuracy: 0.8763\n",
      "Epoch 25: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.7254 - accuracy: 0.8763 - val_loss: 1.6980 - val_accuracy: 0.6262\n",
      "Epoch 26/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.7194 - accuracy: 0.8825\n",
      "Epoch 26: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.7194 - accuracy: 0.8825 - val_loss: 1.5101 - val_accuracy: 0.6770\n",
      "Epoch 27/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.7057 - accuracy: 0.8903\n",
      "Epoch 27: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.7057 - accuracy: 0.8903 - val_loss: 1.5294 - val_accuracy: 0.6841\n",
      "Epoch 28/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6989 - accuracy: 0.8975\n",
      "Epoch 28: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.6989 - accuracy: 0.8975 - val_loss: 2.1304 - val_accuracy: 0.5932\n",
      "Epoch 29/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6832 - accuracy: 0.9045\n",
      "Epoch 29: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.6832 - accuracy: 0.9045 - val_loss: 1.8446 - val_accuracy: 0.6630\n",
      "Epoch 30/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6904 - accuracy: 0.9046\n",
      "Epoch 30: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.6904 - accuracy: 0.9046 - val_loss: 1.8573 - val_accuracy: 0.6537\n",
      "Epoch 31/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6958 - accuracy: 0.9069\n",
      "Epoch 31: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.6958 - accuracy: 0.9069 - val_loss: 2.1044 - val_accuracy: 0.6039\n",
      "Epoch 32/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6762 - accuracy: 0.9147\n",
      "Epoch 32: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.6762 - accuracy: 0.9147 - val_loss: 1.7871 - val_accuracy: 0.6634\n",
      "Epoch 33/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6800 - accuracy: 0.9152\n",
      "Epoch 33: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.6800 - accuracy: 0.9152 - val_loss: 2.0300 - val_accuracy: 0.6188\n",
      "Epoch 34/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6871 - accuracy: 0.9136\n",
      "Epoch 34: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.6871 - accuracy: 0.9136 - val_loss: 1.7708 - val_accuracy: 0.6721\n",
      "Epoch 35/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6665 - accuracy: 0.9229\n",
      "Epoch 35: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.6665 - accuracy: 0.9229 - val_loss: 2.3873 - val_accuracy: 0.6034\n",
      "Epoch 36/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6623 - accuracy: 0.9266\n",
      "Epoch 36: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.6623 - accuracy: 0.9266 - val_loss: 2.0952 - val_accuracy: 0.6494\n",
      "Epoch 37/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6941 - accuracy: 0.9163\n",
      "Epoch 37: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.6941 - accuracy: 0.9163 - val_loss: 2.1040 - val_accuracy: 0.6342\n",
      "Epoch 38/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6697 - accuracy: 0.9273\n",
      "Epoch 38: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.6697 - accuracy: 0.9273 - val_loss: 2.3138 - val_accuracy: 0.6245\n",
      "Epoch 39/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6407 - accuracy: 0.9365\n",
      "Epoch 39: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.6407 - accuracy: 0.9365 - val_loss: 2.2188 - val_accuracy: 0.6406\n",
      "Epoch 40/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6877 - accuracy: 0.9200\n",
      "Epoch 40: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.6877 - accuracy: 0.9200 - val_loss: 2.4700 - val_accuracy: 0.5822\n",
      "Epoch 41/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6546 - accuracy: 0.9331\n",
      "Epoch 41: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.6546 - accuracy: 0.9331 - val_loss: 2.0275 - val_accuracy: 0.6318\n",
      "Epoch 42/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6466 - accuracy: 0.9362\n",
      "Epoch 42: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.6466 - accuracy: 0.9362 - val_loss: 2.0782 - val_accuracy: 0.6431\n",
      "Epoch 43/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6540 - accuracy: 0.9334\n",
      "Epoch 43: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.6540 - accuracy: 0.9334 - val_loss: 2.0936 - val_accuracy: 0.6375\n",
      "Epoch 44/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6313 - accuracy: 0.9405\n",
      "Epoch 44: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.6313 - accuracy: 0.9405 - val_loss: 1.9109 - val_accuracy: 0.6658\n",
      "Epoch 45/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6676 - accuracy: 0.9287\n",
      "Epoch 45: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.6676 - accuracy: 0.9287 - val_loss: 2.1053 - val_accuracy: 0.6563\n",
      "Epoch 46/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6628 - accuracy: 0.9325\n",
      "Epoch 46: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.6628 - accuracy: 0.9325 - val_loss: 2.1367 - val_accuracy: 0.6670\n",
      "Epoch 47/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6445 - accuracy: 0.9389\n",
      "Epoch 47: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.6445 - accuracy: 0.9389 - val_loss: 1.9396 - val_accuracy: 0.6630\n",
      "Epoch 48/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6552 - accuracy: 0.9345\n",
      "Epoch 48: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.6552 - accuracy: 0.9345 - val_loss: 1.8232 - val_accuracy: 0.6877\n",
      "Epoch 49/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6464 - accuracy: 0.9392\n",
      "Epoch 49: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.6464 - accuracy: 0.9392 - val_loss: 3.3727 - val_accuracy: 0.5866\n",
      "Epoch 50/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6359 - accuracy: 0.9429\n",
      "Epoch 50: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.6359 - accuracy: 0.9429 - val_loss: 2.3727 - val_accuracy: 0.6097\n",
      "Epoch 51/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6445 - accuracy: 0.9388\n",
      "Epoch 51: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.6445 - accuracy: 0.9388 - val_loss: 2.1203 - val_accuracy: 0.6631\n",
      "Epoch 52/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6543 - accuracy: 0.9361\n",
      "Epoch 52: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.6543 - accuracy: 0.9361 - val_loss: 1.8610 - val_accuracy: 0.6835\n",
      "Epoch 53/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6317 - accuracy: 0.9434\n",
      "Epoch 53: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.6317 - accuracy: 0.9434 - val_loss: 1.9115 - val_accuracy: 0.6764\n",
      "Epoch 54/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6291 - accuracy: 0.9437\n",
      "Epoch 54: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.6291 - accuracy: 0.9437 - val_loss: 2.4907 - val_accuracy: 0.6150\n",
      "Epoch 55/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6468 - accuracy: 0.9384\n",
      "Epoch 55: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.6468 - accuracy: 0.9384 - val_loss: 2.1488 - val_accuracy: 0.6400\n",
      "Epoch 56/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6351 - accuracy: 0.9447\n",
      "Epoch 56: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.6351 - accuracy: 0.9447 - val_loss: 1.7997 - val_accuracy: 0.6937\n",
      "Epoch 57/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6290 - accuracy: 0.9438\n",
      "Epoch 57: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.6290 - accuracy: 0.9438 - val_loss: 2.9464 - val_accuracy: 0.5633\n",
      "Epoch 58/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6362 - accuracy: 0.9444\n",
      "Epoch 58: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.6362 - accuracy: 0.9444 - val_loss: 2.2587 - val_accuracy: 0.6262\n",
      "Epoch 59/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6177 - accuracy: 0.9477\n",
      "Epoch 59: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.6177 - accuracy: 0.9477 - val_loss: 1.7343 - val_accuracy: 0.6994\n",
      "Epoch 60/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6285 - accuracy: 0.9428\n",
      "Epoch 60: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.6285 - accuracy: 0.9428 - val_loss: 2.0932 - val_accuracy: 0.6676\n",
      "Epoch 61/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6565 - accuracy: 0.9375\n",
      "Epoch 61: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.6565 - accuracy: 0.9375 - val_loss: 2.3934 - val_accuracy: 0.6246\n",
      "Epoch 62/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6269 - accuracy: 0.9498\n",
      "Epoch 62: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.6269 - accuracy: 0.9498 - val_loss: 2.0775 - val_accuracy: 0.6679\n",
      "Epoch 63/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6330 - accuracy: 0.9453\n",
      "Epoch 63: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.6330 - accuracy: 0.9453 - val_loss: 2.5934 - val_accuracy: 0.6138\n",
      "Epoch 64/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6234 - accuracy: 0.9482\n",
      "Epoch 64: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.6234 - accuracy: 0.9482 - val_loss: 2.0811 - val_accuracy: 0.6391\n",
      "Epoch 65/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6252 - accuracy: 0.9460\n",
      "Epoch 65: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.6252 - accuracy: 0.9460 - val_loss: 2.1230 - val_accuracy: 0.6362\n",
      "Epoch 66/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6277 - accuracy: 0.9475\n",
      "Epoch 66: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.6277 - accuracy: 0.9475 - val_loss: 2.0373 - val_accuracy: 0.6705\n",
      "Epoch 67/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6213 - accuracy: 0.9484\n",
      "Epoch 67: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.6213 - accuracy: 0.9484 - val_loss: 1.8831 - val_accuracy: 0.6948\n",
      "Epoch 68/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6168 - accuracy: 0.9509\n",
      "Epoch 68: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.6168 - accuracy: 0.9509 - val_loss: 1.7981 - val_accuracy: 0.6872\n",
      "Epoch 69/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6292 - accuracy: 0.9441\n",
      "Epoch 69: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.6292 - accuracy: 0.9441 - val_loss: 2.9461 - val_accuracy: 0.6009\n",
      "Epoch 70/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6153 - accuracy: 0.9506\n",
      "Epoch 70: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.6153 - accuracy: 0.9506 - val_loss: 1.9424 - val_accuracy: 0.6769\n",
      "Epoch 71/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6098 - accuracy: 0.9511\n",
      "Epoch 71: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.6098 - accuracy: 0.9511 - val_loss: 2.0407 - val_accuracy: 0.6669\n",
      "Epoch 72/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6121 - accuracy: 0.9485\n",
      "Epoch 72: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.6121 - accuracy: 0.9485 - val_loss: 1.6882 - val_accuracy: 0.7121\n",
      "Epoch 73/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6067 - accuracy: 0.9506\n",
      "Epoch 73: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.6067 - accuracy: 0.9506 - val_loss: 10.8542 - val_accuracy: 0.3623\n",
      "Epoch 74/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5985 - accuracy: 0.9519\n",
      "Epoch 74: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.5985 - accuracy: 0.9519 - val_loss: 1.7249 - val_accuracy: 0.6914\n",
      "Epoch 75/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6078 - accuracy: 0.9499\n",
      "Epoch 75: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.6078 - accuracy: 0.9499 - val_loss: 1.8052 - val_accuracy: 0.7070\n",
      "Epoch 76/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6048 - accuracy: 0.9511\n",
      "Epoch 76: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.6048 - accuracy: 0.9511 - val_loss: 1.9919 - val_accuracy: 0.7009\n",
      "Epoch 77/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6039 - accuracy: 0.9498\n",
      "Epoch 77: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.6039 - accuracy: 0.9498 - val_loss: 2.0438 - val_accuracy: 0.6634\n",
      "Epoch 78/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6017 - accuracy: 0.9519\n",
      "Epoch 78: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.6017 - accuracy: 0.9519 - val_loss: 2.6508 - val_accuracy: 0.5978\n",
      "Epoch 79/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6025 - accuracy: 0.9491\n",
      "Epoch 79: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.6025 - accuracy: 0.9491 - val_loss: 1.7375 - val_accuracy: 0.7162\n",
      "Epoch 80/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6015 - accuracy: 0.9508\n",
      "Epoch 80: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.6015 - accuracy: 0.9508 - val_loss: 1.7102 - val_accuracy: 0.7273\n",
      "Epoch 81/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5962 - accuracy: 0.9516\n",
      "Epoch 81: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.5962 - accuracy: 0.9516 - val_loss: 1.8432 - val_accuracy: 0.7065\n",
      "Epoch 82/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6092 - accuracy: 0.9481\n",
      "Epoch 82: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.6092 - accuracy: 0.9481 - val_loss: 1.7384 - val_accuracy: 0.7222\n",
      "Epoch 83/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5881 - accuracy: 0.9555\n",
      "Epoch 83: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.5881 - accuracy: 0.9555 - val_loss: 1.9328 - val_accuracy: 0.6791\n",
      "Epoch 84/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5989 - accuracy: 0.9507\n",
      "Epoch 84: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.5989 - accuracy: 0.9507 - val_loss: 2.3796 - val_accuracy: 0.6126\n",
      "Epoch 85/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5896 - accuracy: 0.9540\n",
      "Epoch 85: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.5896 - accuracy: 0.9540 - val_loss: 1.9514 - val_accuracy: 0.6921\n",
      "Epoch 86/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5810 - accuracy: 0.9564\n",
      "Epoch 86: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.5810 - accuracy: 0.9564 - val_loss: 1.6803 - val_accuracy: 0.7267\n",
      "Epoch 87/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5909 - accuracy: 0.9521\n",
      "Epoch 87: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.5909 - accuracy: 0.9521 - val_loss: 1.7684 - val_accuracy: 0.7134\n",
      "Epoch 88/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6104 - accuracy: 0.9459\n",
      "Epoch 88: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.6104 - accuracy: 0.9459 - val_loss: 1.7418 - val_accuracy: 0.7125\n",
      "Epoch 89/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5954 - accuracy: 0.9526\n",
      "Epoch 89: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.5954 - accuracy: 0.9526 - val_loss: 2.0336 - val_accuracy: 0.6551\n",
      "Epoch 90/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5815 - accuracy: 0.9574\n",
      "Epoch 90: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.5815 - accuracy: 0.9574 - val_loss: 1.9568 - val_accuracy: 0.6770\n",
      "Epoch 91/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5741 - accuracy: 0.9578\n",
      "Epoch 91: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.5741 - accuracy: 0.9578 - val_loss: 1.9982 - val_accuracy: 0.6879\n",
      "Epoch 92/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5944 - accuracy: 0.9499\n",
      "Epoch 92: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.5944 - accuracy: 0.9499 - val_loss: 2.1942 - val_accuracy: 0.6634\n",
      "Epoch 93/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5657 - accuracy: 0.9603\n",
      "Epoch 93: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.5657 - accuracy: 0.9603 - val_loss: 1.7536 - val_accuracy: 0.7033\n",
      "Epoch 94/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5806 - accuracy: 0.9530\n",
      "Epoch 94: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.5806 - accuracy: 0.9530 - val_loss: 1.9369 - val_accuracy: 0.6892\n",
      "Epoch 95/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5905 - accuracy: 0.9505\n",
      "Epoch 95: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.5905 - accuracy: 0.9505 - val_loss: 2.7384 - val_accuracy: 0.6382\n",
      "Epoch 96/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5763 - accuracy: 0.9569\n",
      "Epoch 96: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.5763 - accuracy: 0.9569 - val_loss: 2.0121 - val_accuracy: 0.6704\n",
      "Epoch 97/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5780 - accuracy: 0.9550\n",
      "Epoch 97: val_loss did not improve from 1.41868\n",
      "lr:1.00e-01\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.5780 - accuracy: 0.9550 - val_loss: 1.8877 - val_accuracy: 0.6738\n",
      "Epoch 98/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5878 - accuracy: 0.9519\n",
      "Epoch 98: val_loss did not improve from 1.41868\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.5878 - accuracy: 0.9519 - val_loss: 2.1564 - val_accuracy: 0.6549\n",
      "Epoch 99/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5115 - accuracy: 0.9804\n",
      "Epoch 99: val_loss improved from 1.41868 to 1.38360, saving model to ResNet-for-CIFAR-10.h5\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.5115 - accuracy: 0.9804 - val_loss: 1.3836 - val_accuracy: 0.7842\n",
      "Epoch 100/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4598 - accuracy: 0.9987\n",
      "Epoch 100: val_loss improved from 1.38360 to 1.37481, saving model to ResNet-for-CIFAR-10.h5\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.4598 - accuracy: 0.9987 - val_loss: 1.3748 - val_accuracy: 0.7891\n",
      "Epoch 101/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4492 - accuracy: 0.9998\n",
      "Epoch 101: val_loss did not improve from 1.37481\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.4492 - accuracy: 0.9998 - val_loss: 1.3832 - val_accuracy: 0.7914\n",
      "Epoch 102/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4420 - accuracy: 0.9999\n",
      "Epoch 102: val_loss did not improve from 1.37481\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.4420 - accuracy: 0.9999 - val_loss: 1.3864 - val_accuracy: 0.7937\n",
      "Epoch 103/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4359 - accuracy: 1.0000\n",
      "Epoch 103: val_loss did not improve from 1.37481\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.4359 - accuracy: 1.0000 - val_loss: 1.3909 - val_accuracy: 0.7935\n",
      "Epoch 104/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4302 - accuracy: 1.0000\n",
      "Epoch 104: val_loss did not improve from 1.37481\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.4302 - accuracy: 1.0000 - val_loss: 1.3950 - val_accuracy: 0.7936\n",
      "Epoch 105/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4249 - accuracy: 1.0000\n",
      "Epoch 105: val_loss did not improve from 1.37481\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.4249 - accuracy: 1.0000 - val_loss: 1.3942 - val_accuracy: 0.7944\n",
      "Epoch 106/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4194 - accuracy: 1.0000\n",
      "Epoch 106: val_loss did not improve from 1.37481\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 25s 85ms/step - loss: 0.4194 - accuracy: 1.0000 - val_loss: 1.3983 - val_accuracy: 0.7943\n",
      "Epoch 107/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4142 - accuracy: 1.0000\n",
      "Epoch 107: val_loss did not improve from 1.37481\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.4142 - accuracy: 1.0000 - val_loss: 1.3985 - val_accuracy: 0.7948\n",
      "Epoch 108/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4093 - accuracy: 1.0000\n",
      "Epoch 108: val_loss did not improve from 1.37481\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.4093 - accuracy: 1.0000 - val_loss: 1.4013 - val_accuracy: 0.7951\n",
      "Epoch 109/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4044 - accuracy: 1.0000\n",
      "Epoch 109: val_loss did not improve from 1.37481\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.4044 - accuracy: 1.0000 - val_loss: 1.3983 - val_accuracy: 0.7947\n",
      "Epoch 110/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3995 - accuracy: 1.0000\n",
      "Epoch 110: val_loss did not improve from 1.37481\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.3995 - accuracy: 1.0000 - val_loss: 1.3997 - val_accuracy: 0.7958\n",
      "Epoch 111/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3947 - accuracy: 1.0000\n",
      "Epoch 111: val_loss did not improve from 1.37481\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.3947 - accuracy: 1.0000 - val_loss: 1.4000 - val_accuracy: 0.7959\n",
      "Epoch 112/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3901 - accuracy: 1.0000\n",
      "Epoch 112: val_loss did not improve from 1.37481\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.3901 - accuracy: 1.0000 - val_loss: 1.4009 - val_accuracy: 0.7954\n",
      "Epoch 113/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3854 - accuracy: 1.0000\n",
      "Epoch 113: val_loss did not improve from 1.37481\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.3854 - accuracy: 1.0000 - val_loss: 1.4013 - val_accuracy: 0.7945\n",
      "Epoch 114/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3810 - accuracy: 0.9999\n",
      "Epoch 114: val_loss did not improve from 1.37481\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.3810 - accuracy: 0.9999 - val_loss: 1.3995 - val_accuracy: 0.7949\n",
      "Epoch 115/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3763 - accuracy: 1.0000\n",
      "Epoch 115: val_loss did not improve from 1.37481\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.3763 - accuracy: 1.0000 - val_loss: 1.3985 - val_accuracy: 0.7950\n",
      "Epoch 116/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3719 - accuracy: 1.0000\n",
      "Epoch 116: val_loss did not improve from 1.37481\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 25s 85ms/step - loss: 0.3719 - accuracy: 1.0000 - val_loss: 1.3979 - val_accuracy: 0.7948\n",
      "Epoch 117/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3676 - accuracy: 1.0000\n",
      "Epoch 117: val_loss did not improve from 1.37481\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.3676 - accuracy: 1.0000 - val_loss: 1.3974 - val_accuracy: 0.7943\n",
      "Epoch 118/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3632 - accuracy: 1.0000\n",
      "Epoch 118: val_loss did not improve from 1.37481\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.3632 - accuracy: 1.0000 - val_loss: 1.3966 - val_accuracy: 0.7946\n",
      "Epoch 119/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3590 - accuracy: 1.0000\n",
      "Epoch 119: val_loss did not improve from 1.37481\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.3590 - accuracy: 1.0000 - val_loss: 1.3934 - val_accuracy: 0.7954\n",
      "Epoch 120/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3547 - accuracy: 1.0000\n",
      "Epoch 120: val_loss did not improve from 1.37481\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.3547 - accuracy: 1.0000 - val_loss: 1.3924 - val_accuracy: 0.7957\n",
      "Epoch 121/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3506 - accuracy: 1.0000\n",
      "Epoch 121: val_loss did not improve from 1.37481\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.3506 - accuracy: 1.0000 - val_loss: 1.3915 - val_accuracy: 0.7962\n",
      "Epoch 122/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3465 - accuracy: 1.0000\n",
      "Epoch 122: val_loss did not improve from 1.37481\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.3465 - accuracy: 1.0000 - val_loss: 1.3928 - val_accuracy: 0.7960\n",
      "Epoch 123/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3425 - accuracy: 1.0000\n",
      "Epoch 123: val_loss did not improve from 1.37481\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.3425 - accuracy: 1.0000 - val_loss: 1.3912 - val_accuracy: 0.7961\n",
      "Epoch 124/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3384 - accuracy: 1.0000\n",
      "Epoch 124: val_loss did not improve from 1.37481\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.3384 - accuracy: 1.0000 - val_loss: 1.3890 - val_accuracy: 0.7961\n",
      "Epoch 125/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3345 - accuracy: 1.0000\n",
      "Epoch 125: val_loss did not improve from 1.37481\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.3345 - accuracy: 1.0000 - val_loss: 1.3899 - val_accuracy: 0.7958\n",
      "Epoch 126/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3306 - accuracy: 1.0000\n",
      "Epoch 126: val_loss did not improve from 1.37481\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.3306 - accuracy: 1.0000 - val_loss: 1.3864 - val_accuracy: 0.7971\n",
      "Epoch 127/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3267 - accuracy: 1.0000\n",
      "Epoch 127: val_loss did not improve from 1.37481\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.3267 - accuracy: 1.0000 - val_loss: 1.3850 - val_accuracy: 0.7962\n",
      "Epoch 128/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3229 - accuracy: 1.0000\n",
      "Epoch 128: val_loss did not improve from 1.37481\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.3229 - accuracy: 1.0000 - val_loss: 1.3850 - val_accuracy: 0.7965\n",
      "Epoch 129/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3191 - accuracy: 1.0000\n",
      "Epoch 129: val_loss did not improve from 1.37481\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.3191 - accuracy: 1.0000 - val_loss: 1.3826 - val_accuracy: 0.7964\n",
      "Epoch 130/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3154 - accuracy: 1.0000\n",
      "Epoch 130: val_loss did not improve from 1.37481\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.3154 - accuracy: 1.0000 - val_loss: 1.3824 - val_accuracy: 0.7965\n",
      "Epoch 131/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3117 - accuracy: 1.0000\n",
      "Epoch 131: val_loss did not improve from 1.37481\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.3117 - accuracy: 1.0000 - val_loss: 1.3817 - val_accuracy: 0.7974\n",
      "Epoch 132/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3081 - accuracy: 1.0000\n",
      "Epoch 132: val_loss did not improve from 1.37481\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.3081 - accuracy: 1.0000 - val_loss: 1.3796 - val_accuracy: 0.7972\n",
      "Epoch 133/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3045 - accuracy: 1.0000\n",
      "Epoch 133: val_loss did not improve from 1.37481\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.3045 - accuracy: 1.0000 - val_loss: 1.3780 - val_accuracy: 0.7975\n",
      "Epoch 134/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3009 - accuracy: 1.0000\n",
      "Epoch 134: val_loss did not improve from 1.37481\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.3009 - accuracy: 1.0000 - val_loss: 1.3761 - val_accuracy: 0.7975\n",
      "Epoch 135/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2974 - accuracy: 1.0000\n",
      "Epoch 135: val_loss did not improve from 1.37481\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.2974 - accuracy: 1.0000 - val_loss: 1.3748 - val_accuracy: 0.7972\n",
      "Epoch 136/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2939 - accuracy: 1.0000\n",
      "Epoch 136: val_loss did not improve from 1.37481\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.2939 - accuracy: 1.0000 - val_loss: 1.3749 - val_accuracy: 0.7969\n",
      "Epoch 137/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2905 - accuracy: 1.0000\n",
      "Epoch 137: val_loss improved from 1.37481 to 1.37211, saving model to ResNet-for-CIFAR-10.h5\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.2905 - accuracy: 1.0000 - val_loss: 1.3721 - val_accuracy: 0.7965\n",
      "Epoch 138/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2872 - accuracy: 1.0000\n",
      "Epoch 138: val_loss improved from 1.37211 to 1.36987, saving model to ResNet-for-CIFAR-10.h5\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.2872 - accuracy: 1.0000 - val_loss: 1.3699 - val_accuracy: 0.7968\n",
      "Epoch 139/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2838 - accuracy: 1.0000\n",
      "Epoch 139: val_loss did not improve from 1.36987\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.2838 - accuracy: 1.0000 - val_loss: 1.3701 - val_accuracy: 0.7968\n",
      "Epoch 140/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2805 - accuracy: 1.0000\n",
      "Epoch 140: val_loss did not improve from 1.36987\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.2805 - accuracy: 1.0000 - val_loss: 1.3705 - val_accuracy: 0.7966\n",
      "Epoch 141/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2772 - accuracy: 1.0000\n",
      "Epoch 141: val_loss improved from 1.36987 to 1.36764, saving model to ResNet-for-CIFAR-10.h5\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.2772 - accuracy: 1.0000 - val_loss: 1.3676 - val_accuracy: 0.7967\n",
      "Epoch 142/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2740 - accuracy: 1.0000\n",
      "Epoch 142: val_loss did not improve from 1.36764\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.2740 - accuracy: 1.0000 - val_loss: 1.3677 - val_accuracy: 0.7963\n",
      "Epoch 143/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2708 - accuracy: 1.0000\n",
      "Epoch 143: val_loss improved from 1.36764 to 1.36551, saving model to ResNet-for-CIFAR-10.h5\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.2708 - accuracy: 1.0000 - val_loss: 1.3655 - val_accuracy: 0.7974\n",
      "Epoch 144/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2676 - accuracy: 1.0000\n",
      "Epoch 144: val_loss did not improve from 1.36551\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.2676 - accuracy: 1.0000 - val_loss: 1.3655 - val_accuracy: 0.7971\n",
      "Epoch 145/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2645 - accuracy: 1.0000\n",
      "Epoch 145: val_loss improved from 1.36551 to 1.36450, saving model to ResNet-for-CIFAR-10.h5\n",
      "lr:1.00e-02\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.2645 - accuracy: 1.0000 - val_loss: 1.3645 - val_accuracy: 0.7966\n",
      "Epoch 146/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2614 - accuracy: 1.0000\n",
      "Epoch 146: val_loss improved from 1.36450 to 1.36128, saving model to ResNet-for-CIFAR-10.h5\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.2614 - accuracy: 1.0000 - val_loss: 1.3613 - val_accuracy: 0.7967\n",
      "Epoch 147/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2597 - accuracy: 1.0000\n",
      "Epoch 147: val_loss did not improve from 1.36128\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 25s 85ms/step - loss: 0.2597 - accuracy: 1.0000 - val_loss: 1.3646 - val_accuracy: 0.7958\n",
      "Epoch 148/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2594 - accuracy: 1.0000\n",
      "Epoch 148: val_loss did not improve from 1.36128\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.2594 - accuracy: 1.0000 - val_loss: 1.3653 - val_accuracy: 0.7960\n",
      "Epoch 149/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2590 - accuracy: 1.0000\n",
      "Epoch 149: val_loss did not improve from 1.36128\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.2590 - accuracy: 1.0000 - val_loss: 1.3643 - val_accuracy: 0.7962\n",
      "Epoch 150/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2588 - accuracy: 1.0000\n",
      "Epoch 150: val_loss did not improve from 1.36128\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.2588 - accuracy: 1.0000 - val_loss: 1.3652 - val_accuracy: 0.7962\n",
      "Epoch 151/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2584 - accuracy: 1.0000\n",
      "Epoch 151: val_loss did not improve from 1.36128\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.2584 - accuracy: 1.0000 - val_loss: 1.3639 - val_accuracy: 0.7958\n",
      "Epoch 152/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2581 - accuracy: 1.0000\n",
      "Epoch 152: val_loss did not improve from 1.36128\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.2581 - accuracy: 1.0000 - val_loss: 1.3631 - val_accuracy: 0.7958\n",
      "Epoch 153/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2579 - accuracy: 1.0000\n",
      "Epoch 153: val_loss did not improve from 1.36128\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.2579 - accuracy: 1.0000 - val_loss: 1.3638 - val_accuracy: 0.7961\n",
      "Epoch 154/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2575 - accuracy: 1.0000\n",
      "Epoch 154: val_loss did not improve from 1.36128\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.2575 - accuracy: 1.0000 - val_loss: 1.3629 - val_accuracy: 0.7968\n",
      "Epoch 155/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2572 - accuracy: 1.0000\n",
      "Epoch 155: val_loss did not improve from 1.36128\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.2572 - accuracy: 1.0000 - val_loss: 1.3642 - val_accuracy: 0.7965\n",
      "Epoch 156/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2569 - accuracy: 1.0000\n",
      "Epoch 156: val_loss did not improve from 1.36128\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.2569 - accuracy: 1.0000 - val_loss: 1.3637 - val_accuracy: 0.7962\n",
      "Epoch 157/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2566 - accuracy: 1.0000\n",
      "Epoch 157: val_loss did not improve from 1.36128\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.2566 - accuracy: 1.0000 - val_loss: 1.3627 - val_accuracy: 0.7965\n",
      "Epoch 158/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2563 - accuracy: 1.0000\n",
      "Epoch 158: val_loss did not improve from 1.36128\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.2563 - accuracy: 1.0000 - val_loss: 1.3638 - val_accuracy: 0.7966\n",
      "Epoch 159/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2560 - accuracy: 1.0000\n",
      "Epoch 159: val_loss did not improve from 1.36128\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.2560 - accuracy: 1.0000 - val_loss: 1.3625 - val_accuracy: 0.7966\n",
      "Epoch 160/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2557 - accuracy: 1.0000\n",
      "Epoch 160: val_loss did not improve from 1.36128\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.2557 - accuracy: 1.0000 - val_loss: 1.3633 - val_accuracy: 0.7967\n",
      "Epoch 161/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2554 - accuracy: 1.0000\n",
      "Epoch 161: val_loss did not improve from 1.36128\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.2554 - accuracy: 1.0000 - val_loss: 1.3627 - val_accuracy: 0.7961\n",
      "Epoch 162/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2551 - accuracy: 1.0000\n",
      "Epoch 162: val_loss did not improve from 1.36128\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.2551 - accuracy: 1.0000 - val_loss: 1.3627 - val_accuracy: 0.7964\n",
      "Epoch 163/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2548 - accuracy: 1.0000\n",
      "Epoch 163: val_loss did not improve from 1.36128\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.2548 - accuracy: 1.0000 - val_loss: 1.3619 - val_accuracy: 0.7963\n",
      "Epoch 164/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2545 - accuracy: 1.0000\n",
      "Epoch 164: val_loss did not improve from 1.36128\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.2545 - accuracy: 1.0000 - val_loss: 1.3621 - val_accuracy: 0.7964\n",
      "Epoch 165/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2542 - accuracy: 1.0000\n",
      "Epoch 165: val_loss did not improve from 1.36128\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.2542 - accuracy: 1.0000 - val_loss: 1.3615 - val_accuracy: 0.7962\n",
      "Epoch 166/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2539 - accuracy: 1.0000\n",
      "Epoch 166: val_loss did not improve from 1.36128\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.2539 - accuracy: 1.0000 - val_loss: 1.3628 - val_accuracy: 0.7960\n",
      "Epoch 167/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2536 - accuracy: 1.0000\n",
      "Epoch 167: val_loss did not improve from 1.36128\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.2536 - accuracy: 1.0000 - val_loss: 1.3624 - val_accuracy: 0.7966\n",
      "Epoch 168/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2533 - accuracy: 1.0000\n",
      "Epoch 168: val_loss did not improve from 1.36128\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.2533 - accuracy: 1.0000 - val_loss: 1.3615 - val_accuracy: 0.7962\n",
      "Epoch 169/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2531 - accuracy: 1.0000\n",
      "Epoch 169: val_loss did not improve from 1.36128\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.2531 - accuracy: 1.0000 - val_loss: 1.3621 - val_accuracy: 0.7963\n",
      "Epoch 170/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2528 - accuracy: 1.0000\n",
      "Epoch 170: val_loss did not improve from 1.36128\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.2528 - accuracy: 1.0000 - val_loss: 1.3619 - val_accuracy: 0.7961\n",
      "Epoch 171/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2525 - accuracy: 1.0000\n",
      "Epoch 171: val_loss did not improve from 1.36128\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.2525 - accuracy: 1.0000 - val_loss: 1.3618 - val_accuracy: 0.7967\n",
      "Epoch 172/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2522 - accuracy: 1.0000\n",
      "Epoch 172: val_loss improved from 1.36128 to 1.36127, saving model to ResNet-for-CIFAR-10.h5\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.2522 - accuracy: 1.0000 - val_loss: 1.3613 - val_accuracy: 0.7968\n",
      "Epoch 173/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2519 - accuracy: 1.0000\n",
      "Epoch 173: val_loss improved from 1.36127 to 1.36023, saving model to ResNet-for-CIFAR-10.h5\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.2519 - accuracy: 1.0000 - val_loss: 1.3602 - val_accuracy: 0.7962\n",
      "Epoch 174/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2516 - accuracy: 1.0000\n",
      "Epoch 174: val_loss did not improve from 1.36023\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.2516 - accuracy: 1.0000 - val_loss: 1.3618 - val_accuracy: 0.7963\n",
      "Epoch 175/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2513 - accuracy: 1.0000\n",
      "Epoch 175: val_loss did not improve from 1.36023\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.2513 - accuracy: 1.0000 - val_loss: 1.3622 - val_accuracy: 0.7961\n",
      "Epoch 176/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2510 - accuracy: 1.0000\n",
      "Epoch 176: val_loss did not improve from 1.36023\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.2510 - accuracy: 1.0000 - val_loss: 1.3620 - val_accuracy: 0.7963\n",
      "Epoch 177/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2507 - accuracy: 1.0000\n",
      "Epoch 177: val_loss improved from 1.36023 to 1.35951, saving model to ResNet-for-CIFAR-10.h5\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.2507 - accuracy: 1.0000 - val_loss: 1.3595 - val_accuracy: 0.7962\n",
      "Epoch 178/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2504 - accuracy: 1.0000\n",
      "Epoch 178: val_loss did not improve from 1.35951\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.2504 - accuracy: 1.0000 - val_loss: 1.3603 - val_accuracy: 0.7965\n",
      "Epoch 179/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2501 - accuracy: 1.0000\n",
      "Epoch 179: val_loss did not improve from 1.35951\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.2501 - accuracy: 1.0000 - val_loss: 1.3606 - val_accuracy: 0.7967\n",
      "Epoch 180/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2498 - accuracy: 1.0000\n",
      "Epoch 180: val_loss did not improve from 1.35951\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.2498 - accuracy: 1.0000 - val_loss: 1.3612 - val_accuracy: 0.7969\n",
      "Epoch 181/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2495 - accuracy: 1.0000\n",
      "Epoch 181: val_loss did not improve from 1.35951\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.2495 - accuracy: 1.0000 - val_loss: 1.3603 - val_accuracy: 0.7970\n",
      "Epoch 182/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2492 - accuracy: 1.0000\n",
      "Epoch 182: val_loss did not improve from 1.35951\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.2492 - accuracy: 1.0000 - val_loss: 1.3607 - val_accuracy: 0.7970\n",
      "Epoch 183/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2490 - accuracy: 1.0000\n",
      "Epoch 183: val_loss did not improve from 1.35951\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.2490 - accuracy: 1.0000 - val_loss: 1.3599 - val_accuracy: 0.7964\n",
      "Epoch 184/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2486 - accuracy: 1.0000\n",
      "Epoch 184: val_loss did not improve from 1.35951\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.2486 - accuracy: 1.0000 - val_loss: 1.3599 - val_accuracy: 0.7965\n",
      "Epoch 185/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2484 - accuracy: 1.0000\n",
      "Epoch 185: val_loss did not improve from 1.35951\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.2484 - accuracy: 1.0000 - val_loss: 1.3610 - val_accuracy: 0.7966\n",
      "Epoch 186/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2481 - accuracy: 1.0000\n",
      "Epoch 186: val_loss improved from 1.35951 to 1.35819, saving model to ResNet-for-CIFAR-10.h5\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 25s 84ms/step - loss: 0.2481 - accuracy: 1.0000 - val_loss: 1.3582 - val_accuracy: 0.7960\n",
      "Epoch 187/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2478 - accuracy: 1.0000\n",
      "Epoch 187: val_loss did not improve from 1.35819\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.2478 - accuracy: 1.0000 - val_loss: 1.3598 - val_accuracy: 0.7967\n",
      "Epoch 188/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2475 - accuracy: 1.0000\n",
      "Epoch 188: val_loss did not improve from 1.35819\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 24s 82ms/step - loss: 0.2475 - accuracy: 1.0000 - val_loss: 1.3595 - val_accuracy: 0.7963\n",
      "Epoch 189/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2472 - accuracy: 1.0000\n",
      "Epoch 189: val_loss did not improve from 1.35819\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.2472 - accuracy: 1.0000 - val_loss: 1.3597 - val_accuracy: 0.7969\n",
      "Epoch 190/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2469 - accuracy: 1.0000\n",
      "Epoch 190: val_loss did not improve from 1.35819\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.2469 - accuracy: 1.0000 - val_loss: 1.3585 - val_accuracy: 0.7962\n",
      "Epoch 191/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2466 - accuracy: 1.0000\n",
      "Epoch 191: val_loss did not improve from 1.35819\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.2466 - accuracy: 1.0000 - val_loss: 1.3591 - val_accuracy: 0.7969\n",
      "Epoch 192/192\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2463 - accuracy: 1.0000\n",
      "Epoch 192: val_loss did not improve from 1.35819\n",
      "lr:1.00e-03\n",
      "293/293 [==============================] - 24s 83ms/step - loss: 0.2463 - accuracy: 1.0000 - val_loss: 1.3585 - val_accuracy: 0.7970\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TRAIN\n",
    "\n",
    "if os.path.isfile(cifar_teacher_path):\n",
    "    cifar_teacher_model.load_weights(cifar_teacher_path)\n",
    "else:\n",
    "    cifar_history = cifar_teacher_model.fit(cifar_train_x, cifar_train_y,\n",
    "                    batch_size=batch_size, \n",
    "                    epochs=num_epoch, \n",
    "                    validation_data=(cifar_val_x, cifar_val_y), \n",
    "                    verbose=1,                    \n",
    "                    callbacks=cifar_teacher_callbacks)\n",
    "\n",
    "    plt.plot(cifar_history.history[\"accuracy\"], label = 'Train Accuracy')\n",
    "    plt.plot(cifar_history.history[\"val_accuracy\"], linestyle = 'dashed', label = 'Test Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55db92e2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "55db92e2",
    "outputId": "3d0c85a9-a39c-487b-fba0-5418e21654a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 9ms/step - loss: 1.4134 - accuracy: 0.7887\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4133535623550415, 0.7886999845504761]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar_teacher_model.evaluate(cifar_test_x, cifar_test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db04b76",
   "metadata": {
    "id": "7db04b76"
   },
   "source": [
    "### Reduced Teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd84916",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2bd84916",
    "outputId": "a03f4378-43b9-4379-d112-6b96b577f2a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_1 (Flatten)         (None, 3072)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 789,258\n",
      "Trainable params: 789,258\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cifar_reduced_teacher = create_dense_nn((32, 32, 3), 10)\n",
    "cifar_reduced_teacher.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b08b4c",
   "metadata": {
    "id": "c0b08b4c"
   },
   "source": [
    "### CIFAR Teacher - Reduced Teacher Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f3787d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "80f3787d",
    "outputId": "794d6d07-c146-4ccf-cde7-a306426e8e96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1172/1172 [==============================] - 42s 31ms/step - sparse_categorical_accuracy: 0.9050 - student_loss: 1.5638 - distillation_loss: 0.0412 - val_sparse_categorical_accuracy: 0.7323 - val_student_loss: 1.6290\n",
      "Epoch 2/20\n",
      "1172/1172 [==============================] - 35s 30ms/step - sparse_categorical_accuracy: 0.9133 - student_loss: 1.5548 - distillation_loss: 0.0413 - val_sparse_categorical_accuracy: 0.7218 - val_student_loss: 1.7804\n",
      "Epoch 3/20\n",
      "1172/1172 [==============================] - 35s 30ms/step - sparse_categorical_accuracy: 0.9174 - student_loss: 1.5495 - distillation_loss: 0.0413 - val_sparse_categorical_accuracy: 0.7284 - val_student_loss: 1.6422\n",
      "Epoch 4/20\n",
      "1172/1172 [==============================] - 35s 30ms/step - sparse_categorical_accuracy: 0.9241 - student_loss: 1.5422 - distillation_loss: 0.0413 - val_sparse_categorical_accuracy: 0.7291 - val_student_loss: 1.7416\n",
      "Epoch 5/20\n",
      "1172/1172 [==============================] - 35s 30ms/step - sparse_categorical_accuracy: 0.9315 - student_loss: 1.5348 - distillation_loss: 0.0414 - val_sparse_categorical_accuracy: 0.7462 - val_student_loss: 1.7311\n",
      "Epoch 6/20\n",
      "1172/1172 [==============================] - 35s 30ms/step - sparse_categorical_accuracy: 0.9402 - student_loss: 1.5259 - distillation_loss: 0.0414 - val_sparse_categorical_accuracy: 0.7323 - val_student_loss: 1.7623\n",
      "Epoch 7/20\n",
      "1172/1172 [==============================] - 35s 30ms/step - sparse_categorical_accuracy: 0.9411 - student_loss: 1.5241 - distillation_loss: 0.0414 - val_sparse_categorical_accuracy: 0.7637 - val_student_loss: 1.6210\n",
      "Epoch 8/20\n",
      "1172/1172 [==============================] - 35s 30ms/step - sparse_categorical_accuracy: 0.9493 - student_loss: 1.5170 - distillation_loss: 0.0415 - val_sparse_categorical_accuracy: 0.7505 - val_student_loss: 1.7141\n",
      "Epoch 9/20\n",
      "1172/1172 [==============================] - 35s 30ms/step - sparse_categorical_accuracy: 0.9516 - student_loss: 1.5134 - distillation_loss: 0.0414 - val_sparse_categorical_accuracy: 0.7570 - val_student_loss: 1.6355\n",
      "Epoch 10/20\n",
      "1172/1172 [==============================] - 35s 30ms/step - sparse_categorical_accuracy: 0.9546 - student_loss: 1.5104 - distillation_loss: 0.0415 - val_sparse_categorical_accuracy: 0.7451 - val_student_loss: 1.6689\n",
      "Epoch 11/20\n",
      "1172/1172 [==============================] - 35s 30ms/step - sparse_categorical_accuracy: 0.9558 - student_loss: 1.5087 - distillation_loss: 0.0415 - val_sparse_categorical_accuracy: 0.7666 - val_student_loss: 1.6513\n",
      "Epoch 12/20\n",
      "1172/1172 [==============================] - 35s 30ms/step - sparse_categorical_accuracy: 0.9622 - student_loss: 1.5021 - distillation_loss: 0.0415 - val_sparse_categorical_accuracy: 0.7638 - val_student_loss: 1.8098\n",
      "Epoch 13/20\n",
      "1172/1172 [==============================] - 35s 30ms/step - sparse_categorical_accuracy: 0.9608 - student_loss: 1.5037 - distillation_loss: 0.0415 - val_sparse_categorical_accuracy: 0.7605 - val_student_loss: 1.6523\n",
      "Epoch 14/20\n",
      "1172/1172 [==============================] - 35s 30ms/step - sparse_categorical_accuracy: 0.9667 - student_loss: 1.4976 - distillation_loss: 0.0415 - val_sparse_categorical_accuracy: 0.7618 - val_student_loss: 1.6921\n",
      "Epoch 15/20\n",
      "1172/1172 [==============================] - 35s 30ms/step - sparse_categorical_accuracy: 0.9683 - student_loss: 1.4956 - distillation_loss: 0.0415 - val_sparse_categorical_accuracy: 0.7581 - val_student_loss: 1.6443\n",
      "Epoch 16/20\n",
      "1172/1172 [==============================] - 36s 30ms/step - sparse_categorical_accuracy: 0.9683 - student_loss: 1.4960 - distillation_loss: 0.0415 - val_sparse_categorical_accuracy: 0.7521 - val_student_loss: 1.7139\n",
      "Epoch 17/20\n",
      "1172/1172 [==============================] - 35s 30ms/step - sparse_categorical_accuracy: 0.9698 - student_loss: 1.4939 - distillation_loss: 0.0415 - val_sparse_categorical_accuracy: 0.7504 - val_student_loss: 1.6124\n",
      "Epoch 18/20\n",
      "1172/1172 [==============================] - 35s 30ms/step - sparse_categorical_accuracy: 0.9706 - student_loss: 1.4932 - distillation_loss: 0.0416 - val_sparse_categorical_accuracy: 0.7608 - val_student_loss: 1.6527\n",
      "Epoch 19/20\n",
      "1172/1172 [==============================] - 35s 30ms/step - sparse_categorical_accuracy: 0.9728 - student_loss: 1.4909 - distillation_loss: 0.0416 - val_sparse_categorical_accuracy: 0.7514 - val_student_loss: 1.7774\n",
      "Epoch 20/20\n",
      "1172/1172 [==============================] - 35s 30ms/step - sparse_categorical_accuracy: 0.9753 - student_loss: 1.4890 - distillation_loss: 0.0416 - val_sparse_categorical_accuracy: 0.7477 - val_student_loss: 1.6951\n",
      "313/313 [==============================] - 2s 8ms/step - sparse_categorical_accuracy: 0.7494 - student_loss: 1.7125\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cifar_distiller = Distiller(student=cifar_teacher_model, teacher=cifar_reduced_teacher)\n",
    "cifar_distiller.compile(\n",
    "    optimizer=Adam(),\n",
    "    metrics=[SparseCategoricalAccuracy()],\n",
    "    student_loss_fn=SparseCategoricalCrossentropy(from_logits=True),\n",
    "    distillation_loss_fn=KLDivergence(),\n",
    "    alpha=0.2,\n",
    "    temperature=3,\n",
    ")\n",
    "\n",
    "# Distill teacher to student\n",
    "cifar_reduced_teacher_history = cifar_distiller.fit(cifar_train_x, cifar_train_y, validation_data=(cifar_val_x, cifar_val_y), epochs=20)\n",
    "\n",
    "# Evaluate student on test dataset\n",
    "cifar_distiller.evaluate(cifar_test_x, cifar_test_y)\n",
    "\n",
    "plt.plot(cifar_reduced_teacher_history.history[\"sparse_categorical_accuracy\"], label = 'Train Accuracy')\n",
    "plt.plot(cifar_reduced_teacher_history.history[\"val_sparse_categorical_accuracy\"], linestyle = 'dashed', label = 'Test Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca9c895",
   "metadata": {
    "id": "0ca9c895"
   },
   "source": [
    "## EMINIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76add38e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "76add38e",
    "outputId": "1f2f8d1d-b85a-4cb1-c567-542461e22056"
   },
   "outputs": [],
   "source": [
    "(emnist_train_x, emnist_train_y) = em.extract_training_samples('letters')\n",
    "emnist_train_x, emnist_train_y = shuffle(emnist_train_x, emnist_train_y)\n",
    "(emnist_test_x, emnist_test_y) = em.extract_test_samples('letters')\n",
    "emnist_train_x = emnist_train_x.reshape((emnist_train_x.shape[0], 28, 28, 1))\n",
    "emnist_test_x = emnist_test_x.reshape((emnist_test_x.shape[0], 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58f18c97",
   "metadata": {
    "id": "58f18c97"
   },
   "outputs": [],
   "source": [
    "emnist_train_x = emnist_train_x.astype('float32')\n",
    "emnist_test_x = emnist_test_x.astype('float32')\n",
    "emnist_train_x = emnist_train_x / 255.0\n",
    "emnist_test_x = emnist_test_x / 255.0\n",
    "\n",
    "emnist_train_x, emnist_val_x, emnist_train_y, emnist_val_y = train_test_split(emnist_train_x, emnist_train_y, \n",
    "    test_size=0.25, random_state= 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f471e6dd",
   "metadata": {
    "id": "f471e6dd"
   },
   "source": [
    "### Teacher "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4dd5c9a",
   "metadata": {
    "id": "c4dd5c9a"
   },
   "outputs": [],
   "source": [
    "emnist_teacher_model = create_resnet((28, 28, 1), 27)\n",
    "emnist_teacher_model.type = \"emnist_resnet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8bdcd8d6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8bdcd8d6",
    "outputId": "91c1154e-0a30-4f2d-fc14-b5de64c7cc97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leraning rate is controled by epoch.\n"
     ]
    }
   ],
   "source": [
    "emnist_teacher_model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "emnist_teacher_path = \"ResNet-for-EMNIST.h5\"\n",
    "emnist_teacher_checkpoint = ModelCheckpoint(filepath = emnist_teacher_path, monitor=\"val_loss\", verbose=1, save_best_only=True)\n",
    "emnist_teacher_learning_controller = LearningController(num_epoch)\n",
    "emnist_teacher_callbacks = [emnist_teacher_checkpoint, emnist_teacher_learning_controller]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48a7733",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "a48a7733",
    "outputId": "c61e1dc7-23fb-4ece-c903-758084ad216f",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/192\n",
      "732/732 [==============================] - ETA: 0s - loss: 1.9652 - accuracy: 0.5815\n",
      "Epoch 1: val_loss improved from inf to 1.06250, saving model to ResNet-for-EMNIST.h5\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 58s 70ms/step - loss: 1.9652 - accuracy: 0.5815 - val_loss: 1.0625 - val_accuracy: 0.8113\n",
      "Epoch 2/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.8749 - accuracy: 0.8648\n",
      "Epoch 2: val_loss improved from 1.06250 to 0.82547, saving model to ResNet-for-EMNIST.h5\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 68ms/step - loss: 0.8749 - accuracy: 0.8647 - val_loss: 0.8255 - val_accuracy: 0.8765\n",
      "Epoch 3/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.7308 - accuracy: 0.9024\n",
      "Epoch 3: val_loss improved from 0.82547 to 0.72353, saving model to ResNet-for-EMNIST.h5\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.7308 - accuracy: 0.9024 - val_loss: 0.7235 - val_accuracy: 0.9037\n",
      "Epoch 4/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.6770 - accuracy: 0.9169\n",
      "Epoch 4: val_loss improved from 0.72353 to 0.67778, saving model to ResNet-for-EMNIST.h5\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.6770 - accuracy: 0.9169 - val_loss: 0.6778 - val_accuracy: 0.9154\n",
      "Epoch 5/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.6446 - accuracy: 0.9262\n",
      "Epoch 5: val_loss improved from 0.67778 to 0.65991, saving model to ResNet-for-EMNIST.h5\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.6445 - accuracy: 0.9262 - val_loss: 0.6599 - val_accuracy: 0.9239\n",
      "Epoch 6/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.6233 - accuracy: 0.9315\n",
      "Epoch 6: val_loss did not improve from 0.65991\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 50s 68ms/step - loss: 0.6232 - accuracy: 0.9315 - val_loss: 0.6800 - val_accuracy: 0.9105\n",
      "Epoch 7/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.6069 - accuracy: 0.9365\n",
      "Epoch 7: val_loss improved from 0.65991 to 0.62605, saving model to ResNet-for-EMNIST.h5\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.6069 - accuracy: 0.9364 - val_loss: 0.6261 - val_accuracy: 0.9331\n",
      "Epoch 8/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.5941 - accuracy: 0.9399\n",
      "Epoch 8: val_loss did not improve from 0.62605\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.5941 - accuracy: 0.9399 - val_loss: 0.6465 - val_accuracy: 0.9252\n",
      "Epoch 9/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.5843 - accuracy: 0.9422\n",
      "Epoch 9: val_loss improved from 0.62605 to 0.61688, saving model to ResNet-for-EMNIST.h5\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.5843 - accuracy: 0.9422 - val_loss: 0.6169 - val_accuracy: 0.9336\n",
      "Epoch 10/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.5740 - accuracy: 0.9451\n",
      "Epoch 10: val_loss did not improve from 0.61688\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.5740 - accuracy: 0.9451 - val_loss: 0.6340 - val_accuracy: 0.9277\n",
      "Epoch 11/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.5649 - accuracy: 0.9474\n",
      "Epoch 11: val_loss improved from 0.61688 to 0.61534, saving model to ResNet-for-EMNIST.h5\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.5650 - accuracy: 0.9473 - val_loss: 0.6153 - val_accuracy: 0.9341\n",
      "Epoch 12/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.5585 - accuracy: 0.9491\n",
      "Epoch 12: val_loss improved from 0.61534 to 0.61467, saving model to ResNet-for-EMNIST.h5\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.5585 - accuracy: 0.9491 - val_loss: 0.6147 - val_accuracy: 0.9304\n",
      "Epoch 13/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.5500 - accuracy: 0.9517\n",
      "Epoch 13: val_loss improved from 0.61467 to 0.60037, saving model to ResNet-for-EMNIST.h5\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.5503 - accuracy: 0.9516 - val_loss: 0.6004 - val_accuracy: 0.9375\n",
      "Epoch 14/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.5443 - accuracy: 0.9536\n",
      "Epoch 14: val_loss did not improve from 0.60037\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.5442 - accuracy: 0.9536 - val_loss: 0.6171 - val_accuracy: 0.9330\n",
      "Epoch 15/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.5365 - accuracy: 0.9546\n",
      "Epoch 15: val_loss did not improve from 0.60037\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.5365 - accuracy: 0.9546 - val_loss: 0.6127 - val_accuracy: 0.9337\n",
      "Epoch 16/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.5310 - accuracy: 0.9569\n",
      "Epoch 16: val_loss did not improve from 0.60037\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.5310 - accuracy: 0.9569 - val_loss: 0.6078 - val_accuracy: 0.9347\n",
      "Epoch 17/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.5237 - accuracy: 0.9585\n",
      "Epoch 17: val_loss did not improve from 0.60037\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.5236 - accuracy: 0.9585 - val_loss: 0.6024 - val_accuracy: 0.9363\n",
      "Epoch 18/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.5183 - accuracy: 0.9606\n",
      "Epoch 18: val_loss did not improve from 0.60037\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.5183 - accuracy: 0.9606 - val_loss: 0.6085 - val_accuracy: 0.9341\n",
      "Epoch 19/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.5115 - accuracy: 0.9621\n",
      "Epoch 19: val_loss improved from 0.60037 to 0.59121, saving model to ResNet-for-EMNIST.h5\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.5115 - accuracy: 0.9621 - val_loss: 0.5912 - val_accuracy: 0.9379\n",
      "Epoch 20/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.5061 - accuracy: 0.9634\n",
      "Epoch 20: val_loss did not improve from 0.59121\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.5061 - accuracy: 0.9634 - val_loss: 0.5912 - val_accuracy: 0.9376\n",
      "Epoch 21/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.5019 - accuracy: 0.9639\n",
      "Epoch 21: val_loss did not improve from 0.59121\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.5019 - accuracy: 0.9640 - val_loss: 0.6182 - val_accuracy: 0.9319\n",
      "Epoch 22/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4946 - accuracy: 0.9669\n",
      "Epoch 22: val_loss improved from 0.59121 to 0.58731, saving model to ResNet-for-EMNIST.h5\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.4946 - accuracy: 0.9669 - val_loss: 0.5873 - val_accuracy: 0.9394\n",
      "Epoch 23/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4894 - accuracy: 0.9684\n",
      "Epoch 23: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.4894 - accuracy: 0.9684 - val_loss: 0.6089 - val_accuracy: 0.9323\n",
      "Epoch 24/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4847 - accuracy: 0.9693\n",
      "Epoch 24: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.4847 - accuracy: 0.9693 - val_loss: 0.6004 - val_accuracy: 0.9363\n",
      "Epoch 25/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4794 - accuracy: 0.9709\n",
      "Epoch 25: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.4794 - accuracy: 0.9709 - val_loss: 0.6047 - val_accuracy: 0.9334\n",
      "Epoch 26/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4746 - accuracy: 0.9725\n",
      "Epoch 26: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.4746 - accuracy: 0.9725 - val_loss: 0.6061 - val_accuracy: 0.9334\n",
      "Epoch 27/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4696 - accuracy: 0.9735\n",
      "Epoch 27: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.4697 - accuracy: 0.9735 - val_loss: 0.6093 - val_accuracy: 0.9330\n",
      "Epoch 28/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4650 - accuracy: 0.9750\n",
      "Epoch 28: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.4650 - accuracy: 0.9750 - val_loss: 0.6253 - val_accuracy: 0.9299\n",
      "Epoch 29/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4601 - accuracy: 0.9764\n",
      "Epoch 29: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.4601 - accuracy: 0.9763 - val_loss: 0.6141 - val_accuracy: 0.9352\n",
      "Epoch 30/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4555 - accuracy: 0.9776\n",
      "Epoch 30: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.4556 - accuracy: 0.9776 - val_loss: 0.6001 - val_accuracy: 0.9369\n",
      "Epoch 31/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4522 - accuracy: 0.9783\n",
      "Epoch 31: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.4522 - accuracy: 0.9783 - val_loss: 0.6339 - val_accuracy: 0.9282\n",
      "Epoch 32/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4476 - accuracy: 0.9794\n",
      "Epoch 32: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.4476 - accuracy: 0.9794 - val_loss: 0.6230 - val_accuracy: 0.9325\n",
      "Epoch 33/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4440 - accuracy: 0.9799\n",
      "Epoch 33: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.4439 - accuracy: 0.9799 - val_loss: 0.6153 - val_accuracy: 0.9355\n",
      "Epoch 34/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4381 - accuracy: 0.9822\n",
      "Epoch 34: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.4381 - accuracy: 0.9822 - val_loss: 0.6080 - val_accuracy: 0.9368\n",
      "Epoch 35/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4355 - accuracy: 0.9826\n",
      "Epoch 35: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.4355 - accuracy: 0.9826 - val_loss: 0.6316 - val_accuracy: 0.9298\n",
      "Epoch 36/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4318 - accuracy: 0.9838\n",
      "Epoch 36: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.4318 - accuracy: 0.9838 - val_loss: 0.6435 - val_accuracy: 0.9282\n",
      "Epoch 37/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4272 - accuracy: 0.9852\n",
      "Epoch 37: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.4273 - accuracy: 0.9852 - val_loss: 0.6295 - val_accuracy: 0.9316\n",
      "Epoch 38/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4256 - accuracy: 0.9852\n",
      "Epoch 38: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.4256 - accuracy: 0.9852 - val_loss: 0.6524 - val_accuracy: 0.9261\n",
      "Epoch 39/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4217 - accuracy: 0.9866\n",
      "Epoch 39: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.4217 - accuracy: 0.9866 - val_loss: 0.6378 - val_accuracy: 0.9309\n",
      "Epoch 40/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4200 - accuracy: 0.9859\n",
      "Epoch 40: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.4200 - accuracy: 0.9859 - val_loss: 0.6537 - val_accuracy: 0.9308\n",
      "Epoch 41/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4154 - accuracy: 0.9880\n",
      "Epoch 41: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.4155 - accuracy: 0.9880 - val_loss: 0.6536 - val_accuracy: 0.9278\n",
      "Epoch 42/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4145 - accuracy: 0.9877\n",
      "Epoch 42: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.4145 - accuracy: 0.9876 - val_loss: 0.6290 - val_accuracy: 0.9339\n",
      "Epoch 43/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4110 - accuracy: 0.9885\n",
      "Epoch 43: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.4110 - accuracy: 0.9885 - val_loss: 0.6385 - val_accuracy: 0.9345\n",
      "Epoch 44/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4105 - accuracy: 0.9884\n",
      "Epoch 44: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.4106 - accuracy: 0.9884 - val_loss: 0.6352 - val_accuracy: 0.9337\n",
      "Epoch 45/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4057 - accuracy: 0.9900\n",
      "Epoch 45: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.4057 - accuracy: 0.9900 - val_loss: 0.6501 - val_accuracy: 0.9282\n",
      "Epoch 46/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4056 - accuracy: 0.9893\n",
      "Epoch 46: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.4056 - accuracy: 0.9893 - val_loss: 0.6534 - val_accuracy: 0.9301\n",
      "Epoch 47/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3999 - accuracy: 0.9915\n",
      "Epoch 47: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.3999 - accuracy: 0.9915 - val_loss: 0.7214 - val_accuracy: 0.9207\n",
      "Epoch 48/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4008 - accuracy: 0.9903\n",
      "Epoch 48: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.4008 - accuracy: 0.9903 - val_loss: 0.6532 - val_accuracy: 0.9283\n",
      "Epoch 49/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3989 - accuracy: 0.9910\n",
      "Epoch 49: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.3989 - accuracy: 0.9910 - val_loss: 0.7239 - val_accuracy: 0.9122\n",
      "Epoch 50/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3955 - accuracy: 0.9915\n",
      "Epoch 50: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.3955 - accuracy: 0.9915 - val_loss: 0.6823 - val_accuracy: 0.9287\n",
      "Epoch 51/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3950 - accuracy: 0.9916\n",
      "Epoch 51: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.3950 - accuracy: 0.9916 - val_loss: 0.6839 - val_accuracy: 0.9260\n",
      "Epoch 52/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3925 - accuracy: 0.9918\n",
      "Epoch 52: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.3925 - accuracy: 0.9918 - val_loss: 0.6431 - val_accuracy: 0.9325\n",
      "Epoch 53/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3877 - accuracy: 0.9935\n",
      "Epoch 53: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.3877 - accuracy: 0.9935 - val_loss: 0.6761 - val_accuracy: 0.9261\n",
      "Epoch 54/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3868 - accuracy: 0.9936\n",
      "Epoch 54: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.3867 - accuracy: 0.9936 - val_loss: 0.6669 - val_accuracy: 0.9324\n",
      "Epoch 55/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3864 - accuracy: 0.9931\n",
      "Epoch 55: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.3864 - accuracy: 0.9931 - val_loss: 0.6882 - val_accuracy: 0.9267\n",
      "Epoch 56/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3858 - accuracy: 0.9929\n",
      "Epoch 56: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.3858 - accuracy: 0.9929 - val_loss: 0.7149 - val_accuracy: 0.9227\n",
      "Epoch 57/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3869 - accuracy: 0.9923\n",
      "Epoch 57: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.3869 - accuracy: 0.9923 - val_loss: 0.7824 - val_accuracy: 0.9027\n",
      "Epoch 58/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3793 - accuracy: 0.9947\n",
      "Epoch 58: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.3793 - accuracy: 0.9947 - val_loss: 0.6720 - val_accuracy: 0.9303\n",
      "Epoch 59/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3787 - accuracy: 0.9945\n",
      "Epoch 59: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.3787 - accuracy: 0.9945 - val_loss: 0.6927 - val_accuracy: 0.9250\n",
      "Epoch 60/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3779 - accuracy: 0.9943\n",
      "Epoch 60: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.3779 - accuracy: 0.9943 - val_loss: 0.7726 - val_accuracy: 0.9187\n",
      "Epoch 61/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3775 - accuracy: 0.9944\n",
      "Epoch 61: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.3775 - accuracy: 0.9944 - val_loss: 0.6907 - val_accuracy: 0.9276\n",
      "Epoch 62/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3716 - accuracy: 0.9962\n",
      "Epoch 62: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.3716 - accuracy: 0.9962 - val_loss: 0.6979 - val_accuracy: 0.9233\n",
      "Epoch 63/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3751 - accuracy: 0.9945\n",
      "Epoch 63: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.3751 - accuracy: 0.9945 - val_loss: 0.6661 - val_accuracy: 0.9298\n",
      "Epoch 64/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3728 - accuracy: 0.9950\n",
      "Epoch 64: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.3728 - accuracy: 0.9950 - val_loss: 0.7193 - val_accuracy: 0.9267\n",
      "Epoch 65/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3693 - accuracy: 0.9956\n",
      "Epoch 65: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.3693 - accuracy: 0.9956 - val_loss: 0.6599 - val_accuracy: 0.9332\n",
      "Epoch 66/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3639 - accuracy: 0.9976\n",
      "Epoch 66: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.3639 - accuracy: 0.9976 - val_loss: 0.7323 - val_accuracy: 0.9228\n",
      "Epoch 67/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3678 - accuracy: 0.9955\n",
      "Epoch 67: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.3678 - accuracy: 0.9955 - val_loss: 0.7286 - val_accuracy: 0.9216\n",
      "Epoch 68/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3690 - accuracy: 0.9943\n",
      "Epoch 68: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.3690 - accuracy: 0.9943 - val_loss: 0.6837 - val_accuracy: 0.9252\n",
      "Epoch 69/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3624 - accuracy: 0.9969\n",
      "Epoch 69: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.3625 - accuracy: 0.9968 - val_loss: 0.7410 - val_accuracy: 0.9190\n",
      "Epoch 70/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3716 - accuracy: 0.9928\n",
      "Epoch 70: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.3716 - accuracy: 0.9928 - val_loss: 0.7032 - val_accuracy: 0.9269\n",
      "Epoch 71/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3598 - accuracy: 0.9972\n",
      "Epoch 71: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.3598 - accuracy: 0.9972 - val_loss: 0.7081 - val_accuracy: 0.9274\n",
      "Epoch 72/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3632 - accuracy: 0.9952\n",
      "Epoch 72: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.3632 - accuracy: 0.9952 - val_loss: 0.6832 - val_accuracy: 0.9305\n",
      "Epoch 73/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3551 - accuracy: 0.9981\n",
      "Epoch 73: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.3551 - accuracy: 0.9981 - val_loss: 0.7125 - val_accuracy: 0.9274\n",
      "Epoch 74/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3622 - accuracy: 0.9948\n",
      "Epoch 74: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.3622 - accuracy: 0.9948 - val_loss: 0.6802 - val_accuracy: 0.9322\n",
      "Epoch 75/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3545 - accuracy: 0.9976\n",
      "Epoch 75: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.3545 - accuracy: 0.9976 - val_loss: 0.7018 - val_accuracy: 0.9272\n",
      "Epoch 76/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3567 - accuracy: 0.9964\n",
      "Epoch 76: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.3567 - accuracy: 0.9964 - val_loss: 0.6922 - val_accuracy: 0.9313\n",
      "Epoch 77/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3514 - accuracy: 0.9979\n",
      "Epoch 77: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.3514 - accuracy: 0.9979 - val_loss: 0.6951 - val_accuracy: 0.9299\n",
      "Epoch 78/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3537 - accuracy: 0.9968\n",
      "Epoch 78: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.3537 - accuracy: 0.9968 - val_loss: 0.6958 - val_accuracy: 0.9304\n",
      "Epoch 79/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3532 - accuracy: 0.9966\n",
      "Epoch 79: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.3532 - accuracy: 0.9966 - val_loss: 0.6996 - val_accuracy: 0.9321\n",
      "Epoch 80/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3526 - accuracy: 0.9964\n",
      "Epoch 80: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.3527 - accuracy: 0.9964 - val_loss: 0.6780 - val_accuracy: 0.9333\n",
      "Epoch 81/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3534 - accuracy: 0.9961\n",
      "Epoch 81: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.3534 - accuracy: 0.9961 - val_loss: 0.7169 - val_accuracy: 0.9292\n",
      "Epoch 82/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3489 - accuracy: 0.9972\n",
      "Epoch 82: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.3489 - accuracy: 0.9972 - val_loss: 0.6874 - val_accuracy: 0.9333\n",
      "Epoch 83/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3434 - accuracy: 0.9987\n",
      "Epoch 83: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.3434 - accuracy: 0.9987 - val_loss: 0.7051 - val_accuracy: 0.9291\n",
      "Epoch 84/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3464 - accuracy: 0.9972\n",
      "Epoch 84: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.3464 - accuracy: 0.9972 - val_loss: 0.6938 - val_accuracy: 0.9309\n",
      "Epoch 85/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3455 - accuracy: 0.9974\n",
      "Epoch 85: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.3455 - accuracy: 0.9974 - val_loss: 0.6968 - val_accuracy: 0.9315\n",
      "Epoch 86/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3391 - accuracy: 0.9993\n",
      "Epoch 86: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.3391 - accuracy: 0.9993 - val_loss: 0.6810 - val_accuracy: 0.9348\n",
      "Epoch 87/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3408 - accuracy: 0.9983\n",
      "Epoch 87: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.3408 - accuracy: 0.9983 - val_loss: 0.6825 - val_accuracy: 0.9358\n",
      "Epoch 88/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3354 - accuracy: 0.9998\n",
      "Epoch 88: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.3354 - accuracy: 0.9998 - val_loss: 0.6732 - val_accuracy: 0.9378\n",
      "Epoch 89/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3338 - accuracy: 0.9999\n",
      "Epoch 89: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.3339 - accuracy: 0.9999 - val_loss: 0.6827 - val_accuracy: 0.9366\n",
      "Epoch 90/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3325 - accuracy: 1.0000\n",
      "Epoch 90: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.3325 - accuracy: 1.0000 - val_loss: 0.6794 - val_accuracy: 0.9389\n",
      "Epoch 91/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3317 - accuracy: 0.9998\n",
      "Epoch 91: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.3318 - accuracy: 0.9998 - val_loss: 0.6844 - val_accuracy: 0.9379\n",
      "Epoch 92/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3312 - accuracy: 0.9998\n",
      "Epoch 92: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.3312 - accuracy: 0.9998 - val_loss: 0.6798 - val_accuracy: 0.9380\n",
      "Epoch 93/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3292 - accuracy: 1.0000\n",
      "Epoch 93: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.3292 - accuracy: 1.0000 - val_loss: 0.6777 - val_accuracy: 0.9391\n",
      "Epoch 94/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3281 - accuracy: 1.0000\n",
      "Epoch 94: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.3281 - accuracy: 1.0000 - val_loss: 0.6800 - val_accuracy: 0.9391\n",
      "Epoch 95/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3270 - accuracy: 1.0000\n",
      "Epoch 95: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.3270 - accuracy: 1.0000 - val_loss: 0.6793 - val_accuracy: 0.9404\n",
      "Epoch 96/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3261 - accuracy: 1.0000\n",
      "Epoch 96: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.3261 - accuracy: 1.0000 - val_loss: 0.6812 - val_accuracy: 0.9399\n",
      "Epoch 97/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3251 - accuracy: 1.0000\n",
      "Epoch 97: val_loss did not improve from 0.58731\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.3251 - accuracy: 1.0000 - val_loss: 0.6789 - val_accuracy: 0.9402\n",
      "Epoch 98/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3241 - accuracy: 1.0000\n",
      "Epoch 98: val_loss did not improve from 0.58731\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.3241 - accuracy: 1.0000 - val_loss: 0.6781 - val_accuracy: 0.9383\n",
      "Epoch 99/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.5864 - accuracy: 0.9150\n",
      "Epoch 99: val_loss did not improve from 0.58731\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.5864 - accuracy: 0.9151 - val_loss: 0.6664 - val_accuracy: 0.8945\n",
      "Epoch 100/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4844 - accuracy: 0.9406\n",
      "Epoch 100: val_loss improved from 0.58731 to 0.55527, saving model to ResNet-for-EMNIST.h5\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.4844 - accuracy: 0.9406 - val_loss: 0.5553 - val_accuracy: 0.9222\n",
      "Epoch 101/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4558 - accuracy: 0.9468\n",
      "Epoch 101: val_loss improved from 0.55527 to 0.51719, saving model to ResNet-for-EMNIST.h5\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.4558 - accuracy: 0.9468 - val_loss: 0.5172 - val_accuracy: 0.9302\n",
      "Epoch 102/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4337 - accuracy: 0.9513\n",
      "Epoch 102: val_loss did not improve from 0.51719\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.4337 - accuracy: 0.9513 - val_loss: 0.5251 - val_accuracy: 0.9273\n",
      "Epoch 103/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.4159 - accuracy: 0.9539\n",
      "Epoch 103: val_loss improved from 0.51719 to 0.51705, saving model to ResNet-for-EMNIST.h5\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.4159 - accuracy: 0.9539 - val_loss: 0.5170 - val_accuracy: 0.9285\n",
      "Epoch 104/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3985 - accuracy: 0.9573\n",
      "Epoch 104: val_loss improved from 0.51705 to 0.45360, saving model to ResNet-for-EMNIST.h5\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.3985 - accuracy: 0.9572 - val_loss: 0.4536 - val_accuracy: 0.9426\n",
      "Epoch 105/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3863 - accuracy: 0.9593\n",
      "Epoch 105: val_loss did not improve from 0.45360\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.3863 - accuracy: 0.9593 - val_loss: 0.4698 - val_accuracy: 0.9378\n",
      "Epoch 106/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3722 - accuracy: 0.9612\n",
      "Epoch 106: val_loss did not improve from 0.45360\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.3722 - accuracy: 0.9612 - val_loss: 0.4569 - val_accuracy: 0.9410\n",
      "Epoch 107/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3597 - accuracy: 0.9624\n",
      "Epoch 107: val_loss did not improve from 0.45360\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.3597 - accuracy: 0.9624 - val_loss: 0.4939 - val_accuracy: 0.9274\n",
      "Epoch 108/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3500 - accuracy: 0.9643\n",
      "Epoch 108: val_loss did not improve from 0.45360\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.3500 - accuracy: 0.9643 - val_loss: 0.4706 - val_accuracy: 0.9327\n",
      "Epoch 109/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3356 - accuracy: 0.9669\n",
      "Epoch 109: val_loss did not improve from 0.45360\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.3357 - accuracy: 0.9669 - val_loss: 0.4770 - val_accuracy: 0.9345\n",
      "Epoch 110/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3288 - accuracy: 0.9669\n",
      "Epoch 110: val_loss improved from 0.45360 to 0.43843, saving model to ResNet-for-EMNIST.h5\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.3288 - accuracy: 0.9669 - val_loss: 0.4384 - val_accuracy: 0.9383\n",
      "Epoch 111/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3194 - accuracy: 0.9683\n",
      "Epoch 111: val_loss improved from 0.43843 to 0.43702, saving model to ResNet-for-EMNIST.h5\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.3195 - accuracy: 0.9683 - val_loss: 0.4370 - val_accuracy: 0.9405\n",
      "Epoch 112/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3101 - accuracy: 0.9696\n",
      "Epoch 112: val_loss did not improve from 0.43702\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.3101 - accuracy: 0.9696 - val_loss: 0.4597 - val_accuracy: 0.9313\n",
      "Epoch 113/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.3026 - accuracy: 0.9704\n",
      "Epoch 113: val_loss did not improve from 0.43702\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.3026 - accuracy: 0.9704 - val_loss: 0.4920 - val_accuracy: 0.9275\n",
      "Epoch 114/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.2951 - accuracy: 0.9710\n",
      "Epoch 114: val_loss did not improve from 0.43702\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.2951 - accuracy: 0.9710 - val_loss: 0.5185 - val_accuracy: 0.9176\n",
      "Epoch 115/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.2899 - accuracy: 0.9712\n",
      "Epoch 115: val_loss did not improve from 0.43702\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.2899 - accuracy: 0.9712 - val_loss: 0.4996 - val_accuracy: 0.9240\n",
      "Epoch 116/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.2830 - accuracy: 0.9720\n",
      "Epoch 116: val_loss did not improve from 0.43702\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.2830 - accuracy: 0.9720 - val_loss: 0.4993 - val_accuracy: 0.9180\n",
      "Epoch 117/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.2718 - accuracy: 0.9742\n",
      "Epoch 117: val_loss did not improve from 0.43702\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.2718 - accuracy: 0.9742 - val_loss: 0.5043 - val_accuracy: 0.9169\n",
      "Epoch 118/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.2707 - accuracy: 0.9730\n",
      "Epoch 118: val_loss did not improve from 0.43702\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.2707 - accuracy: 0.9730 - val_loss: 0.4603 - val_accuracy: 0.9372\n",
      "Epoch 119/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.2682 - accuracy: 0.9731\n",
      "Epoch 119: val_loss did not improve from 0.43702\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.2682 - accuracy: 0.9731 - val_loss: 0.4590 - val_accuracy: 0.9275\n",
      "Epoch 120/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.2567 - accuracy: 0.9757\n",
      "Epoch 120: val_loss did not improve from 0.43702\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.2567 - accuracy: 0.9757 - val_loss: 0.6110 - val_accuracy: 0.8900\n",
      "Epoch 121/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.2521 - accuracy: 0.9763\n",
      "Epoch 121: val_loss did not improve from 0.43702\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.2521 - accuracy: 0.9763 - val_loss: 0.4699 - val_accuracy: 0.9263\n",
      "Epoch 122/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.2473 - accuracy: 0.9769\n",
      "Epoch 122: val_loss did not improve from 0.43702\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.2473 - accuracy: 0.9769 - val_loss: 0.4414 - val_accuracy: 0.9343\n",
      "Epoch 123/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.2449 - accuracy: 0.9759\n",
      "Epoch 123: val_loss did not improve from 0.43702\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.2449 - accuracy: 0.9759 - val_loss: 0.5434 - val_accuracy: 0.9066\n",
      "Epoch 124/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.2335 - accuracy: 0.9794\n",
      "Epoch 124: val_loss improved from 0.43702 to 0.42931, saving model to ResNet-for-EMNIST.h5\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.2335 - accuracy: 0.9794 - val_loss: 0.4293 - val_accuracy: 0.9374\n",
      "Epoch 125/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.2365 - accuracy: 0.9769\n",
      "Epoch 125: val_loss did not improve from 0.42931\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 49s 66ms/step - loss: 0.2365 - accuracy: 0.9769 - val_loss: 0.4474 - val_accuracy: 0.9309\n",
      "Epoch 126/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.2284 - accuracy: 0.9788\n",
      "Epoch 126: val_loss improved from 0.42931 to 0.40018, saving model to ResNet-for-EMNIST.h5\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.2284 - accuracy: 0.9788 - val_loss: 0.4002 - val_accuracy: 0.9345\n",
      "Epoch 127/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.2229 - accuracy: 0.9798\n",
      "Epoch 127: val_loss did not improve from 0.40018\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.2229 - accuracy: 0.9798 - val_loss: 0.4829 - val_accuracy: 0.9204\n",
      "Epoch 128/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.2190 - accuracy: 0.9797\n",
      "Epoch 128: val_loss did not improve from 0.40018\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.2190 - accuracy: 0.9797 - val_loss: 0.4311 - val_accuracy: 0.9312\n",
      "Epoch 129/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.2142 - accuracy: 0.9809\n",
      "Epoch 129: val_loss did not improve from 0.40018\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.2142 - accuracy: 0.9809 - val_loss: 0.4404 - val_accuracy: 0.9341\n",
      "Epoch 130/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.2118 - accuracy: 0.9808\n",
      "Epoch 130: val_loss did not improve from 0.40018\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.2118 - accuracy: 0.9808 - val_loss: 0.5031 - val_accuracy: 0.9199\n",
      "Epoch 131/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.2039 - accuracy: 0.9823\n",
      "Epoch 131: val_loss did not improve from 0.40018\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.2039 - accuracy: 0.9823 - val_loss: 0.5008 - val_accuracy: 0.9224\n",
      "Epoch 132/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.2023 - accuracy: 0.9823\n",
      "Epoch 132: val_loss did not improve from 0.40018\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.2023 - accuracy: 0.9823 - val_loss: 0.4326 - val_accuracy: 0.9311\n",
      "Epoch 133/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.2020 - accuracy: 0.9819\n",
      "Epoch 133: val_loss did not improve from 0.40018\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.2021 - accuracy: 0.9818 - val_loss: 0.5124 - val_accuracy: 0.9188\n",
      "Epoch 134/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.2025 - accuracy: 0.9805\n",
      "Epoch 134: val_loss did not improve from 0.40018\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.2025 - accuracy: 0.9805 - val_loss: 0.4366 - val_accuracy: 0.9267\n",
      "Epoch 135/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1919 - accuracy: 0.9839\n",
      "Epoch 135: val_loss did not improve from 0.40018\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1919 - accuracy: 0.9839 - val_loss: 0.4173 - val_accuracy: 0.9324\n",
      "Epoch 136/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1881 - accuracy: 0.9843\n",
      "Epoch 136: val_loss did not improve from 0.40018\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1881 - accuracy: 0.9843 - val_loss: 0.5109 - val_accuracy: 0.9086\n",
      "Epoch 137/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1807 - accuracy: 0.9865\n",
      "Epoch 137: val_loss did not improve from 0.40018\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1807 - accuracy: 0.9865 - val_loss: 0.4279 - val_accuracy: 0.9353\n",
      "Epoch 138/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1855 - accuracy: 0.9837\n",
      "Epoch 138: val_loss did not improve from 0.40018\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1855 - accuracy: 0.9837 - val_loss: 0.4308 - val_accuracy: 0.9301\n",
      "Epoch 139/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1838 - accuracy: 0.9838\n",
      "Epoch 139: val_loss did not improve from 0.40018\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1838 - accuracy: 0.9838 - val_loss: 0.5718 - val_accuracy: 0.9052\n",
      "Epoch 140/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1795 - accuracy: 0.9847\n",
      "Epoch 140: val_loss did not improve from 0.40018\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1795 - accuracy: 0.9847 - val_loss: 0.4374 - val_accuracy: 0.9315\n",
      "Epoch 141/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1732 - accuracy: 0.9866\n",
      "Epoch 141: val_loss did not improve from 0.40018\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1732 - accuracy: 0.9866 - val_loss: 0.4440 - val_accuracy: 0.9295\n",
      "Epoch 142/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1710 - accuracy: 0.9868\n",
      "Epoch 142: val_loss did not improve from 0.40018\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1711 - accuracy: 0.9868 - val_loss: 0.4114 - val_accuracy: 0.9316\n",
      "Epoch 143/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1735 - accuracy: 0.9856\n",
      "Epoch 143: val_loss did not improve from 0.40018\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1735 - accuracy: 0.9856 - val_loss: 0.4399 - val_accuracy: 0.9241\n",
      "Epoch 144/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1688 - accuracy: 0.9864\n",
      "Epoch 144: val_loss did not improve from 0.40018\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1688 - accuracy: 0.9864 - val_loss: 0.4554 - val_accuracy: 0.9281\n",
      "Epoch 145/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1646 - accuracy: 0.9877\n",
      "Epoch 145: val_loss did not improve from 0.40018\n",
      "lr:1.00e-02\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1646 - accuracy: 0.9877 - val_loss: 0.4401 - val_accuracy: 0.9249\n",
      "Epoch 146/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1566 - accuracy: 0.9902\n",
      "Epoch 146: val_loss did not improve from 0.40018\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1566 - accuracy: 0.9902 - val_loss: 0.4272 - val_accuracy: 0.9345\n",
      "Epoch 147/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1390 - accuracy: 0.9967\n",
      "Epoch 147: val_loss improved from 0.40018 to 0.37298, saving model to ResNet-for-EMNIST.h5\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1390 - accuracy: 0.9967 - val_loss: 0.3730 - val_accuracy: 0.9470\n",
      "Epoch 148/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1318 - accuracy: 0.9993\n",
      "Epoch 148: val_loss did not improve from 0.37298\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1318 - accuracy: 0.9993 - val_loss: 0.3894 - val_accuracy: 0.9463\n",
      "Epoch 149/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1300 - accuracy: 0.9998\n",
      "Epoch 149: val_loss did not improve from 0.37298\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1300 - accuracy: 0.9998 - val_loss: 0.4001 - val_accuracy: 0.9464\n",
      "Epoch 150/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1290 - accuracy: 0.9999\n",
      "Epoch 150: val_loss did not improve from 0.37298\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1290 - accuracy: 0.9999 - val_loss: 0.4093 - val_accuracy: 0.9469\n",
      "Epoch 151/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1283 - accuracy: 1.0000\n",
      "Epoch 151: val_loss did not improve from 0.37298\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1283 - accuracy: 1.0000 - val_loss: 0.4167 - val_accuracy: 0.9467\n",
      "Epoch 152/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1277 - accuracy: 1.0000\n",
      "Epoch 152: val_loss did not improve from 0.37298\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1277 - accuracy: 1.0000 - val_loss: 0.4223 - val_accuracy: 0.9470\n",
      "Epoch 153/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1272 - accuracy: 1.0000\n",
      "Epoch 153: val_loss did not improve from 0.37298\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1272 - accuracy: 1.0000 - val_loss: 0.4262 - val_accuracy: 0.9471\n",
      "Epoch 154/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1268 - accuracy: 1.0000\n",
      "Epoch 154: val_loss did not improve from 0.37298\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1268 - accuracy: 1.0000 - val_loss: 0.4295 - val_accuracy: 0.9473\n",
      "Epoch 155/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1263 - accuracy: 1.0000\n",
      "Epoch 155: val_loss did not improve from 0.37298\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1263 - accuracy: 1.0000 - val_loss: 0.4330 - val_accuracy: 0.9474\n",
      "Epoch 156/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1259 - accuracy: 1.0000\n",
      "Epoch 156: val_loss did not improve from 0.37298\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1259 - accuracy: 1.0000 - val_loss: 0.4348 - val_accuracy: 0.9474\n",
      "Epoch 157/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1255 - accuracy: 1.0000\n",
      "Epoch 157: val_loss did not improve from 0.37298\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1255 - accuracy: 1.0000 - val_loss: 0.4361 - val_accuracy: 0.9472\n",
      "Epoch 158/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1251 - accuracy: 1.0000\n",
      "Epoch 158: val_loss did not improve from 0.37298\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1251 - accuracy: 1.0000 - val_loss: 0.4379 - val_accuracy: 0.9472\n",
      "Epoch 159/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1247 - accuracy: 1.0000\n",
      "Epoch 159: val_loss did not improve from 0.37298\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1247 - accuracy: 1.0000 - val_loss: 0.4395 - val_accuracy: 0.9474\n",
      "Epoch 160/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1243 - accuracy: 1.0000\n",
      "Epoch 160: val_loss did not improve from 0.37298\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1243 - accuracy: 1.0000 - val_loss: 0.4416 - val_accuracy: 0.9472\n",
      "Epoch 161/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1239 - accuracy: 1.0000\n",
      "Epoch 161: val_loss did not improve from 0.37298\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1239 - accuracy: 1.0000 - val_loss: 0.4427 - val_accuracy: 0.9476\n",
      "Epoch 162/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1236 - accuracy: 1.0000\n",
      "Epoch 162: val_loss did not improve from 0.37298\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1236 - accuracy: 1.0000 - val_loss: 0.4432 - val_accuracy: 0.9477\n",
      "Epoch 163/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1232 - accuracy: 1.0000\n",
      "Epoch 163: val_loss did not improve from 0.37298\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1232 - accuracy: 1.0000 - val_loss: 0.4442 - val_accuracy: 0.9477\n",
      "Epoch 164/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1228 - accuracy: 1.0000\n",
      "Epoch 164: val_loss did not improve from 0.37298\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1228 - accuracy: 1.0000 - val_loss: 0.4460 - val_accuracy: 0.9478\n",
      "Epoch 165/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1224 - accuracy: 1.0000\n",
      "Epoch 165: val_loss did not improve from 0.37298\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1224 - accuracy: 1.0000 - val_loss: 0.4470 - val_accuracy: 0.9477\n",
      "Epoch 166/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1221 - accuracy: 1.0000\n",
      "Epoch 166: val_loss did not improve from 0.37298\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1221 - accuracy: 1.0000 - val_loss: 0.4472 - val_accuracy: 0.9478\n",
      "Epoch 167/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1217 - accuracy: 1.0000\n",
      "Epoch 167: val_loss did not improve from 0.37298\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1217 - accuracy: 1.0000 - val_loss: 0.4486 - val_accuracy: 0.9480\n",
      "Epoch 168/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1213 - accuracy: 1.0000\n",
      "Epoch 168: val_loss did not improve from 0.37298\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1213 - accuracy: 1.0000 - val_loss: 0.4496 - val_accuracy: 0.9479\n",
      "Epoch 169/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1210 - accuracy: 1.0000\n",
      "Epoch 169: val_loss did not improve from 0.37298\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1210 - accuracy: 1.0000 - val_loss: 0.4500 - val_accuracy: 0.9478\n",
      "Epoch 170/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1206 - accuracy: 1.0000\n",
      "Epoch 170: val_loss did not improve from 0.37298\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1206 - accuracy: 1.0000 - val_loss: 0.4502 - val_accuracy: 0.9478\n",
      "Epoch 171/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1202 - accuracy: 1.0000\n",
      "Epoch 171: val_loss did not improve from 0.37298\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1202 - accuracy: 1.0000 - val_loss: 0.4506 - val_accuracy: 0.9477\n",
      "Epoch 172/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1199 - accuracy: 1.0000\n",
      "Epoch 172: val_loss did not improve from 0.37298\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1199 - accuracy: 1.0000 - val_loss: 0.4516 - val_accuracy: 0.9478\n",
      "Epoch 173/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1195 - accuracy: 1.0000\n",
      "Epoch 173: val_loss did not improve from 0.37298\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1195 - accuracy: 1.0000 - val_loss: 0.4525 - val_accuracy: 0.9477\n",
      "Epoch 174/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1192 - accuracy: 1.0000\n",
      "Epoch 174: val_loss did not improve from 0.37298\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1192 - accuracy: 1.0000 - val_loss: 0.4525 - val_accuracy: 0.9480\n",
      "Epoch 175/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1188 - accuracy: 1.0000\n",
      "Epoch 175: val_loss did not improve from 0.37298\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1188 - accuracy: 1.0000 - val_loss: 0.4535 - val_accuracy: 0.9481\n",
      "Epoch 176/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1185 - accuracy: 1.0000\n",
      "Epoch 176: val_loss did not improve from 0.37298\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1185 - accuracy: 1.0000 - val_loss: 0.4540 - val_accuracy: 0.9479\n",
      "Epoch 177/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1181 - accuracy: 1.0000\n",
      "Epoch 177: val_loss did not improve from 0.37298\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1181 - accuracy: 1.0000 - val_loss: 0.4546 - val_accuracy: 0.9479\n",
      "Epoch 178/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1178 - accuracy: 1.0000\n",
      "Epoch 178: val_loss did not improve from 0.37298\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1178 - accuracy: 1.0000 - val_loss: 0.4546 - val_accuracy: 0.9479\n",
      "Epoch 179/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1174 - accuracy: 1.0000\n",
      "Epoch 179: val_loss did not improve from 0.37298\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1174 - accuracy: 1.0000 - val_loss: 0.4553 - val_accuracy: 0.9479\n",
      "Epoch 180/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1171 - accuracy: 1.0000\n",
      "Epoch 180: val_loss did not improve from 0.37298\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1171 - accuracy: 1.0000 - val_loss: 0.4550 - val_accuracy: 0.9482\n",
      "Epoch 181/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1167 - accuracy: 1.0000\n",
      "Epoch 181: val_loss did not improve from 0.37298\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1167 - accuracy: 1.0000 - val_loss: 0.4560 - val_accuracy: 0.9483\n",
      "Epoch 182/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1164 - accuracy: 1.0000\n",
      "Epoch 182: val_loss did not improve from 0.37298\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1164 - accuracy: 1.0000 - val_loss: 0.4564 - val_accuracy: 0.9483\n",
      "Epoch 183/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1160 - accuracy: 1.0000\n",
      "Epoch 183: val_loss did not improve from 0.37298\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1160 - accuracy: 1.0000 - val_loss: 0.4567 - val_accuracy: 0.9481\n",
      "Epoch 184/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1157 - accuracy: 1.0000\n",
      "Epoch 184: val_loss did not improve from 0.37298\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1157 - accuracy: 1.0000 - val_loss: 0.4570 - val_accuracy: 0.9482\n",
      "Epoch 185/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1154 - accuracy: 1.0000\n",
      "Epoch 185: val_loss did not improve from 0.37298\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1154 - accuracy: 1.0000 - val_loss: 0.4572 - val_accuracy: 0.9483\n",
      "Epoch 186/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1150 - accuracy: 1.0000\n",
      "Epoch 186: val_loss did not improve from 0.37298\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1150 - accuracy: 1.0000 - val_loss: 0.4576 - val_accuracy: 0.9482\n",
      "Epoch 187/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1147 - accuracy: 1.0000\n",
      "Epoch 187: val_loss did not improve from 0.37298\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1147 - accuracy: 1.0000 - val_loss: 0.4583 - val_accuracy: 0.9483\n",
      "Epoch 188/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1143 - accuracy: 1.0000\n",
      "Epoch 188: val_loss did not improve from 0.37298\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1143 - accuracy: 1.0000 - val_loss: 0.4585 - val_accuracy: 0.9482\n",
      "Epoch 189/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1140 - accuracy: 1.0000\n",
      "Epoch 189: val_loss did not improve from 0.37298\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1140 - accuracy: 1.0000 - val_loss: 0.4594 - val_accuracy: 0.9481\n",
      "Epoch 190/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1137 - accuracy: 1.0000\n",
      "Epoch 190: val_loss did not improve from 0.37298\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1137 - accuracy: 1.0000 - val_loss: 0.4597 - val_accuracy: 0.9480\n",
      "Epoch 191/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1133 - accuracy: 1.0000\n",
      "Epoch 191: val_loss did not improve from 0.37298\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1133 - accuracy: 1.0000 - val_loss: 0.4594 - val_accuracy: 0.9479\n",
      "Epoch 192/192\n",
      "731/732 [============================>.] - ETA: 0s - loss: 0.1130 - accuracy: 1.0000\n",
      "Epoch 192: val_loss did not improve from 0.37298\n",
      "lr:1.00e-03\n",
      "732/732 [==============================] - 49s 67ms/step - loss: 0.1130 - accuracy: 1.0000 - val_loss: 0.4595 - val_accuracy: 0.9479\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TRAIN\n",
    "\n",
    "if os.path.isfile(emnist_teacher_path):\n",
    "    emnist_teacher_model.load_weights(emnist_teacher_path)\n",
    "else:\n",
    "    emnist_history = emnist_teacher_model.fit(emnist_train_x, emnist_train_y, \n",
    "                    epochs=num_epoch, \n",
    "                    batch_size=batch_size, \n",
    "                    validation_data=(emnist_val_x, emnist_val_y), \n",
    "                    callbacks=emnist_teacher_callbacks,\n",
    "          )\n",
    "\n",
    "\n",
    "    plt.plot(emnist_history.history[\"accuracy\"], label = 'Train Accuracy')\n",
    "    plt.plot(emnist_history.history[\"val_accuracy\"], linestyle = 'dashed', label = 'Test Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2413435",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f2413435",
    "outputId": "920dd242-a1e1-43a5-9216-9b51feaea80a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "650/650 [==============================] - 6s 9ms/step - loss: 0.4857 - accuracy: 0.9454\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.48568499088287354, 0.9453846216201782]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emnist_teacher_model.evaluate(emnist_test_x, emnist_test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc95e25d",
   "metadata": {
    "id": "fc95e25d"
   },
   "source": [
    "### Reduced Teacher "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cdd7f3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b4cdd7f3",
    "outputId": "ccdab602-30b5-4abf-e3bb-566ed8b946a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_3 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               200960    \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 27)                6939      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 207,899\n",
      "Trainable params: 207,899\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "emnist_reduced_teacher = create_dense_nn((28, 28, 1), 27)\n",
    "emnist_reduced_teacher.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e49c57",
   "metadata": {
    "id": "e3e49c57"
   },
   "source": [
    "### EMNIST Teacher - Reduced Teacher Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b1282c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "a0b1282c",
    "outputId": "983b776c-3fdd-4d0b-8acf-f5660860f5e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2925/2925 [==============================] - 83s 27ms/step - sparse_categorical_accuracy: 0.8943 - student_loss: 2.4677 - distillation_loss: 0.0176 - val_sparse_categorical_accuracy: 0.8904 - val_student_loss: 2.5423\n",
      "Epoch 2/20\n",
      "2925/2925 [==============================] - 77s 26ms/step - sparse_categorical_accuracy: 0.9148 - student_loss: 2.4451 - distillation_loss: 0.0176 - val_sparse_categorical_accuracy: 0.9210 - val_student_loss: 2.5649\n",
      "Epoch 3/20\n",
      "2925/2925 [==============================] - 78s 27ms/step - sparse_categorical_accuracy: 0.9216 - student_loss: 2.4374 - distillation_loss: 0.0176 - val_sparse_categorical_accuracy: 0.9114 - val_student_loss: 2.5430\n",
      "Epoch 4/20\n",
      "2925/2925 [==============================] - 77s 26ms/step - sparse_categorical_accuracy: 0.9272 - student_loss: 2.4316 - distillation_loss: 0.0176 - val_sparse_categorical_accuracy: 0.8925 - val_student_loss: 2.5730\n",
      "Epoch 5/20\n",
      "2925/2925 [==============================] - 77s 26ms/step - sparse_categorical_accuracy: 0.9303 - student_loss: 2.4285 - distillation_loss: 0.0176 - val_sparse_categorical_accuracy: 0.9240 - val_student_loss: 2.4476\n",
      "Epoch 6/20\n",
      "2925/2925 [==============================] - 77s 26ms/step - sparse_categorical_accuracy: 0.9327 - student_loss: 2.4257 - distillation_loss: 0.0176 - val_sparse_categorical_accuracy: 0.9335 - val_student_loss: 2.5357\n",
      "Epoch 7/20\n",
      "2925/2925 [==============================] - 78s 27ms/step - sparse_categorical_accuracy: 0.9352 - student_loss: 2.4232 - distillation_loss: 0.0176 - val_sparse_categorical_accuracy: 0.9382 - val_student_loss: 2.5448\n",
      "Epoch 8/20\n",
      "2925/2925 [==============================] - 78s 27ms/step - sparse_categorical_accuracy: 0.9374 - student_loss: 2.4209 - distillation_loss: 0.0176 - val_sparse_categorical_accuracy: 0.9333 - val_student_loss: 2.4522\n",
      "Epoch 9/20\n",
      "2925/2925 [==============================] - 80s 27ms/step - sparse_categorical_accuracy: 0.9392 - student_loss: 2.4191 - distillation_loss: 0.0176 - val_sparse_categorical_accuracy: 0.9402 - val_student_loss: 2.4380\n",
      "Epoch 10/20\n",
      "2925/2925 [==============================] - 79s 27ms/step - sparse_categorical_accuracy: 0.9418 - student_loss: 2.4166 - distillation_loss: 0.0177 - val_sparse_categorical_accuracy: 0.9357 - val_student_loss: 2.5321\n",
      "Epoch 11/20\n",
      "2925/2925 [==============================] - 81s 28ms/step - sparse_categorical_accuracy: 0.9418 - student_loss: 2.4162 - distillation_loss: 0.0176 - val_sparse_categorical_accuracy: 0.9392 - val_student_loss: 2.4764\n",
      "Epoch 12/20\n",
      "2925/2925 [==============================] - 80s 27ms/step - sparse_categorical_accuracy: 0.9440 - student_loss: 2.4141 - distillation_loss: 0.0177 - val_sparse_categorical_accuracy: 0.9402 - val_student_loss: 2.4884\n",
      "Epoch 13/20\n",
      "2925/2925 [==============================] - 81s 28ms/step - sparse_categorical_accuracy: 0.9478 - student_loss: 2.4104 - distillation_loss: 0.0177 - val_sparse_categorical_accuracy: 0.9436 - val_student_loss: 2.5043\n",
      "Epoch 14/20\n",
      "2925/2925 [==============================] - 80s 27ms/step - sparse_categorical_accuracy: 0.9506 - student_loss: 2.4075 - distillation_loss: 0.0177 - val_sparse_categorical_accuracy: 0.9401 - val_student_loss: 2.5172\n",
      "Epoch 15/20\n",
      "2925/2925 [==============================] - 80s 27ms/step - sparse_categorical_accuracy: 0.9520 - student_loss: 2.4062 - distillation_loss: 0.0176 - val_sparse_categorical_accuracy: 0.9404 - val_student_loss: 2.4500\n",
      "Epoch 16/20\n",
      "2925/2925 [==============================] - 81s 28ms/step - sparse_categorical_accuracy: 0.9514 - student_loss: 2.4065 - distillation_loss: 0.0176 - val_sparse_categorical_accuracy: 0.9459 - val_student_loss: 2.4667\n",
      "Epoch 17/20\n",
      "2925/2925 [==============================] - 80s 27ms/step - sparse_categorical_accuracy: 0.9532 - student_loss: 2.4050 - distillation_loss: 0.0176 - val_sparse_categorical_accuracy: 0.9422 - val_student_loss: 2.4894\n",
      "Epoch 18/20\n",
      "2925/2925 [==============================] - 79s 27ms/step - sparse_categorical_accuracy: 0.9531 - student_loss: 2.4050 - distillation_loss: 0.0177 - val_sparse_categorical_accuracy: 0.9387 - val_student_loss: 2.4261\n",
      "Epoch 19/20\n",
      "2925/2925 [==============================] - 81s 28ms/step - sparse_categorical_accuracy: 0.9546 - student_loss: 2.4034 - distillation_loss: 0.0177 - val_sparse_categorical_accuracy: 0.9485 - val_student_loss: 2.4843\n",
      "Epoch 20/20\n",
      "2925/2925 [==============================] - 79s 27ms/step - sparse_categorical_accuracy: 0.9551 - student_loss: 2.4027 - distillation_loss: 0.0177 - val_sparse_categorical_accuracy: 0.9455 - val_student_loss: 2.4492\n",
      "650/650 [==============================] - 5s 7ms/step - sparse_categorical_accuracy: 0.9409 - student_loss: 2.4167\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "emnist_distiller = Distiller(student=emnist_teacher_model, teacher=emnist_reduced_teacher)\n",
    "emnist_distiller.compile(\n",
    "    optimizer=Adam(),\n",
    "    metrics=[SparseCategoricalAccuracy()],\n",
    "    student_loss_fn=SparseCategoricalCrossentropy(from_logits=True),\n",
    "    distillation_loss_fn=KLDivergence(),\n",
    "    alpha=0.2,\n",
    "    temperature=3,\n",
    ")\n",
    "\n",
    "# Distill teacher to student\n",
    "emnist_reduced_teacher_history = emnist_distiller.fit(emnist_train_x, emnist_train_y, validation_data=(emnist_val_x, emnist_val_y), epochs=20)\n",
    "\n",
    "# Evaluate student on test dataset\n",
    "emnist_distiller.evaluate(emnist_test_x, emnist_test_y)\n",
    "\n",
    "plt.plot(emnist_reduced_teacher_history.history[\"sparse_categorical_accuracy\"], label = 'Train Accuracy')\n",
    "plt.plot(emnist_reduced_teacher_history.history[\"val_sparse_categorical_accuracy\"], linestyle = 'dashed', label = 'Test Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427805c6",
   "metadata": {
    "id": "427805c6"
   },
   "source": [
    "# Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1a9584",
   "metadata": {
    "id": "3e1a9584"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d78af8",
   "metadata": {
    "id": "65d78af8"
   },
   "outputs": [],
   "source": [
    "parameters = [\n",
    "    ('gini', 3),\n",
    "    ('gini', 5),\n",
    "    ('gini', 10),\n",
    "    ('entropy', 3),\n",
    "    ('entropy', 5),\n",
    "    ('entropy', 10),\n",
    "]\n",
    "seed = 1234"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528daa70",
   "metadata": {
    "id": "528daa70"
   },
   "source": [
    "## CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d300e0",
   "metadata": {
    "id": "b9d300e0",
    "outputId": "646cd7f8-8295-4bb0-8211-663289ba148f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37500, 32, 32, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar_train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44029d50",
   "metadata": {
    "id": "44029d50"
   },
   "outputs": [],
   "source": [
    "nsamples, nx, ny, dim = cifar_train_x.shape\n",
    "dt_cifar_train_x = cifar_train_x.reshape((nsamples,nx*ny*dim))\n",
    "\n",
    "nsamples, nx, ny, dim = cifar_val_x.shape\n",
    "dt_cifar_val_x = cifar_val_x.reshape((nsamples,nx*ny*dim))\n",
    "\n",
    "nsamples, nx, ny, dim = cifar_test_x.shape\n",
    "dt_cifar_test_x = cifar_test_x.reshape((nsamples,nx*ny*dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22aecc7b",
   "metadata": {
    "id": "22aecc7b",
    "outputId": "415be8ef-0a6c-4b5f-e670-96902d6711cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree parameters: critertion - gini max depth - 3\n",
      "Training accuracy: 0.24013333333333334%\n",
      "Validation accuracy: 0.23672%\n",
      "Test accuracy: 0.2398%\n",
      "\n",
      "Tree parameters: critertion - gini max depth - 5\n",
      "Training accuracy: 0.27296%\n",
      "Validation accuracy: 0.26592%\n",
      "Test accuracy: 0.2642%\n",
      "\n",
      "Tree parameters: critertion - gini max depth - 10\n",
      "Training accuracy: 0.4394133333333333%\n",
      "Validation accuracy: 0.29176%\n",
      "Test accuracy: 0.2945%\n",
      "\n",
      "Tree parameters: critertion - entropy max depth - 3\n",
      "Training accuracy: 0.2392%\n",
      "Validation accuracy: 0.23368%\n",
      "Test accuracy: 0.2377%\n",
      "\n",
      "Tree parameters: critertion - entropy max depth - 5\n",
      "Training accuracy: 0.2736266666666667%\n",
      "Validation accuracy: 0.26096%\n",
      "Test accuracy: 0.2598%\n",
      "\n",
      "Tree parameters: critertion - entropy max depth - 10\n",
      "Training accuracy: 0.43933333333333335%\n",
      "Validation accuracy: 0.2952%\n",
      "Test accuracy: 0.3034%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for criterion, max_depth in parameters:\n",
    "    decision_tree = DecisionTreeClassifier(criterion=criterion, max_depth=max_depth, random_state=seed)\n",
    "    decision_tree.fit(dt_cifar_train_x, cifar_train_y)\n",
    "    \n",
    "    cifar_tree_train_predictions = decision_tree.predict(dt_cifar_train_x)\n",
    "    cifar_tree_val_predictions = decision_tree.predict(dt_cifar_val_x)\n",
    "    cifar_tree_test_predictions = decision_tree.predict(dt_cifar_test_x)\n",
    "    \n",
    "    cifar_tree_train_accuracy = metrics.accuracy_score(cifar_tree_train_predictions, cifar_train_y)\n",
    "    cifar_tree_val_accuracy = metrics.accuracy_score(cifar_tree_val_predictions, cifar_val_y)\n",
    "    cifar_tree_test_accuracy = metrics.accuracy_score(cifar_tree_test_predictions, cifar_test_y)\n",
    "    \n",
    "    print(f\"Tree parameters: critertion - {criterion} max depth - {max_depth}\")\n",
    "    print(f'Training accuracy: {cifar_tree_train_accuracy}%')\n",
    "    print(f'Validation accuracy: {cifar_tree_val_accuracy}%')\n",
    "    print(f'Test accuracy: {cifar_tree_test_accuracy}%')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a517933",
   "metadata": {
    "id": "6a517933"
   },
   "source": [
    "## EMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd2f29c",
   "metadata": {
    "id": "8dd2f29c"
   },
   "outputs": [],
   "source": [
    "nsamples, nx, ny, dim = emnist_train_x.shape\n",
    "dt_emnist_train_x = emnist_train_x.reshape((nsamples,nx*ny*dim))\n",
    "\n",
    "nsamples, nx, ny, dim = emnist_val_x.shape\n",
    "dt_emnist_val_x = emnist_val_x.reshape((nsamples,nx*ny*dim))\n",
    "\n",
    "nsamples, nx, ny, dim = emnist_test_x.shape\n",
    "dt_emnist_test_x = emnist_test_x.reshape((nsamples,nx*ny*dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52b05ef",
   "metadata": {
    "id": "c52b05ef",
    "outputId": "ea13904f-6334-4357-db73-218876f46469"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree parameters: critertion - gini max depth - 3\n",
      "Training accuracy: 0.21027777777777779%\n",
      "Validation accuracy: 0.20631410256410257%\n",
      "Test accuracy: 0.20860576923076923%\n",
      "\n",
      "Tree parameters: critertion - gini max depth - 5\n",
      "Training accuracy: 0.4167948717948718%\n",
      "Validation accuracy: 0.412275641025641%\n",
      "Test accuracy: 0.4084615384615385%\n",
      "\n",
      "Tree parameters: critertion - gini max depth - 10\n",
      "Training accuracy: 0.6911752136752137%\n",
      "Validation accuracy: 0.6471153846153846%\n",
      "Test accuracy: 0.6442788461538461%\n",
      "\n",
      "Tree parameters: critertion - entropy max depth - 3\n",
      "Training accuracy: 0.1983119658119658%\n",
      "Validation accuracy: 0.1967948717948718%\n",
      "Test accuracy: 0.19798076923076924%\n",
      "\n",
      "Tree parameters: critertion - entropy max depth - 5\n",
      "Training accuracy: 0.4022542735042735%\n",
      "Validation accuracy: 0.3996794871794872%\n",
      "Test accuracy: 0.3971153846153846%\n",
      "\n",
      "Tree parameters: critertion - entropy max depth - 10\n",
      "Training accuracy: 0.7022008547008547%\n",
      "Validation accuracy: 0.6594551282051282%\n",
      "Test accuracy: 0.6613461538461538%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for criterion, max_depth in parameters:\n",
    "    decision_tree = DecisionTreeClassifier(criterion=criterion, max_depth=max_depth, random_state=seed)\n",
    "    decision_tree.fit(dt_emnist_train_x, emnist_train_y)\n",
    "    \n",
    "    emnist_tree_train_predictions = decision_tree.predict(dt_emnist_train_x)\n",
    "    emnist_tree_val_predictions = decision_tree.predict(dt_emnist_val_x)\n",
    "    emnist_tree_test_predictions = decision_tree.predict(dt_emnist_test_x)\n",
    "    \n",
    "    emnist_tree_train_accuracy = metrics.accuracy_score(emnist_tree_train_predictions, emnist_train_y)\n",
    "    emnist_tree_val_accuracy = metrics.accuracy_score(emnist_tree_val_predictions, emnist_val_y)\n",
    "    emnist_tree_test_accuracy = metrics.accuracy_score(emnist_tree_test_predictions, emnist_test_y)\n",
    "    \n",
    "    print(f\"Tree parameters: critertion - {criterion} max depth - {max_depth}\")\n",
    "    print(f'Training accuracy: {emnist_tree_train_accuracy}%')\n",
    "    print(f'Validation accuracy: {emnist_tree_val_accuracy}%')\n",
    "    print(f'Test accuracy: {emnist_tree_test_accuracy}%')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2c012f",
   "metadata": {
    "id": "5f2c012f"
   },
   "source": [
    "# Part 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e07fe5",
   "metadata": {},
   "source": [
    "### 1. Distilling a Neural Network Into a Soft Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2035d28",
   "metadata": {
    "id": "d2035d28"
   },
   "outputs": [],
   "source": [
    "# https://arxiv.org/pdf/1711.09784.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "627fec94",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "627fec94",
    "outputId": "e6a8bf3f-092e-49e3-eb9f-ebd3710a4931"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f890b09c950>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32edc8aa",
   "metadata": {
    "id": "32edc8aa"
   },
   "outputs": [],
   "source": [
    "emnist_teacher_model.load_weights(emnist_teacher_path)\n",
    "cifar_teacher_model.load_weights(cifar_teacher_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b3d705f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4b3d705f",
    "outputId": "473481be-9fc2-4c93-ab3c-8a0a881e0832"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2925/2925 [==============================] - 243s 83ms/step\n",
      "1172/1172 [==============================] - 120s 102ms/step\n"
     ]
    }
   ],
   "source": [
    "emnist_y_train_soft = emnist_teacher_model.predict(emnist_train_x)\n",
    "cifar_y_train_soft = cifar_teacher_model.predict(cifar_train_x)\n",
    "\n",
    "emnist_train_x_flat = emnist_train_x.reshape((emnist_train_x.shape[0], -1))\n",
    "emnist_val_x_flat = emnist_val_x.reshape((emnist_val_x.shape[0], -1))\n",
    "emnist_test_x_flat = emnist_test_x.reshape((emnist_test_x.shape[0], -1))\n",
    "\n",
    "cifar_train_x_flat = cifar_train_x.reshape((cifar_train_x.shape[0], -1))\n",
    "cifar_val_x_flat = cifar_val_x.reshape((cifar_val_x.shape[0], -1))\n",
    "cifar_test_x_flat = cifar_test_x.reshape((cifar_test_x.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12068e80",
   "metadata": {
    "id": "12068e80"
   },
   "outputs": [],
   "source": [
    "def get_model_predictions(model, inputs, softmax = nn.Softmax(1)):\n",
    "    inputs = inputs.cuda()\n",
    "    outputs = model(inputs)\n",
    "    outputs = outputs.cpu()\n",
    "    return softmax(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa4c7fab",
   "metadata": {
    "id": "fa4c7fab"
   },
   "outputs": [],
   "source": [
    "class Leaf():\n",
    "    def __init__(self, nb_classes):\n",
    "        device = torch.device('cpu')\n",
    "        self.distribution = nn.Parameter(torch.rand(nb_classes).to(device))\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.path_prob = 0\n",
    "      \n",
    "\n",
    "    def forward(self):\n",
    "        # simply softmax of the learned distribution vector\n",
    "        return(self.softmax(self.distribution.view(1,-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "accb981c",
   "metadata": {
    "id": "accb981c"
   },
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, depth, nb_classes, input_size, lmbda):\n",
    "        self.input_size = input_size\n",
    "        self.nb_classes = nb_classes\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "        self.fc = nn.Linear(self.input_size, 1)\n",
    "        self.beta = nn.Parameter(torch.rand(1).to(device)) # inverse temperature\n",
    "\n",
    "        # to compute penalty\n",
    "        self.root_lmbda = lmbda\n",
    "        self.lmbda = lmbda * 2 ** (-depth)\n",
    "        self.alpha = 0 # will be set according to inputs\n",
    "\n",
    "        if depth > 0:\n",
    "            self.children = self.build_children(depth)  \n",
    "        else:\n",
    "            self.children = [Leaf(nb_classes), Leaf(nb_classes)]\n",
    "  \n",
    "\n",
    "    def build_children(self, depth):\n",
    "        return [Node(depth - 1, self.nb_classes, self.input_size, self.root_lmbda), \n",
    "            Node(depth - 1, self.nb_classes, self.input_size, self.root_lmbda)]\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return(torch.sigmoid(self.beta*self.fc(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "404b2f01",
   "metadata": {
    "id": "404b2f01"
   },
   "outputs": [],
   "source": [
    "class SoftDecisionTree(nn.Module):\n",
    "    def __init__(self, depth, nb_classes, input_size, learning_rate = 1e-2, weight_decay = 5e-4, lmbda = 0.1):\n",
    "        super(SoftDecisionTree, self).__init__()\n",
    "        # output_dim\n",
    "        self.nb_classes = nb_classes\n",
    "\n",
    "        #input_dim\n",
    "        self.input_size = input_size\n",
    "\n",
    "        # build tree\n",
    "        self.root = Node(depth - 1, self.nb_classes, self.input_size, lmbda)\n",
    "        self.nodes = []\n",
    "        self.leaves = []\n",
    "\n",
    "        # set Torch optimizer's parameters\n",
    "        self.collect_parameters()\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "        self.device = torch.device('cpu')\n",
    "\n",
    "  \n",
    "    def collect_parameters(self):\n",
    "        nodes = [self.root]\n",
    "        self.param_list = nn.ParameterList()\n",
    "        self.module_list = nn.ModuleList()\n",
    "        while nodes:\n",
    "            node = nodes.pop(0)\n",
    "            if isinstance(node, Leaf):\n",
    "                self.param_list.append(node.distribution)\n",
    "                self.leaves.append(node)\n",
    "            else:\n",
    "                nodes.append(node.children[0])\n",
    "                nodes.append(node.children[1])\n",
    "                self.module_list.append(node.fc)\n",
    "                self.nodes.append(node)\n",
    "\n",
    "\n",
    "    def forward(self, current_node, inputs, path_prob):\n",
    "        if isinstance(current_node, Leaf): \n",
    "            current_node.path_prob = path_prob\n",
    "            return # end of recursion at a leaf\n",
    "      \n",
    "        # set params for penalty\n",
    "        prob = current_node.forward(inputs)\n",
    "        current_node.alpha = torch.sum(prob * path_prob) / torch.sum(path_prob)\n",
    "\n",
    "        # Left Children -> prob = activation \n",
    "        self.forward(current_node.children[0], inputs, prob * path_prob)\n",
    "        # Right children -> prob = 1 - activation\n",
    "        self.forward(current_node.children[1], inputs, (1-prob) * path_prob)\n",
    "\n",
    "\n",
    "    def get_penalty(self):\n",
    "        C = 0\n",
    "        for node in self.nodes:\n",
    "            C += -node.lmbda * 0.5 *(torch.log(node.alpha) + torch.log(1-node.alpha))\n",
    "\n",
    "        return C\n",
    "\n",
    "\n",
    "    def predict_classes(self, batch_size):\n",
    "        chosen_predictors = [max(self.leaves, key=lambda leaf: leaf.path_prob[i])  for i in range(batch_size)]\n",
    "        predictions = [predictor.forward() for predictor in chosen_predictors]\n",
    "\n",
    "        return [np.argmax(pred.detach().cpu().numpy()) for pred in predictions]\n",
    "\n",
    "\n",
    "    def onehot_coding(self, target):\n",
    "        target_onehot = torch.FloatTensor(target.size()[0], self.nb_classes).to(self.device)\n",
    "        target_onehot.data.zero_()\n",
    "        target_onehot.scatter_(1, target.view(-1, 1), 1.0)\n",
    "        return target_onehot\n",
    "    \n",
    "    def get_loss(self, targets):\n",
    "        loss = 0\n",
    "        for leaf in self.leaves:\n",
    "            Q = torch.transpose(leaf.forward(), 0, 1).double()\n",
    "            loss_l = torch.matmul(targets.double(), torch.log(Q))\n",
    "            loss += leaf.path_prob * loss_l\n",
    "    \n",
    "        C = self.get_penalty()\n",
    "        return -loss.mean() + C\n",
    "\n",
    "\n",
    "    def train_epoch(self, train_loader, epoch_nb):\n",
    "        accuracies = []\n",
    "        for inputs, targets in train_loader:\n",
    "\n",
    "            inputs = inputs.to(self.device)\n",
    "            targets = targets.to(self.device)\n",
    "\n",
    "            inputs = inputs.view(len(inputs), -1)   \n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # training\n",
    "            ones = torch.ones((len(inputs), 1)).to(self.device)\n",
    "            self.forward(self.root, inputs, ones)\n",
    "            loss = self.get_loss(targets)\n",
    "\n",
    "            # gradient step\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # keep track of accuracy\n",
    "            predictions = self.predict_classes(len(inputs))\n",
    "            errors = np.count_nonzero(predictions - torch.argmax(targets, dim=1).detach().cpu().numpy())\n",
    "            accuracies.append(100. * (1 -  errors / len(inputs)))\n",
    "\n",
    "        return np.mean(accuracies)\n",
    "\n",
    "\n",
    "    def evaluate_tree(self, data_loader):\n",
    "        accuracies = []\n",
    "        self.eval()\n",
    "        for inputs, targets in data_loader:\n",
    "\n",
    "            inputs = inputs.to(self.device)\n",
    "            targets = targets.to(self.device)\n",
    "\n",
    "            # targets = self.onehot_coding(targets)\n",
    "            inputs = inputs.view(len(inputs), -1)   \n",
    "            self.forward(self.root, inputs, torch.ones((len(inputs), 1)))\n",
    "\n",
    "            predictions = self.predict_classes(len(inputs))\n",
    "            errors = np.count_nonzero(predictions - torch.argmax(targets, dim=1).detach().cpu().numpy())\n",
    "            accuracies.append(100. * (1 -  errors / len(inputs)))\n",
    "\n",
    "        return np.mean(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c6b79f5",
   "metadata": {
    "id": "8c6b79f5"
   },
   "outputs": [],
   "source": [
    "def train_soft_tree(soft_tree, data_loader_train, data_loader_test, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        soft_tree.train_epoch(data_loader_train, epoch)\n",
    "    test_acc = soft_tree.evaluate_tree(data_loader_test)\n",
    "    print(f\"Test accuracy: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aee0fd5",
   "metadata": {
    "id": "9aee0fd5"
   },
   "source": [
    "## CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c967fd8c",
   "metadata": {
    "id": "c967fd8c"
   },
   "outputs": [],
   "source": [
    "def get_cifar_dataset(batch_size):\n",
    "    dataset = TensorDataset(torch.from_numpy(cifar_train_x).type(torch.FloatTensor), torch.from_numpy(cifar_y_train_soft))\n",
    "    one_hot = np.zeros((cifar_test_y.size, cifar_test_y.max() + 1))\n",
    "    one_hot[np.arange(cifar_test_y.size), cifar_test_y] = 1\n",
    "    dataset_test = TensorDataset(torch.from_numpy(cifar_test_x).type(torch.FloatTensor), torch.from_numpy(one_hot))\n",
    "\n",
    "    data_loader_train = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    data_loader_test  = DataLoader(dataset_test, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    return data_loader_train, data_loader_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "66f3a820",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "66f3a820",
    "outputId": "0bca5664-7a88-4551-c4f3-4e20b4ca704c"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree depth: 3\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-24-30a3e8b39eb4>:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return(self.softmax(self.distribution.view(1,-1)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 23.18%\n",
      "Tree depth: 5\n",
      "Test accuracy: 29.33%\n",
      "Tree depth: 8\n",
      "Test accuracy: 9.89%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "cifar_data_loader_train, cifar_data_loader_test = get_cifar_dataset(batch_size)\n",
    "\n",
    "for depth in [3, 5, 8]:\n",
    "    print(f'Tree depth: {depth}')\n",
    "    soft_tree = SoftDecisionTree(depth, nb_classes=10, input_size=3*32*32)\n",
    "    train_soft_tree(soft_tree, cifar_data_loader_train, cifar_data_loader_test, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5CCfqWTdj59h",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5CCfqWTdj59h",
    "outputId": "7fca6212-ec2c-4cf8-bc0b-8cf6ef3abeeb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree depth: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-24-30a3e8b39eb4>:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return(self.softmax(self.distribution.view(1,-1)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 27.27%\n",
      "Tree depth: 5\n",
      "Test accuracy: 26.17%\n",
      "Tree depth: 8\n",
      "Test accuracy: 10.10%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "cifar_data_loader_train, cifar_data_loader_test = get_cifar_dataset(batch_size)\n",
    "\n",
    "for depth in [3, 5, 8]:\n",
    "    print(f'Tree depth: {depth}')\n",
    "    soft_tree = SoftDecisionTree(depth, nb_classes=10, input_size=3*32*32)\n",
    "    train_soft_tree(soft_tree, cifar_data_loader_train, cifar_data_loader_test, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7357a799",
   "metadata": {
    "id": "7357a799"
   },
   "source": [
    "## EMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9efa2ef1",
   "metadata": {
    "id": "9efa2ef1"
   },
   "outputs": [],
   "source": [
    "def get_emnist_dataset(batch_size):\n",
    "    dataset = TensorDataset(torch.from_numpy(emnist_train_x).type(torch.FloatTensor), torch.from_numpy(emnist_y_train_soft))\n",
    "    one_hot = np.zeros((emnist_test_y.size, emnist_test_y.max() + 1))\n",
    "    one_hot[np.arange(emnist_test_y.size), emnist_test_y] = 1\n",
    "    dataset_test = TensorDataset(torch.from_numpy(emnist_test_x).type(torch.FloatTensor), torch.from_numpy(one_hot))\n",
    "\n",
    "    data_loader_train = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    data_loader_test  = DataLoader(dataset_test, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    return data_loader_train, data_loader_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "590af4ad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "590af4ad",
    "outputId": "b2447633-0e86-4dc3-d822-7623faf5979a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree depth: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-24-30a3e8b39eb4>:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return(self.softmax(self.distribution.view(1,-1)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 25.60%\n",
      "Tree depth: 5\n",
      "Test accuracy: 37.81%\n",
      "Tree depth: 8\n",
      "Test accuracy: 53.16%\n"
     ]
    }
   ],
   "source": [
    "emnist_data_loader_train, emnist_data_loader_test = get_emnist_dataset(batch_size)\n",
    "\n",
    "for depth in [3, 5, 8]:\n",
    "    print(f'Tree depth: {depth}')\n",
    "    soft_tree = SoftDecisionTree(depth, nb_classes=27, input_size=1*28*28)\n",
    "    train_soft_tree(soft_tree, emnist_data_loader_train, emnist_data_loader_test, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bc4477",
   "metadata": {},
   "source": [
    "## 2. Improving the Interpretability of Deep NeuralNetworks with Knowledge Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1187ad3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://arxiv.org/pdf/1812.10924.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "27b4f683",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bd326cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_tree(model, X, y):\n",
    "    predictions = np.argmax(model.predict(X), axis=1)\n",
    "    compared_results = predictions == y\n",
    "    unique, counts = np.unique(compared_results, return_counts=True)\n",
    "    dict_values = dict(zip(unique, counts))\n",
    "    return dict_values[True] / (dict_values[True] + dict_values[False])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7232c65a",
   "metadata": {},
   "source": [
    "### CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "034ef1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distilled tree with max depth - 3\n",
      "Test accuracy: 23.98%\n",
      "\n",
      "Distilled tree with max depth - 5\n",
      "Test accuracy: 26.419999999999998%\n",
      "\n",
      "Distilled tree with max depth - 10\n",
      "Test accuracy: 29.49%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for max_depth in [3, 5, 10]:\n",
    "    distilled_tree = DecisionTreeRegressor(max_depth=max_depth)\n",
    "    distilled_tree.fit(cifar_train_x_flat, cifar_y_train_soft)\n",
    "    \n",
    "    accuracy = evaluate_tree(distilled_tree, cifar_test_x_flat, cifar_test_y)\n",
    "    print(f'Distilled tree with max depth - {max_depth}')\n",
    "    print(f'Test accuracy: {accuracy * 100}%')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b1aab9",
   "metadata": {},
   "source": [
    "### EMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d37777be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distilled tree with max depth - 3\n",
      "Test accuracy: 21.375%\n",
      "\n",
      "Distilled tree with max depth - 5\n",
      "Test accuracy: 38.82692307692307%\n",
      "\n",
      "Distilled tree with max depth - 10\n",
      "Test accuracy: 63.99038461538461%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for max_depth in [3, 5, 10]:\n",
    "    distilled_tree = DecisionTreeRegressor(max_depth=max_depth)\n",
    "    distilled_tree.fit(emnist_train_x_flat, emnist_y_train_soft)\n",
    "    \n",
    "    accuracy = evaluate_tree(distilled_tree, emnist_test_x_flat, emnist_test_y)\n",
    "    print(f'Distilled tree with max depth - {max_depth}')\n",
    "    print(f'Test accuracy: {accuracy * 100}%')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b86a5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "umapkernel",
   "language": "python",
   "name": "umapkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0244cba9e3ba43fa8d86e606a0a53520": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "02b668a13d7e4cb19f7b635104cd9c4b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3b72750205d5477f9dd20df186542056",
      "placeholder": "​",
      "style": "IPY_MODEL_f5054cffa6ea4f12be8488c44db3c9f2",
      "value": " 29475/50000 [00:00&lt;00:00, 156403.55 examples/s]"
     }
    },
    "05c7287a847b4d3aa0dc966b713be867": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "064675c8f95f40f4aaa8f122692d9e65": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "087b8374f1ae43d6ae3355970cfd3f1d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_11e540259f874c7ca5ff2b64bb034ca0",
      "placeholder": "​",
      "style": "IPY_MODEL_630d0d7038e5465ab0c7ad2833b493aa",
      "value": " 1/1 [00:04&lt;00:00,  2.59s/ url]"
     }
    },
    "0c9b216f5b36425895e2300f4d341492": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b699f772beb2418c9827eb4beebb8711",
      "placeholder": "​",
      "style": "IPY_MODEL_47ddde0b18c844a0b0243c9f1326d8c5",
      "value": "Shuffling ~/tensorflow_datasets/cifar10/3.0.2.incomplete2XQNLO/cifar10-test.tfrecord*...:   0%"
     }
    },
    "0c9bdda417af4101aed2083a8e09da4c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0e6ce32b01f543069113f976af76412e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "10b3883a6a3746e281d0415d5a813d5f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_eeadb96745ac4c01955a9cc49ca70422",
       "IPY_MODEL_418e5953a39346cba2e511d61ca9ba3b",
       "IPY_MODEL_b74e7aa6999643ff938a8f12ca01af38"
      ],
      "layout": "IPY_MODEL_ed09fe40563c415aae8464167b4502a1"
     }
    },
    "11e540259f874c7ca5ff2b64bb034ca0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "15d00b142cd94b03911a2fa477c9e395": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "18b2586c05264c2da5027beccd325db4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "24c564c16d804d8ebd631cb1bb186852": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2a60002d7cac43f2b1a086c42e98a363": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "2e81a0d3314b4ebeb867a451a4994d95": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f4cc2328bba14da9bc88a3e871a65630",
       "IPY_MODEL_5b476977667c459e875464430baeb67d",
       "IPY_MODEL_087b8374f1ae43d6ae3355970cfd3f1d"
      ],
      "layout": "IPY_MODEL_55f281da8951413d881ba93c8edf7643"
     }
    },
    "2f98eee1a36f4caaa96c2317f7c6d45b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "32e9a31e881e4bc6909070a246cc8020": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e0acd1ec81b64d01a48161330c49583d",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ee1d801653a24b88a00303361acd9c4f",
      "value": 1
     }
    },
    "34f8418e8f974779934c426b2e92e3c4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3610d4cf85e24fc8a5c27f31e6d3c013": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3b72750205d5477f9dd20df186542056": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3f19b9bc8f6d4d8c983ce1d34f81a767": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "418e5953a39346cba2e511d61ca9ba3b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d5da1c4a994d48369be9ce70666d1e14",
      "max": 50000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_064675c8f95f40f4aaa8f122692d9e65",
      "value": 50000
     }
    },
    "44fbb912208c445d8b47656d71663a9c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "46f86d65327e42aabe6664ff032dbeaa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_87ef0a051cd0461f9de3edad1e8ba990",
      "placeholder": "​",
      "style": "IPY_MODEL_4a10199c947949abab900fb449715713",
      "value": " 9916/10000 [00:07&lt;00:00, 1345.79 examples/s]"
     }
    },
    "47ddde0b18c844a0b0243c9f1326d8c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4a10199c947949abab900fb449715713": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4b5eb12b8291421088164d9da6247460": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_efc52e4a329d46d0b684ff358cb66b5a",
      "max": 10000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0244cba9e3ba43fa8d86e606a0a53520",
      "value": 10000
     }
    },
    "4f3eb6a685d443a98050c810b80c66eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_34f8418e8f974779934c426b2e92e3c4",
      "placeholder": "​",
      "style": "IPY_MODEL_05c7287a847b4d3aa0dc966b713be867",
      "value": "Generating splits...: 100%"
     }
    },
    "514543680d0e4453a1613e25aff67357": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_70cfb572baaa44a889646639dccfaa91",
       "IPY_MODEL_4b5eb12b8291421088164d9da6247460",
       "IPY_MODEL_46f86d65327e42aabe6664ff032dbeaa"
      ],
      "layout": "IPY_MODEL_e566d55b87e0477d8b7dfa03e93f8f23"
     }
    },
    "5592de8c98914247b347de98e60fcbf8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7f2db97f8af14336b6c7ccf29483ed2d",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_44fbb912208c445d8b47656d71663a9c",
      "value": 2
     }
    },
    "55b23fbf66ea4e4682405f9d65a9dd70": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8398d5fe20dd41e2ad750523d63b31b4",
      "placeholder": "​",
      "style": "IPY_MODEL_2f98eee1a36f4caaa96c2317f7c6d45b",
      "value": "Dl Size...: 100%"
     }
    },
    "55f281da8951413d881ba93c8edf7643": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5ac697d6f3e247d5a111d561bc56d9ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5b476977667c459e875464430baeb67d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fc3ba54af0e647008523e22f374b6bc3",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f482717d8df946d6b669930fe53788f6",
      "value": 1
     }
    },
    "630d0d7038e5465ab0c7ad2833b493aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6931537da6aa4c14820c98f8ded71905": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "699ce7ea32ec4851ab2709f6da3ad791": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_55b23fbf66ea4e4682405f9d65a9dd70",
       "IPY_MODEL_32e9a31e881e4bc6909070a246cc8020",
       "IPY_MODEL_8b0773d43b9e4014aa6f1a7e1222980e"
      ],
      "layout": "IPY_MODEL_87284029504d44a2887d1d9b794c33bf"
     }
    },
    "6f70c89758db4e5fb27af895f65e7222": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "70cfb572baaa44a889646639dccfaa91": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_be9b75b7518c4b80b74b6b9c13fd0427",
      "placeholder": "​",
      "style": "IPY_MODEL_ce9359d077914d4c9df88d09cfce6ad3",
      "value": "Generating test examples...:  99%"
     }
    },
    "7f2db97f8af14336b6c7ccf29483ed2d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7fce1d456e4f45f8853eebad35368f32": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "812a586dd4774aa99d01f8cc6e28c736": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8318fe1b4e1b4bd59e3f8b608fa9be07": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b54faad548a540eea491eb6d8d65e20f",
       "IPY_MODEL_9738eb5a1e914d5595758ebe6804b4f1",
       "IPY_MODEL_bc72f078c13f4b2f91b075f9283d79b0"
      ],
      "layout": "IPY_MODEL_d9d5e8655e4645039a21ef9ea9046302"
     }
    },
    "8398d5fe20dd41e2ad750523d63b31b4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "869e5601ae4549019ff4d83a9576d16a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "87284029504d44a2887d1d9b794c33bf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "87b78abbacd6485aaaf3b4dfc298cbef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "87ef0a051cd0461f9de3edad1e8ba990": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "883354a9df79462398addebf3dd07901": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "8b0773d43b9e4014aa6f1a7e1222980e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_87b78abbacd6485aaaf3b4dfc298cbef",
      "placeholder": "​",
      "style": "IPY_MODEL_fe2825a1081e41b19f388a10af2013c6",
      "value": " 162/162 [00:04&lt;00:00, 83.35 MiB/s]"
     }
    },
    "910a4b6a484e45d4b2cddd80aae33268": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f5f95e763eb04036b3b3f2322143c2cd",
      "placeholder": "​",
      "style": "IPY_MODEL_3f19b9bc8f6d4d8c983ce1d34f81a767",
      "value": "Shuffling ~/tensorflow_datasets/cifar10/3.0.2.incomplete2XQNLO/cifar10-train.tfrecord*...:  59%"
     }
    },
    "91e91ec17f7843e4b093fba531288091": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "9738eb5a1e914d5595758ebe6804b4f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2a60002d7cac43f2b1a086c42e98a363",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0e6ce32b01f543069113f976af76412e",
      "value": 1
     }
    },
    "99f505890c754e04b507ee76ec4a56ef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9c162973a23e450ebf0cf815459978d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4f3eb6a685d443a98050c810b80c66eb",
       "IPY_MODEL_5592de8c98914247b347de98e60fcbf8",
       "IPY_MODEL_fc306416ec354c4886302fed4a1e5d3d"
      ],
      "layout": "IPY_MODEL_883354a9df79462398addebf3dd07901"
     }
    },
    "a3ae645c0e434144a5f87ed3ea273c8c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a709771612f34d748e06b1c9e072847c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "aed4d4bb68af4fbaaab89d6b5e2af97b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_910a4b6a484e45d4b2cddd80aae33268",
       "IPY_MODEL_e7dbf5bd95e2422eb5086c393cab4cdd",
       "IPY_MODEL_02b668a13d7e4cb19f7b635104cd9c4b"
      ],
      "layout": "IPY_MODEL_91e91ec17f7843e4b093fba531288091"
     }
    },
    "b54faad548a540eea491eb6d8d65e20f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7fce1d456e4f45f8853eebad35368f32",
      "placeholder": "​",
      "style": "IPY_MODEL_869e5601ae4549019ff4d83a9576d16a",
      "value": "Extraction completed...: 100%"
     }
    },
    "b699f772beb2418c9827eb4beebb8711": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b74e7aa6999643ff938a8f12ca01af38": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_99f505890c754e04b507ee76ec4a56ef",
      "placeholder": "​",
      "style": "IPY_MODEL_5ac697d6f3e247d5a111d561bc56d9ac",
      "value": " 49885/50000 [00:42&lt;00:00, 1407.69 examples/s]"
     }
    },
    "bafb106450f84b009bac88d64659f41c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bc72f078c13f4b2f91b075f9283d79b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_812a586dd4774aa99d01f8cc6e28c736",
      "placeholder": "​",
      "style": "IPY_MODEL_cc6040969c6044e7b9b9a7f6eff8fac8",
      "value": " 1/1 [00:04&lt;00:00,  4.24s/ file]"
     }
    },
    "be9b75b7518c4b80b74b6b9c13fd0427": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c699df79ed3349039ff734532933edf9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c944bafc8af444eb8f6240139b9a8e6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0c9bdda417af4101aed2083a8e09da4c",
      "max": 10000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bafb106450f84b009bac88d64659f41c",
      "value": 10000
     }
    },
    "cc6040969c6044e7b9b9a7f6eff8fac8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ce9359d077914d4c9df88d09cfce6ad3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d5da1c4a994d48369be9ce70666d1e14": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d9d5e8655e4645039a21ef9ea9046302": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dec8fb115fea4557aa0cc826b30a0321": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0c9b216f5b36425895e2300f4d341492",
       "IPY_MODEL_c944bafc8af444eb8f6240139b9a8e6c",
       "IPY_MODEL_eba48eb71dc04a13aab0ea8bacd33a40"
      ],
      "layout": "IPY_MODEL_15d00b142cd94b03911a2fa477c9e395"
     }
    },
    "e0acd1ec81b64d01a48161330c49583d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "e566d55b87e0477d8b7dfa03e93f8f23": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "e7dbf5bd95e2422eb5086c393cab4cdd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_18b2586c05264c2da5027beccd325db4",
      "max": 50000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a709771612f34d748e06b1c9e072847c",
      "value": 50000
     }
    },
    "eba48eb71dc04a13aab0ea8bacd33a40": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f644873732224e609673d9c66b189688",
      "placeholder": "​",
      "style": "IPY_MODEL_6931537da6aa4c14820c98f8ded71905",
      "value": " 0/10000 [00:00&lt;?, ? examples/s]"
     }
    },
    "ed09fe40563c415aae8464167b4502a1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "ee1d801653a24b88a00303361acd9c4f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "eeadb96745ac4c01955a9cc49ca70422": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_24c564c16d804d8ebd631cb1bb186852",
      "placeholder": "​",
      "style": "IPY_MODEL_c699df79ed3349039ff734532933edf9",
      "value": "Generating train examples...: 100%"
     }
    },
    "efc52e4a329d46d0b684ff358cb66b5a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f482717d8df946d6b669930fe53788f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f4cc2328bba14da9bc88a3e871a65630": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3610d4cf85e24fc8a5c27f31e6d3c013",
      "placeholder": "​",
      "style": "IPY_MODEL_6f70c89758db4e5fb27af895f65e7222",
      "value": "Dl Completed...: 100%"
     }
    },
    "f5054cffa6ea4f12be8488c44db3c9f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f5bcfbe6c8124c8890c566232dce6fe5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f5f95e763eb04036b3b3f2322143c2cd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f644873732224e609673d9c66b189688": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fc306416ec354c4886302fed4a1e5d3d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f5bcfbe6c8124c8890c566232dce6fe5",
      "placeholder": "​",
      "style": "IPY_MODEL_a3ae645c0e434144a5f87ed3ea273c8c",
      "value": " 2/2 [00:50&lt;00:00, 22.08s/ splits]"
     }
    },
    "fc3ba54af0e647008523e22f374b6bc3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "fe2825a1081e41b19f388a10af2013c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
